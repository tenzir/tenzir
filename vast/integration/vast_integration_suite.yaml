config-file: vast.yaml

fixtures:
  BiTopologyTester:
    enter: | # python
      node0 = Server(self.cmd,
                     ['-e', f'127.0.0.1:{VAST_PORT}', '-i', 'node0', 'start'],
                     work_dir, name='node0', port=VAST_PORT,
                     config_file=self.config_file)
      node1 = Server(self.cmd,
                     ['-e', '127.0.0.1:42124', '-i', 'node1', 'start'],
                     work_dir, name='node1', port=42124,
                     config_file=self.config_file)
      cmd += ['-e', f'127.0.0.1:{VAST_PORT}']

    exit: | # python
      node0.stop()
      node1.stop()

  ExampleConfigFileTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f'127.0.0.1:{VAST_PORT}', '-i', 'node', 'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file='vast.yaml.example')
      cmd += ['-e', f'127.0.0.1:{VAST_PORT}']

    exit: | # python
      node.stop()

  ServerTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f'127.0.0.1:{VAST_PORT}', '-i', 'node', 'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file=self.config_file)
      cmd += ['-e', f'127.0.0.1:{VAST_PORT}']

    exit: | # python
      node.stop()

  ServerTesterMetricsEnabled:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f'127.0.0.1:{VAST_PORT}', '-i', 'node',
                    '--enable-metrics', 'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file=self.config_file)
      cmd += ['-e', f'127.0.0.1:{VAST_PORT}']

    exit: | # python
      node.stop()

  AgingTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f'127.0.0.1:{VAST_PORT}',
                     '-i', 'node',
                     '--aging-frequency=2000min',
                     '--aging-query=:ip == 192.168.1.104',
                     '--active-partition-timeout=5s',
                     'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file=self.config_file)
      cmd += ['-e', f'127.0.0.1:{VAST_PORT}']

    exit: | # python
      node.stop()

  DiskMonitorTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f'127.0.0.1:{VAST_PORT}',
                     '-i', 'node',
                     '--max-partition-size=8462',
                     'start',
                     '--disk-budget-check-interval=2',
                     '--disk-budget-check-binary=' + os.path.dirname(os.path.abspath(__file__)) + '/misc/scripts/count_partitions_plus1.sh',
                     '--disk-budget-high=1',
                     '--disk-budget-low=0'],
                    work_dir, name='node', port=VAST_PORT)
      cmd += ['-e', f'127.0.0.1:{VAST_PORT}']

    exit: | # python
      node.stop()

tests:
  JSON schema inference:
    tags: [schema, infer, json]
    steps:
      - command: infer
        input: data/json/empty-object.json
      - command: infer
        input: data/json/basic-types.json
      - command: infer
        input: data/json/string-types.json
      - command: infer
        input: data/json/array-types.json
      - command: infer
        input: data/json/nested-object.json
  Conn log counting:
    tags: [node, counting, zeek]
    steps:
      - command: -N --max-partition-size=64 import zeek
        input: data/zeek/conn.log.gz
      - command: -N count ":ip == 192.168.1.104"
      - command: -N count -e ":ip == 192.168.1.104"
      - command: -N count "resp_p == 80"
      - command: -N count "resp_p != 80"
      - command: -N count 861237
  Node Zeek conn log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export ascii 'where resp_h == 192.168.1.104'
      - command: -N export ascii 'where orig_bytes > 1k && orig_bytes < 1Ki'
      - command: -N export ascii 'where :string == "OrfTtuI5G4e" || :port == 67 || :uint64 == 67'
      - command: '-N export ascii "where #type == \"zeek.conn\" && resp_h == 192.168.1.104"'
      - command: '-N export ascii "where #type != \"zeek.conn\" && #type != \"vast.metrics\""'
      - command: '-N export ascii "where #type != \"foobar\" && resp_h == 192.168.1.104"'
  Node Zeek multiple imports:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N import zeek
        input: data/zeek/dns.log.gz
      - command: -N export ascii 'where resp_h == 192.168.1.104'
      - command: -N export ascii 'where zeek.conn.id.resp_h == 192.168.1.104'
      - command: '-N count ":timestamp >= 1970-01-01 && #type != \"vast.metrics\""'
      - command: -N count '#type == "zeek.conn"'
  Node Zeek dns log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/dns.log.gz
      - command: -N export ascii 'where resp_h == 192.168.1.104'
      - command: -N export zeek 'where resp_h == 192.168.1.104'
        transformation: awk '!/^#(open|close)/'
      - command: -N count ':port == 53'
      - command: '-N count ":uint64 == 53 && #type == \"zeek.dns\""'
  Node Zeek http log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/http.log.gz
      - command: -N export ascii 'where resp_h == 216.240.189.196'
  Node Zeek snmp log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/snmp.log.gz
      - command: -N export ascii 'where duration >= 3s'
  Node Zeek JSON:
    tags: [node, import-export, zeek, json]
    steps:
      - command: -N import -b zeek-json
        input: data/zeek/zeek.json
      - command: '-N export json "where \"zeek\" in #type"'
        transformation: jq -ec '.'
  Export json:
    tags: [json, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export json 'where resp_h == 192.168.1.104'
        transformation: jq -ec '.'
      - command: -N export json --omit-nulls 'where resp_h == 192.168.1.104'
        transformation: jq -ec '.'
  Shutdown with Metrics:
    tags: [metrics, shutdown]
    fixture: ServerTesterMetricsEnabled
    steps:
      - command: status
        # We just care about the exit code; the steps list cannot be empty, so
        # it's like a dummy check that confirms whether the server can be
        # connected to.
        transformation: jq -ec 'empty'
  Malformed Query:
    tags: [export]
    fixture: ServerTester
    steps:
      - command: export json 'yo that is not a query'
        expected_result: error
      - command: and that is not a command
        expected_result: error
  Flush:
    tags: [server, import, flush]
    fixture: ServerTester
    steps:
      - command: import zeek
        input: data/zeek/conn.log.gz
      - command: status catalog
        transformation: jq -ec '.catalog."num-partitions"'
      - command: flush
      - command: status catalog
        transformation: jq -ec '.catalog."num-partitions"'
  Export Timeout:
    tags: [server, export]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      # We use the `null` export format since timeouts are always racy; we
      # cannot reliably make a 0 second timeout return 0 events.
      - command: export --timeout=0s null 'where resp_h == 192.168.1.104'
        expected_result: error
      - command: export --continuous --timeout=0s null 'where resp_h == 192.168.1.104'
        expected_result: error
  Server Zeek conn log:
    tags: [server, import-export, zeek]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'where resp_h == 192.168.1.104'
      # import some more to make sure accounting data is in the system.
      - command: import -b --batch-size=10 zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-size=1000 zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-size=100000 zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-size=1 -n 242 zeek
        input: data/zeek/conn.log.gz
      - command: flush
      - command: status --detailed
        transformation: jq '.catalog.schemas | with_entries(.value = .value."num-events")'
      - command: status --detailed
        transformation: jq -ec 'del(.version) | del(.system."swap-space-usage") | paths(scalars) as $p | {path:$p, type:(getpath($p) | type)}'
      - command: status --detailed index importer
        transformation: jq -ec 'paths(scalars) as $p | {path:$p, type:(getpath($p) | type)}'
  Server Zeek multiple imports:
    tags: [server, import-export, zeek]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: show schemas --expand
      - command: import -b zeek
        input: data/zeek/dns.log.gz
      - command: export ascii 'where resp_h == 192.168.1.104'
      - command: export ascii 'where zeek.conn.id.resp_h == 192.168.1.104'
      - command: 'count ":timestamp >= 1970-01-01 && #type != \"vast.metrics\""'
      - command: count '#type == "zeek.conn"'
      - command: show schemas --yaml
  Query Operators:
    tags: [server, operator]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'where conn.duration <= 1.0s'
      - command: export json 'where duration >= 10.0s && duration < 15s'
      - command: export json --numeric-durations 'where duration >= 1.8s && duration < 2.0s'
      - command: export ascii 'where service  == "smtp"'
      - command: export ascii 'where missed_bytes  != 0'
      - command: export ascii 'where id.orig_h !in 192.168.1.0/24'
      - command: export ascii 'where id.orig_h in fe80:5074:1b53:7e7::/64'
      - command: export ascii 'where id.orig_h ni fe80:5074:1b53:7e7::/64'
  Expressions:
    tags: [server, expression]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'where fe80::5074:1b53:7e7:ad4d || 169.254.225.22'
      - command: export ascii 'where "OrfTtuI5G4e" || fe80::5074:1b53:7e7:ad4d'
  Node Type Query:
    tags: [node, type, ch6104]
    steps:
      - command: -N import -n 20 zeek
        input: data/zeek/conn.log.gz
      - command: -N export ascii
  Type Query:
    tags: [server, type, ch5404]
    fixture: ServerTester
    steps:
      - command: import -b -n 20 zeek
        input: data/zeek/conn.log.gz
      - command: 'export ascii "where #type == \"zeek.conn\""'
  Node json zeek conn:
    tags: [node, import-export, zeek, json]
    steps:
      - command: -N import -s @./misc/schema/zeek-conn.schema -t zeek.conn.custom json
        input: data/json/conn.log.json.gz
      - command: -N export ascii 'where duration > 6s'
      - command: -N export ascii 'where :timestamp >= 2011-08-15T03:48'
      - command: -N export ascii 'where :time >= 2011-08-15T03:48'
  Node suricata alert:
    tags: [node, import-export, suricata, eve, import-filter]
    steps:
      - command: -N import suricata '#type != "suricata.stats" && event_type != "flow"'
        input: data/suricata/eve.json
      - command: -N export ascii 'where src_ip == 147.32.84.165'
      - command: '-N export csv "where #type == /suricata.*/"'
      - command: '-N export zeek "where #type == /suricata.alert/"'
      - command: -N export json --omit-nulls --omit-empty-records

  Node suricata rrdata:
    tags: [node, import-export, suricata, eve]
    steps:
      - command: -N import json --selector=event_type:suricata
        input: data/suricata/rrdata-eve.json
      - command: -N export ascii
      - command: -N export json

  Node argus csv:
    tags: [node, import-export, argus, csv]
    steps:
      - command: -N import -t argus.record csv
        input: data/csv/argus-M57-10k-pkts.csv.gz
      - command: -N export ascii 'where State != "CON" && Dur > 4900ms'
      - command: -N export ascii 'where Cause == "Status" && Dur > 1s'

  Node argus ssv:
    tags: [node, import-export, argus, ssv, csv]
    steps:
      - command: -N import -t argus.record csv '--separator=" "'
        input: data/csv/argus-additional-fields.ssv
      - command: -N export csv

  Node argus tsv:
    tags: [node, import-export, argus, tsv, csv]
    steps:
      - command: -N import -t argus.record csv '--separator="\t"'
        input: data/csv/argus-reordered.tsv
      - command: -N export csv

  Manual 2:
    tags: [examples, disabled]
    fixture: BiTopologyTester
    steps:
      - command: peer 'localhost:42124'
      - command: status

  Multi addr query:
    tags: [import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export ascii
        input: queries/multi_addr.txt

  Taxonomy queries:
    tags: [concepts, models]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/pcap/zeek/conn.log.gz
      - command: import -b suricata
        input: data/pcap/suricata/eve.json.gz
      - command: count "net.src.ip == 192.168.168.100"
      - command: count "net.connection == <192.168.168.100, _, 72.247.178.18, _, _>"
      # We omit the whitespace after the colon on purpose, otherwise pyyaml
      # thinks this is a key-value pair.
      - command: count 'net.connection == <net.src.ip:192.168.168.100, net.dst.port:80>'
      - command: count 'net.connection != <net.src.ip:192.168.168.100, net.dst.port:80>'
      - command: count "net.connection == <_, _, _, _, _>"
      - command: count "net.connection == <_, _, _, 80, _>"
      - command: count "net.connection != <_, _, _, 80, _>"

  Show Taxonomies:
    tags: [concepts, models]
    fixture: ServerTester
    steps:
      - command: show concepts
        transformation: jq '.[] | select(.concept.name == "net.app") | .concept.fields | map(split(".")[0]) | unique'
      - command: show --yaml models
      - command: show schemas
  Arrow Full Data Model:
    condition: version | jq -e '."Apache Arrow"'
    tags: [export, arrow]
    steps:
      - command: -N import --batch-encoding=arrow --schema-file="@./misc/schema/all-types.schema" --type=all_types json
        input: data/json/all-types.json
      - command: '-N export arrow "where #type == \"all_types\""'
        transformation: python @./misc/scripts/print-arrow.py
  Arrow Export:
    condition: version | jq -e '."Apache Arrow"'
    tags: [export, arrow]
    steps:
      - command: -N import --batch-encoding=arrow zeek
        input: data/zeek/conn.log.gz
      - command: '-N export -n 10 arrow "where #type == \"zeek.conn\""'
        transformation: python @./misc/scripts/print-arrow.py
      - command: -N import --batch-encoding=arrow suricata
        input: data/suricata/eve.json
      - command: '-N export arrow "where #type == \"suricata.http\""'
        transformation: python @./misc/scripts/print-arrow.py
  Arrow Import:
    condition: version | jq -e '."Apache Arrow"'
    tags: [import, arrow]
    fixture: ServerTester
    steps:
      - command: import -b --batch-encoding=arrow arrow
        # the input corresponds to conn.log.gz + eve.json (see above)
        input: data/suricata/arrow_ipc.bin
      - command: 'export -n 10 arrow "where #type == \"zeek.conn\""'
        transformation: python @./misc/scripts/print-arrow.py
      - command: 'export arrow "where #type == \"suricata.http\""'
        transformation: python @./misc/scripts/print-arrow.py
      - command: count
  Import syslog:
    tags: [syslog, import]
    steps:
      - command: -N import syslog
        input: data/syslog/syslog.log
      - command: '-N export ascii "where #type == /syslog.*/"'
  Heterogeneous JSONL import:
    tags: [import, json, sysmon]
    steps:
      - command: -N import json
        input: data/json/sysmon.json
      - command: -N import suricata
        input: data/suricata/eve.json
      - command: -N export json 'where "Â®" in :string'
        transformation: jq -ec '.'
      - command: '-N export json "where #type ni \"suricata\""'
        transformation: jq -ec '.'
      - command: -N export json 'where ProcessGuid == /\{[0-9a-f]{8}-[0-9a-f]{4}-5ec2-7.15-[0-9a-f]{12}\}/'
        transformation: jq -ec '.'
  # Testing the #import_time meta extractor is a bit tricky and requires
  # sleeping. If this turns out to be a flaky test, increase durations and
  # queries respectively.
  # TODO: Re-enable this test once its flakiness has been resolved.
  # Import Time:
  #   tags: [import, json, suricata]
  #   steps:
  #     - command: -N import suricata '#type == "suricata.stats"'
  #       input: data/suricata/eve.json
  #       transformation: sleep 5
  #     - command: -N import suricata '#type == "suricata.stats"'
  #       input: data/suricata/eve.json
  #     - command: -N export json '#import_time < 5 seconds ago'
  #       transformation: jq -ec '.'
  #     - command: sleep 5
  #       prepend_vast: false
  #     - command: -N export json '#import_time < 5 seconds ago'
  #       transformation: jq -ec '.'

  Extractor Predicates:
    tags: [import, export, suricata]
    steps:
      - command: -N import suricata
        input: data/suricata/eve.json
      - command: -N count "timestamp && :ip"
      - command: -N count does_not_exist
      - command: -N export json "where flow.alerted"

  # Nesting Records in Lists is not currently fully supported, but its presence
  # should also not crash the JSON reader.
  Nested Records:
    tags: [import, json]
    steps:
      - guard: version | jq -e '."Apache Arrow"'
      - command: '-N import --schema="type custom = record { a: list<record { b: string, c: uint64, }>, d: uint64, }" --type=custom --batch-encoding=arrow json'
        input: data/json/record-in-list.json
      - command: '-N export ascii "where #type == \"custom\""'

  Zeek conn log aging:
    tags: [import-export, aging, zeek]
    fixture: AgingTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'where resp_h == 192.168.1.104'
      - command: version
        transformation: sleep 15
      - command: send eraser run
      - command: version
        transformation: sleep 2
      - command: export ascii 'where resp_h == 192.168.1.104'
  Spawn source:
    tags: [import, spawn-source, zeek]
    fixture: ServerTester
    steps:
      - command: spawn source -r @./data/suricata/eve.json suricata
      - command: flush
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: count '#type == /suricata.*/'
  Example config file:
    tags: [import-export, zeek]
    fixture: ExampleConfigFileTester
    config-file: vast.yaml.example
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'where net.app !in ["dns", "ftp", "http", "ssl"]'
  # The `DiskMonitorTester` is set up to run every second and to delete all
  # partitions on disk. We first import the complete zeek dataset with 8462
  # events, and then wait some to give the disk monitor enough time to run.
  # After that, all events from the first import should have been erased.
  Disk monitor:
    tags: [disk-quota]
    fixture: DiskMonitorTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: version
        transformation: sleep 4
      - command: count '#type == /zeek.*/'
  Export pipeline operator parsing everything but summarize:
    tags: [server, client, import-export, transforms]
    fixture: ServerTester
    steps:
      - command: import -b suricata
        input: data/suricata/eve.json
      - command: |
          export json
            'pass'
      - command: |
          export json
            ' pass
            /* a comment here */
            | select /* and a comment there /**/ timestamp, flow_id, src_ip, dest_ip, src_port
            /**/ /*foo*/
            '
      - command: "export json 'pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp'"
      - command: "export json 'pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id'"
      - command: "export json 'pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id'"
      - command: "export json 'pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip'"
      - command: "export json 'pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip
                               | rename source_ip=src_ip'"
      - command: "export json 'pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip
                               | rename source_ip=src_ip
                               | where #type ==\"suricata.alert\" || #type == \"suricata.fileinfo\"'"
  Export pipeline operator parsing only summarize:
    tags: [server, client, import-export, transforms]
    fixture: ServerTester
    steps:
      - command: import -b -t sysmon.NetworkConnection json
        input: data/json/sysmon.json
      - command: export json 'summarize distinct(SourcePort) by SourceIp'
      - command: export json 'summarize any(Initiated) by SourceIp, SourcePort, DestinationPoint, UtcTime resolution 1 minute'
      - command: export json 'summarize usercount=count(User), initiated=all(Initiated) by ProcessId'
  Export pipeline operator parsing after expression:
    tags: [server, client, import-export, transforms]
    fixture: ServerTester
    steps:
      - command: import -b suricata
        input: data/suricata/eve.json
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip
                               | rename source_ip=src_ip'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip
                               | rename source_ip=src_ip'"
      - command: "export json 'where src_ip==147.32.84.165 && (src_port==1181 || src_port == 138)
                               | pass
                               | select timestamp, flow_id, src_ip, dest_ip, src_port
                               | drop timestamp
                               | hash --salt=\"abcdefghij12\" flow_id
                               | drop flow_id
                               | pseudonymize -m \"crypto-pan\" -s \"123456abcdef\" src_ip, dest_ip
                               | rename source_ip=src_ip
                               | where #type ==\"suricata.alert\" || #type == \"suricata.fileinfo\"'"

  # A (small) stress test to see how the server handles queries that time out
  # before they're completed. We don't care about the output for these, we
  # just want to see if the server fixture shuts down orderly after this.
  Exporter Timeout:
    tags: [server, import-export, timeout]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: import -b zeek
        input: data/zeek/dns.log.gz
      - command: export --timeout=0ms json
        expected_result: error
      - command: export --timeout=0ms json
        expected_result: error
      - command: export --timeout=10ms json
        expected_result: ignore
      - command: export --timeout=20ms json
        expected_result: ignore
      - command: export --timeout=50ms json
        expected_result: ignore
      - command: export --timeout=100ms json
        expected_result: ignore
      - command: export --timeout=200ms json
        expected_result: ignore
      - command: export --timeout=500ms json
        expected_result: ignore
      - command: export --timeout=800ms json
        expected_result: ignore

  Self Repair:
    tags: [fault]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: version
        transformation: "@./misc/scripts/break-sizelimit.sh"
      # We use an extra import step to trigger the repair because querying
      # directly would not wait for the repair step.
      - command: -N import suricata
        input: data/suricata/eve.json
      - command: -N count "zeek.conn.id.orig_h == 192.168.1.104"

  # Checks if the import and export works as usual when we ignore creation
  # of partition indexes for all field names
  Optional Partition Indexes:
    tags: [import, export, csv]
    steps:
      - command: -N --config=@./vast-optional-indexes.yaml import -t argus.record csv '--separator=" "'
        input: data/csv/argus-additional-fields.ssv
      - command: -N export csv

  Process Query For Field With Skip Attribute:
    tags: [import, export]
    steps:
      - command: -N import --schema-file="@./misc/schema/zeek-with-skip.schema" zeek-json
        input: data/zeek/zeek.json
      - command: -N export json 'where username == "steve"'
  Rebuild undersized partitions:
    fixture: ExampleConfigFileTester
    tags: [import, export, rebuild]
    steps:
      - command: import --blocking suricata
        input: data/suricata/eve.json
      - command: import --blocking suricata
        input: data/suricata/eve.json
      - command: status --detailed catalog
        transformation: jq '.catalog.partitions | length'
      - command: export arrow
        transformation: python @./misc/scripts/print-arrow-batch-size.py
      - command: rebuild start --undersized
      - command: status --detailed catalog
        transformation: jq '.catalog.partitions | length'
      - command: export json
      - command: export arrow
        transformation: python @./misc/scripts/print-arrow-batch-size.py

  Export shutdown behavior:
    fixture: ServerTester
    tags: [import, export]
    steps:
      - command: export json
      - command: import --blocking suricata
        input: data/suricata/eve.json
      - command: export --max-events=2 json 'head 1'
        transformation: jq -ers 'length'
      - command: export json 'head 1'
        transformation: jq -ers 'length'
      - command: export --max-events=1 json 'head 0'
      - command: export json 'head 0'

  CEF:
    tags: [server, import-export, cef]
    fixture: ServerTester
    steps:
      - command: exec 'read cef | import'
        input: data/cef/cynet.log
      - command: exec 'read cef | import'
        input: data/cef/checkpoint.log
      - command: exec 'read cef | import'
        input: data/cef/forcepoint.log
      - command: export json 'where cef_version >= 0 && device_vendor == "Cynet"'
      - command: export json 'where 172.31.5.93'
      - command: export json 'where act == /Accept|Bypass/'
      - command: export json 'where dvc == 10.1.1.8'

  Patterns:
    fixture: ServerTester
    tags: [import, export]
    steps:
      - command: import -b suricata
        input: data/suricata/eve.json
      - command: export json 'where event_type == /.*flow$/'
      - command: export json 'where event_type == /.*FLOW$/i'

  Comments:
    fixture: ServerTester
    tags: [pipelines, comments]
    steps:
      - command: import --blocking suricata
        input: data/suricata/eve.json
      - command: export json 'select timestamp /*double beginning /* is valid */'
      - command: export json 'select timestamp | /**/'
        expected_result: error
      - command: export json 'select timestamp /*double ending*/ slash*/'
        expected_result: error

  Local Pipeline Execution:
    tags: [pipelines]
    steps:
      # - is an alternative form of stdin and stdout
      - command: exec 'from stdin read json | write json to stdout'
        input: data/json/sip.log.json.gz
      - command: exec 'from file - read json | write json to stdout'
        input: data/json/sip.log.json.gz
      # stdin and stdout are the defaults
      - command: exec 'read json | write json'
        input: data/json/files.log.json.gz
      # - is an alternative form of stdin and stdout
      - command: exec 'from - read json | write json to -'
        input: data/json/irc.log.json.gz
      - command: exec 'from file - read json | write json to -'
        input: data/json/irc.log.json.gz

  Read from JSON File:
    tags: [pipelines]
    steps:
      - command: exec 'from file @./data/json/record-in-list.json read json | write json to stdout'

  # TODO: uncomment once casting is implemented (uint64_t <-> int64_t is needed)
  # Read from suricata file:
  #   tags: [pipelines]
  #   steps:
  #     - command: exec 'from file @./data/suricata/eve.json read suricata | write json to stdout'

  Measure Events:
    tags: [pipelines]
    steps:
      - command: exec 'read json | measure | drop timestamp | write json'
        input: data/json/files.log.json.gz
      - command: exec 'read json | measure | measure | drop timestamp | write json'
        input: data/json/files.log.json.gz
      - command: exec 'read json | measure --real-time | drop timestamp | write json'
        input: data/json/files.log.json.gz
      - command: exec 'read json | measure --real-time | measure | drop timestamp | write json'
        input: data/json/files.log.json.gz

  Measure Bytes:
    tags: [pipelines]
    steps:
      - command: exec 'load stdin | measure | drop timestamp | write json'
        input: data/json/conn.log.json.gz
      - command: exec 'load stdin | measure | measure | drop timestamp | write json'
        input: data/json/conn.log.json.gz
      - command: exec 'load stdin | measure --real-time | drop timestamp | write json'
        input: data/json/conn.log.json.gz
      - command: exec 'load stdin | measure --real-time | measure | drop timestamp | write json'
        input: data/json/conn.log.json.gz

  Measure Incremental:
    tags: [pipelines]
    steps:
      - command: exec 'read cef | repeat 10 | measure --cumulative | drop timestamp | write json'
        input: data/cef/checkpoint.log
      - command: exec 'read cef | repeat 10 | measure --cumulative --real-time | drop timestamp | write json'
        input: data/cef/checkpoint.log
      - command: exec 'load stdin | repeat 10 | measure --cumulative | drop timestamp | write json'
        input: data/zeek/conn.log.gz
      - command: exec 'load stdin | repeat 10 | measure --cumulative --real-time | drop timestamp | write json'
        input: data/zeek/conn.log.gz

  Empty Record in Pipeline:
    tags: [pipelines]
    steps:
      - command: exec 'from - read json| write json to stdout'
        input: data/json/empty-record.json
      - command: exec 'from - read json | write csv to stdout'
        input: data/json/empty-record.json
      - command: exec 'from - read json | write xsv to stdout'
        input: data/json/empty-record.json

  Repeat:
    tags: [pipelines, repeat]
    steps:
      - command: exec 'load file - | parse cef | write json'
        input: data/cef/forcepoint.log
      - command: exec 'load file - | repeat 5 | parse cef | write json'
        input: data/cef/forcepoint.log
      - command: exec 'load file - | parse cef | repeat 5 | write json'
        input: data/cef/forcepoint.log
      - command: exec 'load file - | parse cef | measure | summarize sum(events) by schema | write json'
        input: data/cef/forcepoint.log
      - command: exec 'load file - | repeat 5 | parse cef | measure | summarize sum(events) by schema | write json'
        input: data/cef/forcepoint.log
      - command: exec 'load file - | parse cef | repeat 5 | measure | summarize sum(events) by schema | write json'
        input: data/cef/forcepoint.log

  Zeek TSV Pipeline Format:
    tags: [pipelines, zeek]
    steps:
      - command: exec 'from stdin read zeek-tsv | write json to stdout'
        input: data/zeek/merge.log
      - command: exec 'from stdin read zeek-tsv | write json to stdout'
        input: data/zeek/merge_with_whitespace_separation.log
      - command: exec 'from stdin read zeek-tsv | head 300 | write zeek-tsv to stdout'
        input: data/zeek/dns.log.gz
      - command: exec 'from stdin read zeek-tsv | head 300 | write csv to stdout'
        input: data/zeek/dns.log.gz
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv --set-separator=";" --empty-field="empty" --unset-field="NULLVAL" to stdout'
        input: data/zeek/whitespace_start.log
      - command: exec 'from stdin read json | write zeek-tsv to stdout'
        input: data/json/snmp.log.json.gz
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/empty.log
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_no_separator_header.log
        expected_result: error
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_no_set_separator_header.log
        expected_result: error
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_no_separator_value.log
        expected_result: error
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_no_empty_and_unset_fields.log
        expected_result: error
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_unequal_fields_types_length.log
        expected_result: error
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_duplicate_close_tag.log
        expected_result: error
      - command: exec 'from stdin read zeek-tsv | write zeek-tsv to stdout'
        input: data/zeek/broken_data_after_close_tag.log
        expected_result: error

  Enumerate:
    tags: [pipelines]
    steps:
      - command: -N import -n 10 zeek
        input: data/zeek/conn.log.gz
      - command: -N import -n 10 zeek
        input: data/zeek/dns.log.gz
      - command: -N export json 'select uid | enumerate index'
      - command: -N export json 'select uid | enumerate uid'

  Sort:
    tags: [pipelines, zeek]
    steps:
      - command: exec 'read zeek-tsv | select ts, uid | sort ts | write json'
        input: data/zeek/merge.log
      - command: exec 'read zeek-tsv | select uid | sort uid desc | write json'
        input: data/zeek/merge.log
      - command: exec 'read zeek-tsv | head | select service | sort service | write json'
        input: data/zeek/conn.log.gz
      - command: exec 'read zeek-tsv | head | select service | sort service nulls-first | write json'
        input: data/zeek/conn.log.gz
