"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[36627],{60259:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/tenzir-platform-is-now-generally-available","metadata":{"permalink":"/archive/tenzir-platform-is-now-generally-available","source":"@site/archive/tenzir-platform-is-now-generally-available/index.md","title":"Tenzir Platform is Now Generally Available","description":"We are thrilled to announce that after a year of rigorous development and","date":"2024-08-06T00:00:00.000Z","formattedDate":"August 6, 2024","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"platform","permalink":"/archive/tags/platform"}],"readingTime":3.965,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir Platform is Now Generally Available","authors":["mavam"],"date":"2024-08-06T00:00:00.000Z","tags":["tenzir","platform"],"comments":true},"nextItem":{"title":"An Intern\'s Reflection","permalink":"/archive/an-interns-reflection"}},"content":"We are thrilled to announce that after a year of rigorous development and\\ntesting, the **Tenzir Platform** is now generally available! Our journey began\\nat Black Hat 2023, where we first introduced the Tenzir Platform in early access\\nmode. Over the past year, we\'ve worked diligently, incorporating user feedback\\nand refining our technology to ensure stability and performance. Today, at Black\\nHat 2024, we are proud to officially launch the Tenzir Platform, confident in\\nits capabilities and excited to bring it to a wider audience.\\n\\n![Tenzir Platform goes GA](tenzir-platform-goes-ga.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is the Tenzir Platform?\\n\\nThe Tenzir Platform is a management layer for Tenzir Nodes. Prior to the\\nexistence of the platform, users had to use a command line interface or REST API\\nto manage their pipelines. The platform exposes a nice web app that makes this\\nprocess fun and easy, plus it avoids the need that users need to have access\\ndeep into the infrastructure to connect to each individual node.\\n\\nThere exist two TLS connections: one from the user\'s browser to the platform,\\nand one from every node the platform. The platform proxies requests and stores\\nhigh-level metadata state. But all the processing via pipelines takes place at\\nthe node. Think of the platform as *control plane* and the connected nodes as\\n*data plane*.\\n\\n## What has changed since the launch?\\n\\nA lot has happened in the last year! We\'d love to summarize all the goodness\\nthat we built and shipped, but to keep it concise, here at the three most\\nexciting things.\\n\\n### Library & Packages\\n\\nUsers can now one-click deploy a bundle of pipelines by installing a **package**\\nfrom the **library**. Over the last year, we heard many times that writing\\npipelines from scratch is a time-consuming task, as it also requires learning\\nour own Tenzir Query Language (TQL). Even though TQL is easy to learn if you\\nhave a Splunk background, there\'s still a ramp-up phase to become productive\\ndata pipeline wizard.\\n\\nPackages fix that. Get ready for instant value by simply installing a package\\nfrom our community library hosted at GitHub at\\n[tenzir/library](https://github.com/tenzir/library). Yes, you got it right:\\n**the community library is open source** and we encourage contributions.\\n\\nWe are ultra-excited about this feature, as it marks a new era of productivity\\nand time-to-value for our customers. Stay tuned for a lot of packages to arrive\\nover the next weeks and months!\\n\\n### TQL2\\n\\nFrom day one, we wanted to make working with data easy. This meant that we\\nneeded to deeply understand our target audience and their preferred way of\\nworking. We quickly learned that pipelines offer a sweet spot between\\neasy-of-use and expressiveness. There are numerous pipeline languages out there,\\nand after analyzing most of them in great depth, we were still not satisfied by\\nthe state of the art. We wanted to push the boundary in terms of *simplicity*\\nand *expressiveness* to innovate.\\n\\n![TQL Innovation](tql-innovation.svg)\\n\\nThis ambition culminated in our own **Tenzir Query Language (TQL)**. The first\\nversion was powerful, yet limited from a language perspective. After gaining\\nhands-on experience and user feedback over the last year, we identified various\\nangles for improvement that resulted TQL2\u2014a full language revamp.\\n\\n### Complete Redesign\\n\\nFinally, the app got a new coat of paint. We went from a playful, monochrome\\nlook to a much sleeker, modern data app. Check out the difference between now\\nand then:\\n\\n![Redesign Comparison](redesign-comparison.png)\\n\\n## How can I use the Platform?\\n\\nAt Tenzir, we host one instance of the platform at\\n[app.tenzir.com](https://app.tenzir.com) for our users and customers.\\n\\nTenzir comes in [four editions](https://tenzir.com/pricing):\\n\\n1. **Community Edition**: Our free edition! For home labbers, researchers, and\\n   curious data hackers.\\n2. **Professional Edition**: For consultants, small businesses, and self-serve\\n   needs.\\n3. **Enterprise Edition**: For enterprises, system integrators, and value-added\\n   resellers.\\n4. **Sovereign Edition**: For on-prem needs, service providers, and OEM\\n   solutions.\\n\\nThe first three editions use the platform that we host. If you need to operate\\nin an air-gapped environment, we also got you covered. The Sovereign Edition\\nallows you to host the entire platform yourself. Our\\n[tenzir/platform](https://github.com/tenzir/platform) GitHub repository hosts\\nexample Docker Compose files, and our docs explain [how to deploy the\\nplatform](/next/installation/deploy-the-platform).\\n\\n### Will the Community Edition remain free?\\n\\nYes. Here is why: Because the platform is primarily proxying information, it\\ndoesn\'t require a lot of resources to operate. As a result, it doesn\'t incur a\\nlot infrastructure cost on our end. It\'s not a loss leader!\\n\\n### How do I get started?\\n\\nJust visit [app.tenzir.com](http://app.tenzir.com) and [create a free\\naccount](/next/installation/create-an-account). May your data pipe dreams become\\na reality!\\n\\n:::tip Need help?\\nAny question or in need for more guidance? Send us an email at info@tenzir.com\\nor visit our [Discord server](/discord) for a direct line to our team and growing\\ncommunity.\\n:::"},{"id":"/an-interns-reflection","metadata":{"permalink":"/archive/an-interns-reflection","source":"@site/archive/an-interns-reflection/index.md","title":"An Intern\'s Reflection","description":"I spent the past twelve weeks interning at Tenzir and am excited to share my","date":"2024-06-14T00:00:00.000Z","formattedDate":"June 14, 2024","tags":[{"label":"internship","permalink":"/archive/tags/internship"}],"readingTime":6.075,"hasTruncateMarker":true,"authors":[{"name":"Bala Vinaithirthan","title":"Software Engineering Intern","url":"https://github.com/balavinaithirthan","email":"balabv1@gmail.com","imageURL":"https://github.com/balavinaithirthan.png","key":"balavinaithirthan"}],"frontMatter":{"title":"An Intern\'s Reflection","authors":["balavinaithirthan"],"date":"2024-06-14T00:00:00.000Z","tags":["internship"],"comments":true},"prevItem":{"title":"Tenzir Platform is Now Generally Available","permalink":"/archive/tenzir-platform-is-now-generally-available"},"nextItem":{"title":"Reduce Cost and Noise with Deduplication","permalink":"/archive/reduce-cost-and-noise-with-deduplication"}},"content":"I spent the past twelve weeks interning at Tenzir and am excited to share my\\nexperiences.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Journey\\n\\nMy first days at Tenzir felt like the first time I surfed. The summer after my\\nfirst year, I spent the mornings at Half Moon Bay, trying desperately to stay on\\nmy surfboard. Just as I would climb a giant wave, another would come behind and\\ncrash against my chest. A never-ending cycle for the weeks of summer.\\n\\nNow, a few years later, I sat in front of a monitor at Tenzir\u2019s headquarters,\\nleft-clicking on my mouse for what felt like hours. Each left click revealed yet\\nanother function, namespace, or variable; an endless cycle of waves. As a\\nstudent who learned from the ground up and got a broad understanding, I found\\nmyself trying to learn the meaning of every function and the intricate specifics\\nof the architecture.\\n\\nTwelve weeks later, I look back at this first week with a smile\u2014both at how much\\nI have grown with Dominik\u2019s mentorship and how little I knew about software\\nengineering and the complexities of these large codebases. In my first project,\\nI tackled problems head-on without understanding their scope. It took many\\nfrustrating sessions of left-clicking loops before I overcame my initial fear of\\nasking for help.\\n\\nTwo days into the internship, Dominik walked me through the setup for the `read`\\nand `write` operators, how to segment my tasks, and how to celebrate small wins.\\nWriting my first feature taught me to tackle issues systematically, just as one\\nmust focus only on the top of the wave. I had done more in that one hour than\\ncombined in the past two days. This began the journey of how I learned to ride\\neach wave as its own and respect the vastness of the codebase.\\n\\nThis journey of learning the basics continued as I spent the rest of the first\\nweek in a Git rebase hell, struggled with linters, and looked up Bash commands I\\nhadn\u2019t used in years. I also began to notice the differences between software\\nengineering and computer science. Soon, my love of runtime guarantees and\\ndetailed architecture shifted to writing code that worked and passed CI/CD.\\n\\nMy time at Tenzir has been eye-opening for my software engineering journey. I\u2019ve\\nlearned about the industry standards of modern C++, the scale of open-source\\nprojects, and practical problem-solving.\\n\\nBeyond writing code, I have learned about effective remote collaboration and the\\nimportance of community in tech, something I plan to carry forward in my career.\\nI am still growing, both in trying to find the balance between big-picture vs\\nindividual requirements and figuring out when to ask for help. Ultimately,\\nTenzir has prepared me for the real world and continues to fuel my passion for\\nlow-level systems.\\n\\n## Projects\\n\\nHere are some of the projects I worked on at Tenzir.\\n\\n### Parquet and Feather Parsers and Printers\\n\\nApache Parquet (and to a lesser extent its sibling Apache Feather) is a widely\\nused data formats for storing tabular data. In line with other Tenzir operators,\\nwe designed a parser plugin and printer plugin to allow users to read and write\\nFeather and Parquet data. We also adapted the Feather streaming interface to\\nstream Feather data and allowed for buffering in the Parquet printer. This\\nenables the following example:\\n\\n```text {0} title=\\"Convert a Feather file to a Parquet file\\"\\nfrom /path/to/file.feather\\n| to /path/to/file.parquet\\n```\\n\\n### Projection Pushdown Optimization\\n\\nProjection pushdown is an optimization technique that reduces data movement in a\\npipeline by pushing the projection operation closer to the data source. Tenzir\\npipelines have several operators capable of projection pushdown optimization,\\ne.g., `select`, `summarize`, `enrich`, and `drop`. This project focused on three\\nareas:\\n\\n1. Creating the framework to extend future projection pushdown optimizations.\\n2. Implementing projection pushdown for the `select` operator.\\n3. Modifying the Feather and Parquet data formats to accept the projection\\n   pushdown.\\n\\nWe successfully moved the `select` operator up through the pipeline:\\n\\n```text {0} title=\\"What the user writes\\"\\nfrom ./example.json\\n| \u2026\\n| select col\\n```\\n\\n```text {0} title=\\"What Tenzir runs\\"\\nfrom ./example.json\\n| select col\\n| \u2026\\n```\\n\\nProjection pushdown smartly detects whether it is safe to move up a projection\\nwithin the pipeline, operator by operator, and makes it so that the projection\\nruns as early as possible.\\n\\nFinally, we modified the Feather and Parquet parsers to accept projection\\npushdown optimizations. For example, in `read parquet | select foo`, Tenzir now\\neliminates the `select` operator entirely and only reads the column `foo` in the\\nParquet parser. Before, it read everything in the Parquet parser, and then later\\non dropped all columns but the projected ones.\\n\\n### Print Operator\\n\\nWe added a `print` operator that allows users to convert records into strings,\\nproviding an inverse to the parse operator. The following is now possible:\\n\\n```json {0} title=\\"Input\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": {\\n    \\"pkts_toserver\\": 1,\\n    \\"pkts_toclient\\": 0,\\n    \\"bytes_toserver\\": 54,\\n    \\"bytes_toclient\\": 0\\n  }\\n}\\n```\\n\\n```text {0} title=\\"Render the field flow as CSV\\"\\nfrom input.json\\n| print flow csv --no-header\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": \\"1,0,54,0\\"\\n}\\n```\\n\\n### Miscellaneous\\n\\nIn between these larger projects, I worked on smaller features and bug fixes.\\n\\nSpecifically, we modified the GeoIP context to allow for users to create empty\\ncontexts and load data later\u2014from anywhere. The following is now possible:\\n\\n```text {0} title=\\"Create an empty GeoIP context\\"\\ncontext create countries geoip\\n```\\n\\n```text {0} title=\\"Load the GeoIP database from a remote location\\"\\nload s3://path/to/countries.mmdb\\n| context load countries\\n```\\n\\nWe also modified the `python` plugin to check for syntax errors before input\\narrives. For example, the following will now error before input is read:\\n\\n```text {0} title=\\"Syntax error: did you mean \'else\'?\\"\\n\u2026\\n| python \'self.x = \\"foo\\" if self.y esle \\"bar\\"\'\\n```\\n\\nFinally, we added two timeout flags to `lookup-table` update that attaches a\\ntimeout to each event. The following is now possible:\\n\\n```text {0} title=\\"Expire lookup table entries after 10 days, or if they\'re not read for 1 day\\"\\nfrom inventory.csv\\n| context update subnets --create-timeout=10d --update-timeout=1d\\n```\\n\\n## Reflection\\n\\nI am grateful for Dominik\u2019s mentorship and Tenzir, which provided a structured\\nenvironment with collaborative coding and exciting projects. Dominik has\\nsignificantly influenced my approach to writing code, encouraging me to consider\\nuser experience and maintainability.\\n\\nInterning at Tenzir was an extraordinary experience. I am thankful to Matthias\\nfor the opportunity and for his advice on blending passion with business. This\\ninternship was a journey of firsts: my first time in Germany, my first role as a\\nC++ developer, and my first real-world application of CS education. I eagerly\\nanticipate the \\"nexts,\\" both at Tenzir and in life.\\n\\n:::note From the Team at Tenzir\\nWe all loved working with Bala\u2014it felt like he arrived just yesterday and became\\npart of the team immediately. This was the first time we\'ve had an intern at\\nTenzir, and it certainly won\'t be the last. His application came out of the\\nblue\u2014there was no advertised role for interns, but after we got an idea of his\\nhigh skill ceiling in two quick interview rounds, we thought we\'d give it a\\nshot. And it was so worth it.\\n\\nWant to get in touch with Bala? Connect with him on\\n[LinkedIn](http://linkedin.com/in/balabv/) and make sure to follow him on\\n[GitHub](https://github.com/balavinaithirthan).\\n:::"},{"id":"/reduce-cost-and-noise-with-deduplication","metadata":{"permalink":"/archive/reduce-cost-and-noise-with-deduplication","source":"@site/archive/reduce-cost-and-noise-with-deduplication/index.md","title":"Reduce Cost and Noise with Deduplication","description":"In the bustling world of data operations, handling large volumes of information","date":"2024-03-28T00:00:00.000Z","formattedDate":"March 28, 2024","tags":[{"label":"deduplicate","permalink":"/archive/tags/deduplicate"},{"label":"cost-savings","permalink":"/archive/tags/cost-savings"}],"readingTime":4.09,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Reduce Cost and Noise with Deduplication","authors":["mavam"],"date":"2024-03-28T00:00:00.000Z","tags":["deduplicate","cost-savings"],"comments":true},"prevItem":{"title":"An Intern\'s Reflection","permalink":"/archive/an-interns-reflection"},"nextItem":{"title":"Introducing Office Hours","permalink":"/archive/introducing-office-hours"}},"content":"In the bustling world of data operations, handling large volumes of information\\nis an everyday affair. Each day, countless bytes of data move around in systems,\\nchallenging organizations to maintain data accuracy, efficiency, and\\ncost-effectiveness. Amid this vast data landscape, one concept has emerged as a\\ncritical ally\u2014**deduplication**.\\n\\n![Deduplicate Example](deduplicate-example.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nDeduplication, the process of eliminating duplicate copies of repeating data,\\nisn\'t just about reducing storage needs\u2014it\'s about clarity, precision, and\\nreducing computational overload. It refines data to its most essential elements,\\nallowing us to gain vital insights with fewer extraneous distractions. And as\\nour data continues to grow in both volume and complexity, the need for effective\\nand efficient deduplication strategies cannot be underestimated. To make this\\ntask of deduplication easier, we\'re introducing an efficient and robust tool:\\nthe [`deduplicate`](/tql2/operators/deduplicate) operator. This operator serves\\nas a powerful ally in the fight against redundant data, enabling easy removal of\\nduplicate entries directly within a data stream or data set\u2014delivering cleaner,\\nleaner, and more useful data that holds the genuine insights your business\\nseeks.\\n\\nJoin us as we dive into the world of deduplication, explore the mechanics and\\nbenefits of the [`deduplicate`](/tql2/operators/deduplicate) operator, and\\nunderstand how it can redefine the way your organization approaches data\\nmanagement and analysis. The journey towards more effective and efficient data\\noperations starts here.\\n\\n:::note Available in Tenzir v4.12\\nWe\'re releasing Tenzir v4.12 next week, which will include the `deduplicate`\\noperator. Until then, you can play with it on the main branch on our [GitHub\\nrepo](https://github.com/tenzir/tenzir). Stay tuned.\\n:::\\n\\n## Why Deduplicate?\\n\\nDeduplication of operational event data is vital in various contexts dealing\\nwith large volumes of data, as it helps streamline information, eliminate\\nredundancies, and save storage. Here are three reasons for why you may want to\\nconsider deduplication as central tool for your data operations:\\n\\n### Enhanced Monitoring and Analytics\\n\\nDeduplication plays a crucial role in system and network monitoring, increasing\\nthe visibility of genuine operational patterns. In anomaly detection and\\nintrusion detection, deduplication refines the focus towards unique\\ninconsistencies or intrusions, paving the way for precise identification of\\nthreats, and thus, enhancing the accuracy and effectiveness of monitoring\\nsystems.\\n\\n### Efficient Threat Detection & Response\\n\\nDeduplication filters out multiple instances of the same security alert,\\nenabling a focus on unique threats. This allows for a swift, focused respons to\\nactual incidents, minimizing the distraction caused by redundant alerts and\\nenhancing overall operational efficiency and mean-time-to-respond (MTTR)\\nsignificantly.\\n\\n### Cost Optimization\\n\\nFor log management and SIEM systems, deduplication decreases ingest volume to\\nreduce costs associated with ingest-based pricing models. Additionally,\\ndeduplication reduces storage demands, optimizes search performance, and enables\\nmore effective threat detection, thus achieving both economic efficiency and\\noperational optimization in SIEM operations.\\n\\n## How Tenzir solves Deduplication\\n\\nLet\'s dive deeper into the `deduplicate` operator. It functions by marking out\\nduplicate events within a data stream, scrutinizing these duplicates based on\\nthe values of one or more fields. This discernment allows the operator to\\ndifferentiate between truly repetitive data and individually unique events. You\\ncan alter this behavioral pattern to suit your specific needs by manipulating\\nthree primary control knobs: **Limit**, **Distance**, and **Timeout**.\\n\\nThe diagram below illustrates these knobs intuitively:\\n\\n![Deduplicate](deduplicate.excalidraw.svg)\\n\\nHere\'s a more in-depth description of the controls:\\n\\n1. **Limit**: This knob controls the quantity of duplicate events that are\\n   permissible before they\'re classified as duplicates. Setting a limit of 1\\n   allows the emission of only unique events, making it the most stringent level\\n   of deduplication. A limit of N allows for the emission of a unique event N\\n   times before considering subsequent instances as duplicates. For instance,\\n   given a data stream `AAABAABBAC` with a limit of 2, the output\\n   post-deduplication would be `AABBC`.\\n\\n2. **Distance**: This knob is a measure of how far apart two events can be for\\n   them to be regarded as duplicates. Deduplicating a stream `AABABACBABB` with\\n   a distance set to 3 would yield `ABC`. A low distance value means that only\\n   closely placed events will be considered duplicates, while a higher value (or\\n   0 for infinity) broadens this criterion.\\n\\n3. **Timeout**: This parameter introduces a temporal aspect to the concept of\\n   duplicates. The timeout value dictates the duration that needs to elapse\\n   before a previously suppressed duplicate is considered a unique event again.\\n   If a duplicate event occurs before the specified timeout, the timer resets.\\n\\nYou can decide the fields to apply these knobs on by specifying a list of field\\nextractors. By default, the operator works on the entire event.\\n\\n:::tip Real-world examples\\nLet\'s make the abstract concrete and dive right in! Read our [user\\nguide](/next/usage/deduplicate-events) on deduplicating events for hands-on\\nexamples.\\n:::\\n\\nBy bringing together these strategically implemented parameters, the\\n`deduplicate` becomes a flexible, customizable tool for decluttering your data\\nstreams. It makes the daunting process of manual deduplication a thing of the\\npast and allows data operators to focus on the rich insights that uniquely\\noccurring events can provide."},{"id":"/introducing-office-hours","metadata":{"permalink":"/archive/introducing-office-hours","source":"@site/archive/introducing-office-hours/index.md","title":"Introducing Office Hours","description":"Did you ever want to get a sneak peek behind the scenes at Tenzir? Now you can!","date":"2024-02-13T00:00:00.000Z","formattedDate":"February 13, 2024","tags":[{"label":"discord","permalink":"/archive/tags/discord"},{"label":"office-hours","permalink":"/archive/tags/office-hours"}],"readingTime":1.615,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Introducing Office Hours","authors":["dominiklohmann"],"date":"2024-02-13T00:00:00.000Z","tags":["discord","office-hours"],"comments":true},"prevItem":{"title":"Reduce Cost and Noise with Deduplication","permalink":"/archive/reduce-cost-and-noise-with-deduplication"},"nextItem":{"title":"Switching Fluent Bit from JSON to MsgPack","permalink":"/archive/switching-fluentbit-from-json-to-msgpack"}},"content":"Did you ever want to get a sneak peek behind the scenes at Tenzir? Now you can!\\n\\n[![Office Hours](office-hours.excalidraw.svg)](https://discord.gg/JGS2N2Dwf6?event=1202543209662906393)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What are Office Hours?\\n\\nStarting immediately, our team will host a dedicated Office Hours event every\\nweek on our Discord server. This is an open-door event where every one of you is\\nwelcome, no matter what your level of experience or expertise.\\n\\n## What should you expect from this?\\n\\nOffice Hours is an exclusive space where you bring up your questions, thoughts,\\nsuggestions, or just pop in to say hi. From giving insights about our operations\\nover discussing the latest updates to providing exclusive previews of upcoming\\nfeatures, we will ensure that each session is loaded up with useful information\\nfor you. It\'s also an excellent opportunity for you to get a sneak peek behind\\nthe scenes, know us better, and see how we make the magic happen!\\n\\n## The topics we\'ll cover\\n\\nWe encourage you to bring any kind of questions to the table. Whether it\'s about\\nour work, our team, our future plans, or whatever else comes to your mind. While\\nwe want these hours to be helpful, we also want to make it a space where we can\\nshare, learn, and grow together as a community.\\n\\n## How you can participate\\n\\nGetting involved is simple:\\n\\n1. If you\'re not already a part of our Discord server, [join\\n   us](https://tenzir.com/discord)!\\n2. We\'ll announce the hours before they are starting every second Tuesday at 8\\n   AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST.\\n3. During the event, head over to the dedicated Office Hours voice channel on\\n   our Discord server to join.\\n\\nEt voil\xe0! You are all set.\\n\\nOffice Hours are designed to welcome all voices, experiences, and perspectives.\\nWe look forward to vibrant, engaging, informative sessions! Whether you want to\\nparticipate actively or just listen in, we\'re looking forward to seeing you\\nthere."},{"id":"/switching-fluentbit-from-json-to-msgpack","metadata":{"permalink":"/archive/switching-fluentbit-from-json-to-msgpack","source":"@site/archive/switching-fluentbit-from-json-to-msgpack/index.md","title":"Switching Fluent Bit from JSON to MsgPack","description":"We re-wired Tenzir\'s fluent-bit operator","date":"2024-01-10T00:00:00.000Z","formattedDate":"January 10, 2024","tags":[{"label":"fluent-bit","permalink":"/archive/tags/fluent-bit"},{"label":"json","permalink":"/archive/tags/json"},{"label":"msgpack","permalink":"/archive/tags/msgpack"},{"label":"performance","permalink":"/archive/tags/performance"}],"readingTime":2.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Switching Fluent Bit from JSON to MsgPack","authors":["mavam"],"date":"2024-01-10T00:00:00.000Z","tags":["fluent-bit","json","msgpack","performance"],"comments":true},"prevItem":{"title":"Introducing Office Hours","permalink":"/archive/introducing-office-hours"},"nextItem":{"title":"Contextualization Made Simple","permalink":"/archive/contextualization-made-simple"}},"content":"We re-wired Tenzir\'s [`fluent-bit`](/tql2/operators/from_fluent_bit) operator\\nand introduced a significant performance boost as a side effect: A 3\u20135x gain for\\nthroughput in events per second (EPS) and 4\u20138x improvement of latency in terms\\nof processing time.\\n\\n![Fluent Bit Speedup](fluent-bit-speedup.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nWhy were these gains available? Because we eliminated one round-trip of internal\\nJSON printing and parsing.\\n\\n## The Issue\\n\\nOur primary goal was actually working around an issue\\nwith the Fluent Bit `lib` output plugin, which we use whenever we have a\\n`fluent-bit` source operator. For example, if you configure an `elasticsearch`\\nFluent Bit source, Tenzir\'s `fluent-bit` operator autocomplete the `lib` output\\nplugin. This plugin has two modes of accepting input: JSON or\\n[MsgPack](https://msgpack.org/).\\n\\nUp to now, we relied on the JSON transfer mode because it was faster to get\\nstarted. However, during testing with the `elasticsearch` input that receives\\nlarge Windows event logs via Winlogbeat, we noticed that Fluent Bit\'s `lib`\\noutput produces messages of the form `[timestamp, object]` where `object` was\\ncropped. This basically generated invalid JSON.\\n\\nThe fix involved switching the exchange format of the `lib` output plugin from\\nJSON to MsgPack. If you\'re curious, take a look at\\n[#3770](https://github.com/tenzir/tenzir/pull/3770) for the full scoop. The\\nimprovement is already in the current development version and will be available\\nwith the next release.\\n\\n## Evaluation\\n\\nWe were curious how much this removal of the extra layer of printing and parsing\\nactually buys us. To this end, we use the following pipeline:\\n\\n```bash\\ntenzir --dump-metrics \'fluent-bit stdin | head 10M | discard\' < eve.json\\n```\\n\\nAdding `--dump-metrics` adds detailed per-operator metrics that help us\\nunderstand where operators spend their time. The [`head`](/tql2/operators/head)\\noperator take the first 10 million events, and [`discard`](/tql2/operators/discard)\\nsimply drops its input. The `eve.json` input into the `tenzir` binary is from\\nour Suricata dataset that we use in the [user guides](/usage). We measured\\nran our measurements on a 2021 Apple MacBook Pro M1 Max, as well as on a Manjaro\\nLinux laptop with a 14-core Intel i7 CPU.\\n\\nOur intuition was that we won\'t see major improvements, because generating JSON\\nisn\'t that expensive and we use [simdjson](https://simdjson.org/) to parse JSON.\\nBut the results surprised us:\\n\\n![Fluent Bit Performance](fluent-bit-performance.svg)\\n\\nOn macOS, events per second tripled from 50k to 150k, and the pipeline runtime\\nwent from 42 to 10 seconds. On Linux, the improvements were even higher. We\\ndon\'t have a good explanation for the rather stark difference between the\\noperating systems. Our hunch is that the allocator performance is the high-order\\nbit explaining the difference.\\n\\n## Summary\\n\\nWe switched from JSON to MsgPack for our\\n[`fluent-bit`](/tql2/operators/from_fluent_bit) source operator. This removed\\none round-trip of printing JSON (in Fluent Bit) and parsing JSON (in Tenzir). We\\nwere surprised to see that this change resulted in such substantial performance\\nimprovements. As a result, you can now run many more Fluent Bit ingestion\\npipelines in parallel at a single node with the same resources, or vertically\\nscale your Fluent Bit pipeline to new limits.\\n\\n:::tip Acknowledgements\\nThanks to Christoph Lobmeyer and Yannik Meinhardt for reporting this issue! \ud83d\ude4f\\n:::"},{"id":"/contextualization-made-simple","metadata":{"permalink":"/archive/contextualization-made-simple","source":"@site/archive/contextualization-made-simple/index.md","title":"Contextualization Made Simple","description":"How would you create a contextualization engine? What are the essential building","date":"2023-12-07T00:00:00.000Z","formattedDate":"December 7, 2023","tags":[{"label":"context","permalink":"/archive/tags/context"},{"label":"enrich","permalink":"/archive/tags/enrich"},{"label":"node","permalink":"/archive/tags/node"},{"label":"pipelines","permalink":"/archive/tags/pipelines"},{"label":"suricata","permalink":"/archive/tags/suricata"},{"label":"threat-intel","permalink":"/archive/tags/threat-intel"},{"label":"iocs","permalink":"/archive/tags/iocs"}],"readingTime":8.955,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Contextualization Made Simple","authors":["mavam"],"date":"2023-12-07T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["context","enrich","node","pipelines","suricata","threat-intel","iocs"],"comments":true},"prevItem":{"title":"Switching Fluent Bit from JSON to MsgPack","permalink":"/archive/switching-fluentbit-from-json-to-msgpack"},"nextItem":{"title":"Enrichment Complexity in the Wild","permalink":"/archive/enrichment-complexity-in-the-wild"}},"content":"How would you create a contextualization engine? What are the essential building\\nblocks? We asked ourselves these questions after studying what\'s out there and\\nbuilt from scratch a high-performance contextualization framework in Tenzir.\\nThis blog post introduces this brand-new framework, provides usage examples, and\\ndescribes how you can build your own context plugin.\\n\\n![Contextualization](contextualization.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\nThis is the second post of the our contextualization series. If you haven\'t read\\nthe first post, go check it out and learn [how Splunk, Elastic, and Sentinel\\nsupport contextualization](/archive/enrichment-complexity-in-the-wild).\\n\\n## Requirements\\n\\nAfter studying how others tackle the enrichment use case and talking to numerous\\npractitioners in the SecOps community, we went to the drawing board to identify\\nwhat we really need.\\n\\n1. **Dynamic context state updates**. In security, we\'re especially interested\\n   in use cases where the enrichment context is dynamic and changes over time.\\n   For example, the threat landscape is often represented in the form of\\n   observables, IOCs, or TTPs. Their utility quickly decays over time. Many\\n   indicators are only useful for a couple of days, as attacker infrastructure\\n   can be ephemeral and change rapidly. As a result, we need the ability to\\n   change our context state to keep a useful representation.\\n\\n2. **Decoupled context management and use**. Conceptually, a context has a write\\n   path to update its state, and a read path access its state. These two paths\\n   operate independently and its the job of the context to coordinate access to\\n   its shared state so that reads and writes do not conflict.\\n\\n3. **Flexible notions of context type**. Most systems out there treat enrichment\\n   as a join that brings two tables together. But what about Bloom filters? And\\n   ML model inference? What about API calls? Or custom libraries that shield a\\n   context? We\'re not always enriching with just a table, but many other types\\n   of context. Hence we need a dedicated abstraction what constitutes a context.\\n\\nA corollary of (3) is that we would like to support various *lookup modes*:\\n\\n![Contextualization Modes](contextualization-modes.excalidraw.svg)\\n\\nWe define these lookup modes as follows:\\n\\n1. **In-band**. The context data is co-located with the dataflow where it should\\n   act upon. This is especially important for high-velocity dataflows where\\n   there\'s a small time budget to perform an enrichment. For example, we\'ve seen\\n   network monitors like [Zeek](https://zeek.org) and\\n   [Suricata](https://suricata.io) links produce structured logs at 250k EPS,\\n   which would mean that enrichment cannot take more than 4 microseconds\\n   per event.\\n2. **Out-of-band**. The context data is far away from the to-be-contextualized\\n   dataflow. We encounter this mode when the context is intellectual property,\\n   when the context state is massive and maintenance is complex, or when it\'s\\n   created on-the-fly based on request by a service. A REST API is the most\\n   common example.\\n3. **Hybrid**. When both performance matters and state is not possible to ship\\n   to the contextualization point itself, then a hybrid approach can be a viable\\n   middle ground. [Google Safe Browsing][safebrowsing] is an example of this\\n   kind, where the Chrome browser keeps a subset of context state that\\n   represents threat actor URLs in the form of partial hashes, and when a users\\n   visits a URL where a partial match occurs, Chrome performs a candidate check\\n   using an API call. More than 99% of checked URLs never make it to the remote\\n   API, making this approach scalable. Note that the extra layer of hashing also\\n   protects the privacy of the entity performing the context lookup.\\n\\n[safebrowsing]: https://security.googleblog.com/2022/08/how-hash-based-safe-browsing-works-in.html\\n\\n## The Tenzir Contextualization Framework\\n\\nAs principled engineers, we took those requirements to the drawing board and\\nbuilt a solution that meets all of them. Two foundations of the Tenzir\\narchitecture made it possible to arrive at an elegant solution that results in a\\nsimple-yet-powerful user experience: (1) a pipeline-based data flow model, and\\n(2) the ability to manage state at continuously running Tenzir nodes. Let\'s walk\\nthrough a typical use case that explains the building blocks.\\n\\n:::info Example Scenario\\nDuring a compromise assessment, the security engineer Pada Wan is tasked with\\nfinding out whether the constituency initiates any connections to known\\ncommand-and-control servers. Pada takes the\\n[ThreatFox](https://threatfox.abuse.ch/) OSINT feed, a community malware where\\npractioners can share IOCs containing IPs, domains, URLs, and hashes. Pada\'s\\norganization uses Suricata to monitor their networks and now wants to leverage\\nDNS logs to identify possible lookups to known attacker infrastructure.\\n\\nPada, follwing first principles, remembers: *\\"Through a pipeline strong and wise,\\nsafe the constituency will stay, hmm.\\"*\\n:::\\n\\n### Create a context\\n\\nFirst create a **context** in a Tenzir node by running the following pipeline:\\n\\n```\\ncontext create threatfox lookup-table\\n```\\n\\nThis yields the following output:\\n\\n```json\\n{\\n  \\"num_entries\\": 0,\\n  \\"name\\": \\"threatfox\\"\\n}\\n```\\n\\nThe `context` operator manages context instances. It takes a context name and\\ntype as positional arguments. The `lookup-table` type is a key-value mapping\\nwhere a key is used to perform the context lookup and the value can be any\\nstructured additional data.\\n\\n### Load data into the context\\n\\nNext we fill the context with the contents of the ThreatFox feed. Here\'s the how\\nwe query the API with a HTTP POST request:\\n\\n```\\nfrom https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n```\\n\\nThe response looks as follows:\\n\\n```json\\n{\\n  \\"query_status\\": \\"ok\\",\\n  \\"data\\": [\\n    {\\n      \\"id\\": \\"1209500\\",\\n      \\"ioc\\": \\"8.219.229.99:4433\\",\\n      \\"threat_type\\": \\"botnet_cc\\",\\n      \\"threat_type_desc\\": \\"Indicator that identifies a botnet command&control server (C&C)\\",\\n      \\"ioc_type\\": \\"ip:port\\",\\n      \\"ioc_type_desc\\": \\"ip:port combination that is used for botnet Command&control (C&C)\\",\\n      \\"malware\\": \\"win.cobalt_strike\\",\\n      \\"malware_printable\\": \\"Cobalt Strike\\",\\n      \\"malware_alias\\": \\"Agentemis,BEACON,CobaltStrike,cobeacon\\",\\n      \\"malware_malpedia\\": \\"https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike\\",\\n      \\"confidence_level\\": 80,\\n      \\"first_seen\\": \\"2023-12-04 16:00:16 UTC\\",\\n      \\"last_seen\\": null,\\n      \\"reference\\": null,\\n      \\"reporter\\": \\"malpulse\\",\\n      \\"tags\\": null\\n    },\\n    {\\n    ..\\n    },\\n    {\\n    ..\\n    }\\n  ]\\n}\\n```\\n\\nUnfortunately the data is not yet in right shape yet. We need one IOC event per\\nlookup table entry, but the above is one giant event with all IOCs in the nested\\n`data` array. We can get to the desired shape with the `yield` operator hoists\\nthe array elements into top-level events. Let\'s take a look at one of the\\nevents:\\n\\n```\\nfrom https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n| yield data[]\\n| head 1\\n```\\n\\n```json\\n{\\n  \\"id\\": \\"1209500\\",\\n  \\"ioc\\": \\"8.219.229.99:4433\\",\\n  \\"threat_type\\": \\"botnet_cc\\",\\n  \\"threat_type_desc\\": \\"Indicator that identifies a botnet command&control server (C&C)\\",\\n  \\"ioc_type\\": \\"ip:port\\",\\n  \\"ioc_type_desc\\": \\"ip:port combination that is used for botnet Command&control (C&C)\\",\\n  \\"malware\\": \\"win.cobalt_strike\\",\\n  \\"malware_printable\\": \\"Cobalt Strike\\",\\n  \\"malware_alias\\": \\"Agentemis,BEACON,CobaltStrike,cobeacon\\",\\n  \\"malware_malpedia\\": \\"https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike\\",\\n  \\"confidence_level\\": 80,\\n  \\"first_seen\\": \\"2023-12-04 16:00:16 UTC\\",\\n  \\"last_seen\\": null,\\n  \\"reference\\": null,\\n  \\"reporter\\": \\"malpulse\\",\\n  \\"tags\\": null\\n}\\n```\\n\\nYes, this is something the `context` update can work with. Now that the data is\\nin the right shape, all we need is piping it to `context update`:\\n\\n```\\nfrom https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n| yield data[]\\n| where ioc_type == \\"domain\\"\\n| context update threatfox --key ioc\\n```\\n\\nThis outputs:\\n\\n```json\\n{\\n  \\"num_entries\\": 57,\\n  \\"name\\": \\"threatfox\\"\\n}\\n```\\n\\nThat is, 57 entries have been added successfully to the `threatfox` context.\\n\\n### Enrich with the context\\n\\nWe\'ve now loaded the context and can use it in other pipelines. As we\'re in a\\ncompromise assessment as example, we\'re interested in a realtime view of the\\nnetwork traffic. So we\'d like to hook the feed of all flow logs streaming into a\\nTenzir node. Let\'s say we have a Suricata `eve.json` file that we follow\\ncontinuously and import into a running node:\\n\\n```\\nfrom file --follow /suricata/eve.json read suricata\\n| import\\n```\\n\\nNow we hook into the DNS live feed for enrichment, keep only the matches, and\\nforward them to a Slack channel via `fluent-bit`:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.dns\\"\\n| enrich threatfox --field dns.rrname\\n| where threatfox.key != null\\n| fluent-bit slack webhook=IR_TEAM_SLACK_CHANNEL_URL\\n```\\n\\nIn more detail:\\n- `export --live` hooks into the import data feed at the node\\n- `where #schema == \\"suricata.dns\\"` restricts the feed to Suricata DNS events\\n- `enrich threatfox --field dns.rrname` joins the lookup table with the RR name\\n  of the DNS request\\n- `where threatfox.key != null` ignores non-matching enrichments\\n- `fluent-bit slack webhook=IR_TEAM_SLACK_CHANNEL_URL` sends the events to a\\n  Slack channel\\n\\nOne such matching enrichment may looks like this:\\n\\n```json\\n{\\n  \\"timestamp\\": \\"2021-11-17T16:57:42.389824\\",\\n  \\"flow_id\\": 1542499730911936,\\n  \\"pcap_cnt\\": 3167,\\n  \\"vlan\\": null,\\n  \\"in_iface\\": null,\\n  \\"src_ip\\": \\"45.85.90.164\\",\\n  \\"src_port\\": 56462,\\n  \\"dest_ip\\": \\"198.71.247.91\\",\\n  \\"dest_port\\": 53,\\n  \\"proto\\": \\"UDP\\",\\n  \\"event_type\\": \\"dns\\",\\n  \\"community_id\\": null,\\n  \\"dns\\": {\\n    \\"version\\": null,\\n    \\"type\\": \\"query\\",\\n    \\"id\\": 1,\\n    \\"flags\\": null,\\n    \\"qr\\": null,\\n    \\"rd\\": null,\\n    \\"ra\\": null,\\n    \\"aa\\": null,\\n    \\"tc\\": null,\\n    \\"rrname\\": \\"bza.fartit.com\\",\\n    \\"rrtype\\": \\"RRSIG\\",\\n    \\"rcode\\": null,\\n    \\"ttl\\": null,\\n    \\"tx_id\\": 0,\\n    \\"grouped\\": null,\\n    \\"answers\\": null\\n  },\\n  \\"threatfox\\": {\\n    \\"key\\": \\"bza.fartit.com\\",\\n    \\"context\\": {\\n      \\"id\\": \\"1209087\\",\\n      \\"ioc\\": \\"bza.fartit.com\\",\\n      \\"threat_type\\": \\"payload_delivery\\",\\n      \\"threat_type_desc\\": \\"Indicator that identifies a malware distribution server (payload delivery)\\",\\n      \\"ioc_type\\": \\"domain\\",\\n      \\"ioc_type_desc\\": \\"Domain name that delivers a malware payload\\",\\n      \\"malware\\": \\"apk.irata\\",\\n      \\"malware_printable\\": \\"IRATA\\",\\n      \\"malware_alias\\": null,\\n      \\"malware_malpedia\\": \\"https://malpedia.caad.fkie.fraunhofer.de/details/apk.irata\\",\\n      \\"confidence_level\\": 100,\\n      \\"first_seen\\": \\"2023-12-03 14:05:20 UTC\\",\\n      \\"last_seen\\": null,\\n      \\"reference\\": \\"\\",\\n      \\"reporter\\": \\"onecert_ir\\",\\n      \\"tags\\": [\\n        \\"irata\\"\\n      ]\\n    },\\n    \\"timestamp\\": \\"2023-12-04T13:52:49.043157\\"\\n  }\\n}\\n```\\n\\nNote the new field `threatfox` that is the context name. The `key` that matched\\nhas the value `bza.fartit.com`, which is also `dns.rrname`. There\'s also a\\n`timestamp` field when the enrichment took place, and the full data that we\\nloaded into the context under a given key.\\n\\n### Summary\\n\\nLet\'s recap what we did:\\n\\n1. Create a context via `context create` that is a lookup table.\\n2. Populate the context via `context update` with the ThreatFox OSINT feed.\\n3. Use the context via `enrich` to filter matching events.\\n4. Forward the enriched events to a Slack channel.\\n\\nThe `enrich` pipeline uses a lookup table to perform an in-band enrichment. Our\\nfirst measurements indicate that there is no noticeable performance overhead.\\n\\nWe can visualize this pipeline as follows:\\n\\n![Contextualization Example](contextualization-example.excalidraw.svg)\\n\\n## Comparison\\n\\nHow is this different to others, e.g., Splunk, Elastic, and Sentinel? If you\\ndon\'t recall how these three work, go back to our [previous blog\\npost](/archive/enrichment-complexity-in-the-wild).\\n\\n1. **Simplicity**. The core abstraction is incredibly simple\u2014an opaque context\\n   that can be used from two sides. You can simultaneously feed the context with\\n   a pipeline to update its state, and use it many other places to enrich your\\n   dataflows.\\n\\n2. **Flexibility**. The `enrich` operator gives you full control where you want\\n   to perform the contextualization. Place it before `import`, and it\'s an\\n   ingest-time enrichment. Put it after `export`, and it\'s a search-time\\n   enrichment. The abstraction is always same, regardless of the location.\\n\\n3. **Extensibility**. This blog post showed only one context type, the lookup\\n   table. This covers the most common enrichment scenario. But you can implement\\n   your own context types. A context plugin receives the full pipeline dataflow,\\n   and as a developer, you get [Apache Arrow](https://arrow.apache.org) record\\n   batches. This columnar representation works seamlessly with many data tools.\\n\\nStay tuned for more context plugins. Up next on our roadmap are three other\\nin-band context types: a Bloom filter, Sigma rules, and a\\n[MaxMind](https://github.com/maxmind/libmaxminddb)-based GeoIP context.\\n\\nYou can try all of this yourself by heading over to\\n[app.tenzir.com](https://app.tenzir.com). Deploy a cloud-based demo node and\\nenrich your life. As always, we\'re here to help and are looking forward to\\nmeeting you in our [Discord community](/discord)."},{"id":"/enrichment-complexity-in-the-wild","metadata":{"permalink":"/archive/enrichment-complexity-in-the-wild","source":"@site/archive/enrichment-complexity-in-the-wild/index.md","title":"Enrichment Complexity in the Wild","description":"Enrichment is a major part of a security data lifecycle and can take on many","date":"2023-11-27T00:00:00.000Z","formattedDate":"November 27, 2023","tags":[{"label":"enrichment","permalink":"/archive/tags/enrichment"},{"label":"context","permalink":"/archive/tags/context"},{"label":"splunk","permalink":"/archive/tags/splunk"},{"label":"elastic","permalink":"/archive/tags/elastic"},{"label":"azure","permalink":"/archive/tags/azure"},{"label":"sentinel","permalink":"/archive/tags/sentinel"},{"label":"kusto","permalink":"/archive/tags/kusto"}],"readingTime":5.14,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Enrichment Complexity in the Wild","authors":["mavam"],"date":"2023-11-27T00:00:00.000Z","tags":["enrichment","context","splunk","elastic","azure","sentinel","kusto"],"comments":true},"prevItem":{"title":"Contextualization Made Simple","permalink":"/archive/contextualization-made-simple"},"nextItem":{"title":"Matching YARA Rules in Byte Pipelines","permalink":"/archive/matching-yara-rules-in-byte-pipelines"}},"content":"Enrichment is a major part of a security data lifecycle and can take on many\\nforms: adding GeoIP locations for all IP addresses in a log, attaching asset\\ninventory data via user or hostname lookups, or extending alerts with magic\\nscore to bump it up the triaging queue. The goal is always to make the data more\\n*actionable* by providing a better ground for decision making.\\n\\nThis is the first part of series of blog posts on contextualization. We kick\\nthings off by looking at how existing systems do enrichment. In the next blog\\npost, we introduce how we address this use case with pipeline-first mindset in\\nthe Tenzir stack.\\n\\n![Enrichment Location](enrichment-location.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nWhen we refer to \\"enrichment\\" we typically mean performing an API call, joining\\nthe data at hand with another table/index in a SIEM, or a doing a lookup with\\nstatic data in CSV file. As shown above, we can do this either at *ingest-time*\\nbefore persisting the data, or at *search-time* when we retrieve historical\\ndata. There are also streaming-only use cases without historical data, but\\nthese can often be modeled as ingest-time enrichment with a different sink.\\n\\n## Existing Solutions\\n\\nLet\'s see how Splunk, Elastic, and Azure Data Explorer handle enrichment.\\n\\n### Splunk\\n\\n[Alex Teixeira](https://www.linkedin.com/in/inode/) wrote a great article on\\n[maintaining dynamic lookups in\\nSplunk](https://detect.fyi/the-salamander-method-how-to-maintain-dynamic-splunk-lookups-4fdae5868e7e). Here\'s the visual summary of his approach:\\n\\n![Salamander Method](salamander-method.png)\\n\\nSuppose we have a Splunk index called `web_access_logs` where we log web access\\nevents, and we want to maintain a weekly updated lookup table of unique visitor\\nIP addresses and their last visit date. Let\'s create an initial lookup table,\\nnamed `weekly_visitors.csv`, with fields like `ip_address` and\\n`last_visit_date`. Then we\'ll set up a scheduled search to run weekly. The\\nsearch should:\\n\\n- Extract the latest week\'s unique IP addresses and their last visit date\\n from `web_access_logs`.\\n- Load the existing `weekly_visitors.csv` using `inputlookup`.\\n- Merge and update the data, discarding IPs older than 7 days.\\n- Output the updated table using `outputlookup`.\\n\\nHere\'s the SPL for for the scheduled search:\\n\\n```spl\\nindex=\\"web_access_logs\\" earliest=-7d@d latest=@d\\n| stats latest(_time) as last_visit by ip_address\\n| eval last_visit_date=strftime(last_visit, \\"%F\\")\\n| inputlookup append=true weekly_visitors.csv\\n| dedup ip_address sortby -last_visit\\n| where last_visit >= relative_time(now(), \\"-7d@d\\")\\n| outputlookup weekly_visitors.csv\\n```\\n\\nIn detail:\\n\\n- `index=\\"web_access_logs\\" earliest=-7d@d latest=@d`: Fetches the last week\'s\\n  web access logs.\\n- `stats latest(_time) as last_visit by ip_address`: Aggregates the latest visit\\n  time for each IP.\\n- `eval last_visit_date=strftime(last_visit, \\"%F\\")`: Formats the last visit\\n  date.\\n- `inputlookup append=true weekly_visitors.csv`: Appends current lookup data for\\n  comparison.\\n- `dedup ip_address sortby -last_visit`: Removes duplicate IPs, keeping the most\\n  recent.\\n- `where last_visit >= relative_time(now(), \\"-7d@d\\")`: Filters out IPs older\\n  than 7 days.\\n- `outputlookup weekly_visitors.csv`: Updates the lookup table with the new\\n  data.\\n\\nThis query demonstrates Alex\' \\"Salamander Method\\" by regularly updating the\\nlookup table with recent data while discarding outdated records, maintaining an\\nup-to-date context for data enrichment.\\n\\nYou\'d use it as follows:\\n\\n```\\nindex=\\"network_security_events\\"\\n| lookup weekly_visitors.csv ip_address as source_ip OUTPUT last_visit_date\\n```\\n\\nThe `lookup` command enriches each event with the `last_visit_date` from\\n`weekly_visitors.csv` based on the matching `ip_address`. In this scenario,\\nyou\'re adding a temporal context to the security events by identifying when each\\nIP address involved in these events last visited your network. This can be\\nparticularly useful for quickly assessing whether a security event is related to\\na new or returning visitor, potentially aiding in the rapid assessment of the\\nevent\'s nature and severity.\\n\\n### Elastic\\n\\nElastic\'s [new ES|QL language](/archive/a-first-look-at-esql) also\\n[supports\\nenrichment](https://www.elastic.co/guide/en/elasticsearch/reference/master/esql-enrich-data.html)\\nusing the [`ENRICH`\\ncommand](https://www.elastic.co/guide/en/elasticsearch/reference/master/esql-commands.html#esql-enrich).\\nEnrichment is a key-based lookup using special index type. The diagram below\\nshows how it works.\\n\\n![ES|QL Enrichment](esql-enrich.png)\\n\\nAssume you have an index `network_security_logs` with fields like `source_ip`\\nand an enrich policy `threat_intel_policy` with data based on IP addresses and a\\nfield `threat_level`.\\n\\n```\\nSELECT e.*, threat.threat_level\\nFROM network_security_logs AS e\\nENRICH threat_intel_policy\\nON e.source_ip\\nWITH threat_level\\n```\\n\\nThis query enriches each record in `network_security_logs` with the\\n`threat_level` field from the threat intelligence data, providing an additional\\nlayer of context.\\n\\n### Sentinel\\n\\nIn Sentinel or Azure Data Explorer, you have a more data-centric view on the\\nproblem, using a combination of the Kusto operators\\n[`lookup`](https://docs.microsoft.com/azure/data-explorer/kusto/query/lookupoperator)\\nand\\n[`join`](https://docs.microsoft.com/azure/data-explorer/kusto/query/joinoperator):\\n\\n```\\nSecurityEvent\\n| lookup kind=leftouter GeoIPTable on $left.IPAddress == $right.GeoIP\\n| join kind=leftouter ThreatIntelTable on $left.IPAddress == $right.ThreatIP\\n```\\n\\nThis query takes a security event, enriches it with the `GeoIPTable` data based\\non the source IP, and then joins it with threat intelligence data from the\\n`ThreatIntelTable`.\\n\\n## Search vs. Ingest Time Enrichment\\n\\nWe\'ve now seen three examples for *search-time* enrichment. For *ingest-time*\\nenrichments, Splunk users need to adapt a config file `transforms.conf`.\\nElastic users can either use a separate tool\\n[Logstash](https://www.elastic.co/logstash) or resort to [Node Ingest\\nPipelines](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html).\\nAzure users can user to [Event\\nHubs](https://docs.microsoft.com/azure/event-hubs/), [Azure Stream\\nAnalytics](https://docs.microsoft.com/azure/stream-analytics/), or [Azure\\nFunctions](https://docs.microsoft.com/azure/azure-functions/). As a Sentinel\\nusers, you can use Data Collection Rules (DCRs) to apply KQL transformations to\\nincoming data before it\'s stored in your workspace. Here\'s an example of how\\nthis can be done:\\n\\n```\\nlet GeoIP = externaldata(country:string, city:string, [ip_range:string])\\n@\\"https://example.com/geoipdata.csv\\" \\nwith (format=\'csv\', ignoreFirstRecord=True);\\nSecurityEvent\\n| extend parsedIP = parse_ipv4(ip_address)\\n| lookup kind=leftouter GeoIP on $left.parsedIP between $right.ip_range\\n```\\n\\nWhat\'s left is taking this transformation and adding it to the data collection\\nrule. Here\'s a diagram from the\\n[documentation](https://learn.microsoft.com/en-us/azure/sentinel/data-transformation)\\non how the Azure pieces fit together:\\n\\n![Data Transformation Architecture](data-transformation-architecture.png)\\n\\n## Conclusion\\n\\nWe reviewed three existing approaches to enrichment by looking at Splunk,\\nElastic, and Sentinel. Common among all systems is the idea of first building a\\ndataset for contextualization, and then using that in a second step. In\\nparticular, the use of the context is decoupled from the management of the\\ncontext.\\n\\nWe could argue a lot about syntax ergonomics and system idiosyncrasies. But that\\nwouldn\'t move the needle much. The foundational mechanisms are the same in the\\ndifferent systems. That said, we did ask ourselves: how can we make enrichment\\n*as easy, fast, and flexible as possible*? Our next blog will have the answer.\\n\\nIn the meantime, feel free to browse through our docs, read our blog posts, or\\njoin our [Discord server](/discord) to talk to the power users in our\\ncommunity. You can always skip everything and dive right in at\\n[app.tenzir.com](https://app.tenzir.com)."},{"id":"/matching-yara-rules-in-byte-pipelines","metadata":{"permalink":"/archive/matching-yara-rules-in-byte-pipelines","source":"@site/archive/matching-yara-rules-in-byte-pipelines/index.md","title":"Matching YARA Rules in Byte Pipelines","description":"The new yara operator matches YARA rules on bytes, producing a","date":"2023-11-01T00:00:00.000Z","formattedDate":"November 1, 2023","tags":[{"label":"yara","permalink":"/archive/tags/yara"},{"label":"operator","permalink":"/archive/tags/operator"},{"label":"dfir","permalink":"/archive/tags/dfir"},{"label":"detection engineering","permalink":"/archive/tags/detection-engineering"}],"readingTime":5.79,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Matching YARA Rules in Byte Pipelines","authors":["mavam"],"date":"2023-11-01T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["yara","operator","dfir","detection engineering"],"comments":true},"prevItem":{"title":"Enrichment Complexity in the Wild","permalink":"/archive/enrichment-complexity-in-the-wild"},"nextItem":{"title":"Integrating Velociraptor into Tenzir Pipelines","permalink":"/archive/integrating-velociraptor-into-tenzir-pipelines"}},"content":"The new `yara` operator matches [YARA][yara] rules on bytes, producing a\\nstructured match output to conveniently integrate alerting tools or trigger next\\nprocessing steps in your detection workflows.\\n\\n[yara]: https://virustotal.github.io/yara/\\n\\n![YARA Operator](yara-operator.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n[YARA][yara] rules are a bedrock piece when it comes to writing detections on\\nbinary data. Malware analysts develop them based on sandbox results or threat\\nreports, incident responders capture the attacker\'s toolchain on disk images or\\nin memory, and security engineers share them with their peers.\\n\\n## Operationalize YARA rules\\n\\nThe most straight-forward way to execute a YARA rule is the official [`yara`\\ncommand-line utility](https://yara.readthedocs.io/en/stable/commandline.html).\\nConsider this rule:\\n\\n```\\nrule test {\\n  meta:\\n    string = \\"string meta data\\"\\n    integer = 42\\n    boolean = true\\n\\n  strings:\\n    $foo = \\"foo\\"\\n    $bar = \\"bar\\"\\n    $baz = \\"baz\\"\\n\\n  condition:\\n    ($foo and $bar) or $baz\\n}\\n```\\n\\nRunning `yara -g -e -s -L test.yara test.txt` on a file `test.txt` with contents\\n`foo bar` yields the following output:\\n\\n```\\ndefault:test [] test.txt\\n0x0:3:$foo: foo\\n0x4:3:$bar: bar\\n```\\n\\nThere are other ways to execute YARA rules, e.g.,\\n[ClamAV](https://www.clamav.net/),\\n[osquery](https://osquery.readthedocs.io/en/stable/deployment/yara/), or\\n[Velociraptor](https://docs.velociraptor.app/vql_reference/plugin/yara/)\u2014which\\nwe also [integrated as pipeline\\noperator](/archive/integrating-velociraptor-into-tenzir-pipelines).\\n\\nAnd now there\'s also Tenzir, with a [`yara`][yara-operator] operator that\\naccepts bytes as input and produces events as output. Let\'s take the simple case\\nof running the above example on string input:\\n\\n```bash\\necho \'foo bar\' | tenzir \'load stdin | yara /tmp/test.yara\'\\n```\\n\\nThe operator generates one `yara.match` event per matching rule:\\n\\n```json\\n{\\n  \\"rule\\": {\\n    \\"identifier\\": \\"test\\",\\n    \\"namespace\\": \\"default\\",\\n    \\"tags\\": [],\\n    \\"meta\\": {\\n      \\"string\\": \\"string meta data\\",\\n      \\"integer\\": 42,\\n      \\"boolean\\": true\\n    },\\n    \\"strings\\": {\\n      \\"$foo\\": \\"foo\\",\\n      \\"$bar\\": \\"bar\\",\\n      \\"$baz\\": \\"baz\\"\\n    }\\n  },\\n  \\"matches\\": {\\n    \\"$foo\\": [\\n      {\\n        \\"data\\": \\"Zm9v\\",\\n        \\"base\\": 0,\\n        \\"offset\\": 0,\\n        \\"match_length\\": 3\\n      }\\n    ],\\n    \\"$bar\\": [\\n      {\\n        \\"data\\": \\"YmFy\\",\\n        \\"base\\": 0,\\n        \\"offset\\": 4,\\n        \\"match_length\\": 3\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nEach match has a `rule` field describing the rule and a `matches` record indexed\\nby string identifier to report a list of matches per rule string. E.g., there is\\none match for `$bar` at byte offset 4 and match length 3. The Base64-encoded\\nexcerpt for the match is `YmFy` (= `\\"bar\\"`).[^1]\\n\\n[^1]: JSON doesn\'t distinguish binary blobs from strings. However, our type\\n    system does, so we encode blob values as Base64-encoded strings for formats\\n    that do not have a native blog representation.\\n\\n## Building a YARA streaming engine\\n\\n:::note Implementation Details\\nYou can skip this section if you are not interested in the inner workings, but\\nit may help understand how YARA works under the hood.\\n:::\\n\\nTenzir byte pipelines consist of a stream of variable-size chunks of memory.\\nE.g., when loading the raw bytes of file via `load file`, the dataflow may\\nconsist of multiple chunks. YARA scanners can also operate on multiple blocks of\\ndata. It might be tempting to treat these as contiguous, adjacent blocks of\\nmemory (we did this initially) and think that it should be possible to match a\\nrule across adjacent a blocks, like this:\\n\\n![YARA scanner blocks](yara-implementation.excalidraw.svg)\\n\\n[This is not the case](https://github.com/VirusTotal/yara/issues/1994). While it\\n*may* work, it\'s possible to write rules where this fails. As a result, simply\\nkeeping the input blocks in memory and feeding them to a scanner *might cause\\nfalse negatives* if you have a rule that should match across chunk boundaries.\\nIn other words, it\'s not possible to build an incremental streaming engine with\\nthe current YARA architecture. Moreover, YARA may perform multiple passes over\\nthe input, so it\'s neither possible to construct a one-pass streaming engine.\\n\\nThis is the reason why the `yara` operator supports two modes of operation:\\n\\n1. **Accumulating**: Accumulate all chunks perform a scan at the end. (default)\\n2. **Blockwise**: scan each block of memory as self-contained unit.\\n   (`--blockwise`)\\n\\nMode (1) copies all chunks in a single buffer. Mode (2) does work in streaming\\nmode, but it only makes sense if each chunk of memory is a self-contained unit,\\ne.g., when getting memory chunks from a message broker.\\n\\n## Mix and match loaders\\n\\nThe `stdin` loader in the above example produces chunks of bytes. But you can\\nuse any connector of your choice that yields bytes. In particular, you can use\\nthe `file` loader:\\n\\n```bash\\ntenzir \'load file --mmap /tmp/test.txt | yara /tmp/test.yara\'\\n```\\n\\n:::note Memory-mapping files\\nPassing `--mmap` to the `file` loader is purely an optimization that results in\\nthe creation of a single memory block as input to the `yara` operator. This\\nmeans the YARA scanner doesn\'t have to iterate over multiple blocks of memory,\\nwhich may be beneficial for intricate rules that require random access into the\\nfile.\\n:::\\n\\nIf you have a ZeroMQ socket where you publish malware samples to be scanned,\\nthen you only need to change the pipeline source:\\n\\n```bash\\ntenzir \'load zmq | yara /tmp/test.yara\'\\n```\\n\\nThis is where the [separation between structured and unstructured\\ndata][separation-of-concerns] in pipelines pays off. You plug in any loader\\nwhile leaving the remainder of `yara` pipeline in place.\\n\\n[separation-of-concerns]: /archive/five-design-principles-for-building-a-data-pipeline-engine#p1-separation-of-concerns\\n\\n## Post-process matches\\n\\nBecause the matches are structured events, you can use all existing operators to\\npost-process them. For example, send them to a Slack channel via\\n`fluent-bit`:\\n\\n```\\nload file --mmap /tmp/test.txt\\n| yara /tmp/test.yara\\n| fluent-bit slack webhook=URL\\n```\\n\\nOr store them with `import` at a Tenzir node to generate match statistics later\\non:\\n\\n```\\nload file --mmap /tmp/test.txt\\n| yara /tmp/test.yara\\n| import\\n```\\n\\n## Create a YARA rule matching service\\n\\nUsing just a few pipelines, you can quickly deploy a YARA rule scanning service\\nthat sends the matches to a Slack webhook. Let\'s that you want to scan malware\\nsample that you receive over a Kafka topic `malware`. Launch the processing\\npipeline as follows:\\n\\n```\\nload kafka --topic malware\\n| yara --blockwise /path/to/rules\\n| fluent-bit slack webhook=URL\\n```\\n\\nThis pipeline requires that every Kafka message is a self-contained malware\\nsample. Because the pipeline runs continuously, we supply the `--blockwise`\\noption so that the `yara` triggers a scan for every Kafka message, as opposed to\\naccumulating all messages indefinitely and only initiating a scan when the input\\nexhausts.\\n\\nYou can now submit a malware sample by sending it to the `malware` Kafka topic:\\n\\n```\\nload file --mmap evil.exe | save kafka --topic malware\\n```\\n\\nThe matches should now arrive as JSON message in the Slack channel associated\\nwith the webhook.\\n\\n## Summary\\n\\nWe\'ve introduced the `yara` operator as a byte-to-events transformation that\\nexposes YARA rule matches as structured events, making them easy to post-process\\nwith the existing collection of Tenzir operators. We also explained how you can\\ncreate a simple YARA rule scanning service that accepts malware samples via\\nKafka and sends the matches to a Slack channel.\\n\\nTry it yourself. Deploy detection pipelines with the `yara` operator for free\\nwith our Community Edition at [app.tenzir.com](https://app.tenzir.com). Missing\\nany other operators that operationalize detections? Swing by our [Discord\\nserver](/discord) and let us know!\\n\\n:::note Acknowledgements\\nThanks to [Thomas Patzke](https://github.com/thomaspatzke) for reviewing this\\nblog post and suggesting to make the default behavior of the operator more safe\\nto use. \ud83d\ude4f\\n:::"},{"id":"/integrating-velociraptor-into-tenzir-pipelines","metadata":{"permalink":"/archive/integrating-velociraptor-into-tenzir-pipelines","source":"@site/archive/integrating-velociraptor-into-tenzir-pipelines/index.md","title":"Integrating Velociraptor into Tenzir Pipelines","description":"The new velociraptor operator allows you to run","date":"2023-10-19T00:00:00.000Z","formattedDate":"October 19, 2023","tags":[{"label":"velociraptor","permalink":"/archive/tags/velociraptor"},{"label":"operator","permalink":"/archive/tags/operator"},{"label":"dfir","permalink":"/archive/tags/dfir"}],"readingTime":3.385,"hasTruncateMarker":true,"authors":[{"name":"Christoph Lobmeyer","title":"Senior Expert Incident Response (External)","url":"https://github.com/lo-chr","image_url":"https://github.com/lo-chr.png","imageURL":"https://github.com/lo-chr.png"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Integrating Velociraptor into Tenzir Pipelines","authors":[{"name":"Christoph Lobmeyer","title":"Senior Expert Incident Response (External)","url":"https://github.com/lo-chr","image_url":"https://github.com/lo-chr.png","imageURL":"https://github.com/lo-chr.png"},"mavam"],"date":"2023-10-19T00:00:00.000Z","last_updated":"2024-12-16T00:00:00.000Z","tags":["velociraptor","operator","dfir"],"comments":true},"prevItem":{"title":"Matching YARA Rules in Byte Pipelines","permalink":"/archive/matching-yara-rules-in-byte-pipelines"},"nextItem":{"title":"Five Design Principles for Building a Data Pipeline Engine","permalink":"/archive/five-design-principles-for-building-a-data-pipeline-engine"}},"content":"The new `velociraptor` operator allows you to run\\n[Velociraptor Query Language (VQL)][vql] expressions against a\\n[Velociraptor][velociraptor] server and process the results in a Tenzir\\npipeline. You can also subscribe to matching artifacts in hunt flows over a\\nlarge fleet of assets, making endpoint telemetry collection and processing a\\nbreeze.\\n\\n[velociraptor]: https://docs.velociraptor.app/\\n[vql]: https://docs.velociraptor.app/docs/vql\\n\\n![Velociraptor and Tenzir](velociraptor-and-tenzir.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n[Velociraptor][velociraptor] is a powerful digital forensics and incident\\nresponse (DFIR) tool for managing and interrogating endpoints. Not only does it\\nsupport ad-hoc extraction of forensic artifacts, but also continuous event\\nmonitoring to get alerted when suspicious things happen, such as the\\ninstallation of new scheduled tasks on a Windows machine.\\n\\nWe have been asked to make it possible to process the data collected at\\nendpoints in a Tenzir pipeline, so that you can store it cost-effectively,\\nfilter it, reshape it, and route it to your destination of choice. The\\n`velociraptor` operator honors this request. Thanks to Velociraptor\'s [gRPC\\nAPI][api] and [Python library][pyvelociraptor] that ship with the\\n[Protobuf][proto] definition, the implementation in C++ was straight-forward.\\n\\n[api]: https://docs.velociraptor.app/docs/server_automation/server_api/\\n[pyvelociraptor]: https://github.com/Velocidex/pyvelociraptor\\n[proto]: https://github.com/Velocidex/pyvelociraptor/blob/master/pyvelociraptor/api.proto\\n\\n## Usage\\n\\nThe `velociraptor` operator is a source that emits events. We implemented two\\nways to interact with a Velociraptor server:\\n\\n1. Send a [VQL][vql] query to a server and process the response.\\n\\n2. Use the `--subscribe <artifact>` option to hook into a continuous feed of\\n   artifacts that match the `<artifact>` regular expression. Whenever a client\\n   responds to a hunt that contains this artifact, the response will be\\n   forwarded to the pipeline and emit the artifact payload in the response field\\n   `HuntResults`.\\n\\n### Raw VQL\\n\\nHere\'s how you execute a VQL query and store the result at a Tenzir node:\\n\\n```bash\\nvelociraptor --query \\"select * from pslist()\\"\\n| import\\n```\\n\\nStoring it via `import` is just one of many options. For ad-hoc investigations,\\nyou often just want to analyze the result, for which a variety of\\ntransformations come in handy. For example:\\n\\n```bash\\nvelociraptor --query \\"select * from pslist()\\"\\n| select Name, Pid, PPid, CommandLine\\n| where Name == \\"remotemanagement\\"\\n```\\n\\n### Artifact Subscription\\n\\nIf you use Velociraptor to perform interactive investigations in DFIR cases, you\\nprobably hunt for forensic artifacts (like dropped files or specific entries in\\nthe Windows registry) on assets connected to your Velociraptor server. For\\nenrichment or to correlate the results with other security related data, you\\nmight want to post-process results of Velociraptor hunts.\\n\\nWith this feature Tenzir can subscribe to results of hunts, containing\\nVelociraptor artifacts of your choice [like the ones shipped with\\nVelociraptor](https://docs.velociraptor.app/artifact_references/). Every time a\\nclient reports back on an artifact that matches the given Regex (like `Windows`\\nor `Windows.Sys.StartupItems`) Tenzir will ingest the result of the underlying\\nquery into the pipeline.\\n\\n```bash\\nvelociraptor --subscribe Windows.Sys.StartupItems | import\\n```\\n\\nThere are many examples of anomalies to search for, like malware families\\npersisting in Windows RunKeys. You can find some inspirations in the procedure\\nexamples of [MITRE ATT&CK Sub-Technique\\nT1547.001](https://attack.mitre.org/techniques/T1547/001/).\\n\\nThe implementation of this feature\u2014specifically the underlying VQL query\u2014is\\ninspired by the built-in capability of Velociraptor to upload results of hunts\\n(the flows) to an elastic server utilizing the [Elastic.Flows.Upload\\nartifact](https://docs.velociraptor.app/artifact_references/pages/elastic.flows.upload/).\\n\\n## Preparation\\n\\nThe `velociraptor` pipeline operator acts as client and it establishes a\\nconnection to a Velociraptor server via gRPC. All Velociraptor client-to-server\\ncommunication is mutually authenticated and encrypted via TLS certificates. This\\nmeans you must provide a client-side certificate, which you can generate as\\nfollows. (Velociraptor ships as a static binary that we\\nrefer to as `velociraptor-binary` here.)\\n\\n1. Create a server configuration `server.yaml`:\\n   ```bash\\n   velociraptor-binary config generate > server.yaml\\n   ```\\n\\n2. Create an API client:\\n   ```bash\\n   velociraptor-binary -c server.yaml config api_client --name tenzir client.yaml\\n   ```\\n\\n   Copy the generated `client.yaml` to your Tenzir plugin configuration\\n   directory as `velociraptor.yaml` so that the operator can find it:\\n   ```bash\\n   cp client.yaml /etc/tenzir/plugin/velociraptor.yaml\\n   ```\\n\\n3. Run the frontend with the server configuration:\\n   ```bash\\n   velociraptor-binary -c server.yaml frontend\\n   ```\\n\\nNow you are ready to run VQL queries!\\n\\n:::note Acknowledgements\\nBig thanks to [Christoph Lobmeyer](https://github.com/lo-chr) who\\ncontributed the intricate expression that is behind the `--subscribe <artifact>`\\noption and wrote parts of this blog post. \ud83d\ude4f\\n:::"},{"id":"/five-design-principles-for-building-a-data-pipeline-engine","metadata":{"permalink":"/archive/five-design-principles-for-building-a-data-pipeline-engine","source":"@site/archive/five-design-principles-for-building-a-data-pipeline-engine/index.md","title":"Five Design Principles for Building a Data Pipeline Engine","description":"One thing we are observing is that organizations are actively seeking out","date":"2023-10-17T00:00:00.000Z","formattedDate":"October 17, 2023","tags":[{"label":"pipelines","permalink":"/archive/tags/pipelines"},{"label":"design","permalink":"/archive/tags/design"}],"readingTime":14.555,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Five Design Principles for Building a Data Pipeline Engine","authors":["mavam"],"date":"2023-10-17T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["pipelines","design"],"comments":true},"prevItem":{"title":"Integrating Velociraptor into Tenzir Pipelines","permalink":"/archive/integrating-velociraptor-into-tenzir-pipelines"},"nextItem":{"title":"We Need to Talk About the Cost of Security Operations Infrastructure","permalink":"/archive/we-need-to-talk-about-the-cost-of-security-operations-infrastructure"}},"content":"One thing we are observing is that organizations are actively seeking out\\nsolutions to better manage their security data operations. Until recently, they\\nhave been aggressively repurposing common data and observability tools. I\\nbelieve that this is a stop-gap measure because there was no alternative. But\\nnow there is a growing ecosystem of security data operations tools to support\\nthe modern security data stack. Ross Haleliuk\'s [epic\\narticle](https://ventureinsecurity.net/p/security-is-about-data-how-different)\\nlays this out at length.\\n\\nIn this article I am explaining the underlying design principles for developing\\nour own data pipeline engine, coming from the perspective of security teams that\\nare building out their detection and response architecture. These principles\\nemerged during design and implementation. Many times, we asked ourselves \\"what\'s\\nthe right way of solving this problem?\\" We often went back to the drawing board\\nand started challenging existing approaches, such as what a data source is, or\\nwhat a connector should do. To our surprise, we found a coherent way to answer\\nthese questions without having to make compromises. When things feel Just Right,\\nit is a good sign to have found the right solution for a particular problem.\\nWhat we are describing here are the lessons learned from studying other systems,\\ndistilled as principles to follow for others.\\n\\n![Five Design Principles](five-design-principles.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\nThis article comes from the data engineering perspective. What makes a data\\npipeline \\"security\\" is left for a future post. Technical SOC architects, CTOs,\\nor Principal Engineers our audience. Enjoy!\\n\\n## A Value-First Approach to Data Pipelines\\n\\nOne way to think of it is that data pipelines offer **data as a service**. The\\npipeline provides the data in the right shape, at the right time, at the right\\nplace. As a data consumer, I do not want to bother with how it got there. I just\\nwant to use it to solve my actual problem. That\'s the value.\\n\\nBut data has to jump through many hoops to get to this goldilocks state. The\\n*pyramid of data value* is an attempt to describe these hoops on a discrete\\nspectrum:\\n\\n![Pyramid of Data Value](pyramid-of-data-value.excalidraw.svg)\\n\\nThe idea we want to convey is that you begin with a massive amount of\\nunstructured data, which makes up for the bulk of data in many organizations.\\nOnce you lift it into a more digestible form by structuring (= parsing) it, you\\ninvest cycles and time to make it easier to extract value from it. A byproduct\\nis often that you end up with less data afterwards. You can continue the process\\nto reshape the data to the exact form to solve a given problem, which often\\naccounts for just a tiny fraction of the original dataset. Data is often still\\nstructured then, but you invest a lot of time in deduplicating, aggregating,\\nenriching, and more, just to get the needed information for the consumer. This\\ndata massaging can take a lot of human effort!\\n\\nIt\'s incredibly costly for security teams to spend 80% of their time massaging\\nthe data. They should spend their focus on executing their mission, which is\\nhunting threats to protect their constituency. It\'s certainly not building tools\\nto wrangle data.\\n\\nWe argue that making it easier for domain experts to work with data\\nfundamentally requires a solution that can seamlessly cross all layers of this\\npyramid, as the boundaries are the places where efficiency leaks. The following\\nprinciples allowed us to get there.\\n\\n## P1: Separation of Concerns\\n\\nA central realization for us was that different types of data need different\\nabstractions that coherently fit together. When following the data journey\\nthrough the pyramid, we start with unstructured data. This means working with\\nraw bytes. You may get them over HTTP, Kafka, a file, or any other carrier\\nmedium. The next step is translating bytes into structured data. Parsing gets\\nthe data onto the \\"reshaping highway\\" where it can be manipulated at ease in a\\nrich data model.\\n\\nIn our nomenclature, connectors are responsible for loading and\\nsaving raw, unstructured bytes; [formats](/formats) translate unstructured bytes\\ninto structured bytes; a rich set of computational [operators](/operators)\\nenable transformations on structured data.\\n\\nThe diagram below shows these key abstractions:\\n\\n![Connectors, Formats, Operators](connectors-formats-operators.excalidraw.svg)\\n\\nOne thing we miss looking at existing systems is a *symmetry* in these\\nabstractions. Data acquisition is often looked just one-way: getting data in and\\nthen calling it a day. No, please don\'t stop here! A structured data\\nrepresentation is easy to work with, but throwing the output as JSON over the\\nfence is not a one-size-fits-all solution. I may need CSV or YAML. I may want a\\nParquet file. I may need it in a special binary form. Deciding in what format to\\nconsume that data is critical for flexibility and performance. This is why we\\ndesigned our connectors and formats to be symmetric: a **loader** takes bytes\\nin, and a **saver** sends bytes out. A **parser** translates bytes to events,\\nand a **printer** translates them back.\\n\\nHere is an example that makes symmetric use of connectors, formats, and\\noperators:\\n\\n```\\nload gcs bucket/in/my/cloud/logs.zstd\\n| decompress zstd\\n| read json\\n| where #schema == \\"ocsf.network_activity\\"\\n| select connection_info.protocol_name,\\n         src_endpoint.ip,\\n         src_endpoint.port,\\n         dst_endpoint.ip,\\n         dst_endpoint.port\\n| to s3 bucket/in/my/other/cloud write parquet\\n```\\n\\nThis pipeline starts by taking compressed, unstructured data, then makes the\\ndata structured by parsing it as JSON, selects the connection 5-tuple, and\\nfinally writes it as a Parquet file into an S3 bucket.\\n\\n:::info Pipeline Languages Everywhere\\nNew pipeline languages are mushrooming all over the place. Splunk started with\\nits own Search Processing Language (SPL) and now released SPL2 to make it\\nmore pipeline-ish. And Elastic also doubled down as well with [their new\\nES|QL](/archive/a-first-look-at-esql). When we designed the Tenzir Query Language\\n(TQL), we drew a lot of inspiration from\\n[PRQL](https://prql-lang.org/), [Nu](https://www.nushell.sh/), and\\n[Zed](https://zed.brimdata.io/). These may sound esoteric to some, but they are\\nremarkably well designed evolutions of pipeline languages that are *not* SQL.\\nDon\'t get us wrong, we love SQL when we have our data engineering hat on, but\\nour users shouldn\'t have to wear that hat. Splunk gets a lot of flak for its\\npricing, but one thing we admire is how well it caters to a broad audience of\\nusers that are not data engineering wizards. Our slightly longer [FAQ\\narticle](/faqs) elaborates on this topic.\\n:::\\n\\n## P2: Typed Operators\\n\\nIn Tenzir [pipelines](/pipelines), we call the atomic building blocks\\n**operators**. They are sometimes also called \\"steps\\" or \\"commands\\". Data flows\\nbetween them:\\n\\n![Pipeline Chaining](pipeline-chaining.excalidraw.svg)\\n\\nMany systems, including Tenzir, distinguish three types of operators:\\n**sources** that produce data, **sinks** that consume data, and\\n**transformations** that do both.\\n\\n![Pipeline Structure](pipeline-structure.excalidraw.svg)\\n\\nMost pipeline engines support one type of data that flows between the operators.\\nIf they exchange raw bytes, they\'d be in the \\"unstructured\\" layer of the\\npyramid. If they exchange JSON or data frames, they\'d be in the \\"structured\\"\\nlayer.\\n\\nBut in order to cut through the pyramid above, Tenzir pipelines make one more\\ndistinction: the elements that operators push through the pipeline are\\n*typed*. Every operator has an input and an output type:\\n\\n![Input and Output Types](operator-pieces.excalidraw.svg)\\n\\nWhen composing pipelines, these types have to match. Otherwise the pipeline is\\nmalformed. Let\'s take the above pipeline example and zoom out to just the\\ntyping:\\n\\n![Visualization](load-decompress-select-to.excalidraw.svg)\\n\\nWith the concept of input and output types in mind, the operator type become\\nmore apparent:\\n\\n![Operator Types](operator-types.excalidraw.svg)\\n\\nThis is quite powerful, because you can also *undulate* between bytes and events\\nwithin a pipeline before it ends in void. Consider this example:\\n\\n```\\nload nic eth0\\n| read pcap\\n| decapsulate\\n| where src_ip !in [0.0.0.0, ::]\\n| write pcap\\n| zeek\\n| write parquet\\n| save file /tmp/zeek-logs.parquet\\n```\\n\\nThis pipeline starts with PCAPs, transforms the acquired packets to events,\\ndecapsulates them to filter on some packet headers, goes back to PCAP, runs\\nZeek[^1] on the filtered trace, and then writes the log as Parquet file to disk.\\n\\n[^1]: The `zeek` operator is user-defined operator for `shell \\"zeek -r - \u2026\\" |\\n    read zeek-tsv`. We wrote a [blog post on how you can use `shell` as escape\\n    hatch to integrate arbitrary\\n    tools](/archive/shell-yeah-supercharging-zeek-and-suricata-with-tenzir) in a\\n    pipeline.\\n\\nVisually, this pipeline has the following operator typing:\\n\\n![Undulating Pipeline](undulating-pipeline.excalidraw.svg)\\n\\n## P3: Multi-Schema Dataflows\\n\\nTo further unlock value within the structured data layer of the pyramid, we made\\nour pipelines **multi-schema**: a single pipeline can process heterogeneous\\ntypes of events, each of which have their own schemas. Multi-schema dataflows\\nrequire automatic schema inference at parse time, which all our parsers support.\\n\\nThis behavior is different from engines that work with structured data where\\noperators typically work with fixed set of tables. While schema-less systems,\\nsuch as document-oriented databases, offer more simplicity, their\\none-record-at-a-time processing comes at the cost of performance. In the\\nspectrum of performance and ease of use, Tenzir therefore fills a\\ngap:\\n\\n![Structured vs. Document-Oriented](structured-vs-document-oriented.excalidraw.svg)\\n\\n:::info Eclectic & Super-structured Data\\n[Zed](https://amyousterhout.com/papers/zed_cidr23.pdf) has a type system similar\\nto Tenzir, with the difference that Zed associates types *with every single\\nvalue*. Unlike Zed, Tenzir uses a \\"data frame\\" abstraction and relies on\\nhomogeneous Arrow record batches of up to 65,535 rows.\\n:::\\n\\nIf the schema in a pipeline changes, we simply create a new batch of events. The\\nworst case for Tenzir is a ordered stream of schema-switching events, with every\\nevent having a new schema than the previous one. That said, even for those data\\nstreams we can efficiently build homogeneous batches when the inter-event order\\ndoes not matter significantly. Similar to predicate pushdown, Tenzir operators\\nsupport \\"ordering pushdown\\" to signal to upstream operators that the event order\\nonly matters intra-schema but not inter-schema. In this case we transparently\\ndemultiplex a heterogeneous stream into *N* homogeneous streams, each of which\\nyields batches of up to 65k events. The `import`\\noperator is an example of such an operator, and it pushes its ordering upstream\\nso that we can efficiently parse, say, a diverse stream of NDJSON records, such\\nas Suricata\'s EVE JSON or Zeek\'s streaming JSON.\\n\\nYou could call multi-schema dataflows *multiplexed* and there exist dedicated\\noperators to demultiplex a stream. As of now, this is hard-coded per operator.\\nFor example, `to directory /tmp/dir write parquet`\\ndemultiplexes a stream of events so that batches with the same schema go to the\\nsame Parquet file.\\n\\nThe diagram below illustrates the multi-schema aspect of dataflows for schemas\\nA, B, and C:\\n\\n![Multi-schema Example](multi-schema-example.excalidraw.svg)\\n\\nSome operators only work with exactly one instance per schema internally, such\\nas `write` when combined with the `parquet`, `feather`, or `csv` formats. These\\nformats cannot handle multiple input schemas at once. A demultiplexing operator\\nlike `to directory .. write <format>` removes this limitation by writing one\\nfile per schema instead.\\n\\nWe are having ideas to make this schema (de)multiplexing explicit with a\\n`per-schema` operator modifier that you can write in front of every operator.\\nSimilarly, we are going to add union types in the future, making it possible to\\nconvert a heterogeneous stream of structured data into a homogeneous one.\\n\\nIt\'s important to note that most of the time you don\'t have to worry about\\nschemas. They are there for you when you want to work with them, but it\'s often\\nenough to just specified the fields that you want to work with, e.g., `where\\nid.orig_h in 10.0.0.0/8`, or `select src_ip, dest_ip, proto`. Schemas are\\ninferred automatically in parsers, but you can also seed a parser with a schema\\nthat you define explicitly.\\n\\n## P4: Unified Live Stream Processing and Historical Queries\\n\\nSystems for stream processing and running historical queries have different\\nrequirements, and combining them into a single system can be a daunting\\nchallenge. But there is an architectural sweetspot at the right level of\\nabstraction where you can elegantly combine them. From a user persepctive, our\\ngoal was to seamlessly exchange the beginning of a pipeline to select the source\\nof the data, be it a historical or continuous one:\\n\\n![Unified Processing](unified-processing.excalidraw.svg)\\n\\nOur desired user experience for interacting with historical looks like this:\\n\\n1. **Ingest**: to persist data at a node, create a pipeline that ends with the\\n   `import` sink.\\n2. **Query**: to run a historical query, create a pipeline that begins with the\\n   `export` operator.\\n\\nFor example, to ingest JSON from a Kafka, you write `from kafka --topic foo |\\nimport`. To query the stored data, you write `export | where file == 42`. The\\nlatter example suggests that the pipeline *first* exports everything, and only\\n*then* starts filtering with `where`, performing a full scan over the stored\\ndata. But this is not what\'s happening. Our pipelines support **predicate\\npushdown** for every operator. This means that `export` receives the filter\\nexpression before it starts executing, enabling index lookups or other\\noptimizations to efficiently execute queries with high selectivity where scans\\nwould be sub-optimal.\\n\\nThe central insight here is to ensure that predicate pushdown (as well as other\\nforms of signalling) exist throughout the entire pipeline engine, and that the\\nengine can communicate this context to the storage engine.\\n\\nOur own storage engine is not a full-fledged database, but rather a thin\\nindexing layer over a set of Parquet/Feather files. The sparse indexes (sketch\\ndata structures, such as min-max synopses, Bloom filters, etc.) avoid full scans\\nfor every query. The storage engine also has a *catalog* that tracks evolving\\nschemas, performs expression binding, and provides a transactional interface to\\nadd and replace partitions during compaction.\\n\\nThe diagram below shows the main components of the database engine:\\n\\n![Database Architecture](database-architecture.excalidraw.svg)\\n\\nBecause of this transparent optimization, you can just exchange the source of a\\npipeline and switch between historical and streaming execution without degrading\\nperformance. A typical use case begins some exploratory data analysis involving\\na few `export` pipelines, but then would deploy the pipeline on streaming data\\nby exchanging the source with, say, `from kafka`.\\n\\nThe difference between `import` and `from file <path> read parquet` (or `export`\\nand `to file <path> write parquet`) is that the storage engine has the extra\\ncatalog and indexes, managing the complexity of dealing with a large set of raw\\nParquet files.\\n\\n:::info Delta, Iceberg, and Hudi?\\nWe kept the catalog purposefully simple to iterate fast and gain experience in a\\ncontrolled system, rather than starting Lakehouse-grade with [Delta\\nLake](https://delta.io/), [Iceberg](https://iceberg.apache.org/), or\\n[Hudi](https://hudi.apache.org/). We are looking forward to having the resources\\nto integrate with the existing lake management tooling.\\n:::\\n\\n## P5: Built-in Networking to Create Data Fabrics\\n\\n:::info Control Plane vs. Data Plane\\nThe term *data fabric* is woven into many meanings. From a Tenzir perspective,\\nthe set of interconnected pipelines through which data flows constitutes the\\n**data plane**, whereas the surrounding management platform at\\n[app.tenzir.com](https://app.tenzir.com) to control the nodes constitute the\\n**control plane**. When we refer to \\"data fabric\\" we mean to the data plane\\naspect.\\n:::\\n\\nTenzir pipelines have built-in network communication, allowing you to create a\\ndistributed fabric of dataflows to express intricate use cases. There are two\\ntypes of network connections: *implicit* and *explicit* ones:\\n\\n![Implicit vs. Explicit](implicit-vs-explicit-networking.excalidraw.svg)\\n\\nAn implicit network connection exists, for example, when you use the `tenzir`\\nbinary on the command line to run a pipeline that ends in\\n`import`:\\n\\n```bash\\ntenzir \'load gcs bkt/eve.json\\n       | read suricata\\n       | where #schema != \\"suricata.stats\\"\\n       | import\\n       \'\\n```\\n\\nThis results in the following pipeline execution:\\n\\n![Import Networking](import-networking.excalidraw.svg)\\n\\nA historical query, like `export | where <expr> | to <connector>`, has the\\nnetwork connection at the other end:\\n\\n![Export Networking](export-networking.excalidraw.svg)\\n\\nTenzir pipelines are eschewing networking to minimize latency and maximize\\nthroughput. So we generally transfer ownership of operators between processes as\\nlate as possible to prefer local, high-bandwidth communication. For maximum\\ncontrol over placement of computation, you can override the automatic operator\\nlocation with the `local` and `remote` operator modifiers.\\n\\nThe above examples are implicit network connections because they\'re not visible\\nin the pipeline definition. An explicit network connection terminates a pipeline\\nas source or sink:\\n\\n![Pipeline Fabric](pipeline-fabric.excalidraw.svg)\\n\\nThis fictive data fabric above consists of a heterogeneous set of technologies,\\ninterconnected by pipelines. You can also turn any pipeline into an API using\\nthe `serve` sink, effectively creating a dataflow microservice that you can\\naccess with a HTTP client from the other side:\\n\\n![Serve Operator](serve.excalidraw.svg)\\n\\nBecause you have full control over the location where you run the pipeline, you\\ncan push it all the way to the \\"last mile.\\" This helps especially when there\\nare compliance and data residency concerns that must be properly addressed.\\n\\n## Summary\\n\\nWe\'ve presented for design principles that we found to be key enabler to extract\\nvalue out of data pipelines:\\n\\n1. Separating the different data processing concerns, it is possible to\\n   achieve high modularity and composability. Tenzir therefore has connectors,\\n   formats, and operators as central processing building blocks.\\n2. Typed operators make it possible to process multiple types of data in the\\n   same engine, avoiding the need to switch tools just because the pipeline\\n   engine has a narrow focus.\\n3. Multi-schema dataflows give us the best of structured and document-oriented\\n   engines. Coupled with schema inference, this creates a user experience where\\n   schemas are optional, but still can be applied when strict typing is needed.\\n4. Unifying live and historical data processing is the holy grail to covering a\\n   wide variety of workloads. Our engine offers a new way to combine the two\\n   with an intuitive language.\\n5. Built-in networking makes it possible to create data fabrics at ease.\\n   Spanning pipelines across multiple nodes, either implicitly or explicitly\\n   (via ZeroMQ, Kafka, AMQP, etc.), provides a powerful mechanism to meet the\\n   most intricate architectural requirements.\\n\\nTenzir pipelines embody all of these principles. Try it yourself with our free\\nCommunity Edition at [app.tenzir.com](https://app.tenzir.com)."},{"id":"/we-need-to-talk-about-the-cost-of-security-operations-infrastructure","metadata":{"permalink":"/archive/we-need-to-talk-about-the-cost-of-security-operations-infrastructure","source":"@site/archive/we-need-to-talk-about-the-cost-of-security-operations-infrastructure/index.md","title":"We Need to Talk About the Cost of Security Operations Infrastructure","description":"In today\'s digital age, businesses are under immense pressure to bolster their","date":"2023-09-21T00:00:00.000Z","formattedDate":"September 21, 2023","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"pipelines","permalink":"/archive/tags/pipelines"},{"label":"siem","permalink":"/archive/tags/siem"},{"label":"cost","permalink":"/archive/tags/cost"},{"label":"dataops","permalink":"/archive/tags/dataops"},{"label":"secdataops","permalink":"/archive/tags/secdataops"},{"label":"finops","permalink":"/archive/tags/finops"}],"readingTime":4.15,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"authors":"oliverrochford","date":"2023-09-21T00:00:00.000Z","tags":["tenzir","pipelines","siem","cost","dataops","secdataops","finops"],"comments":true},"prevItem":{"title":"Five Design Principles for Building a Data Pipeline Engine","permalink":"/archive/five-design-principles-for-building-a-data-pipeline-engine"},"nextItem":{"title":"A First Look at ES|QL","permalink":"/archive/a-first-look-at-esql"}},"content":"In today\'s digital age, businesses are under immense pressure to bolster their\\ncybersecurity. Understanding the financial implications of security tools is\\nvital to ensure optimal ROI through risk reduction and breach resilience. This\\nis particularly true for consumption-based security solutions like Security\\nInformation and Event Management (SIEM).\\n\\n![capex-vs-opex](capex-vs-opex.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## The SIEM Cost Challenge\\n\\nHaving worked both as a SIEM consultant and an industry analyst, I\'ve observed\\nthat SIEM costs are a recurring point of contention. As a consequence, SIEM\\nsolutions, the cornerstone of many security operations programs, have garnered a\\nreputation for being a financial black hole.\\n\\nAccurately forecasting SIEM costs has remained elusive. Given that a typical\\n[SIEM takes over 6 months to deploy][panther], predicting its data consumption a\\nyear in advance is speculative. Even vendors sometimes fall short in providing\\nrealistic cost estimates.\\n\\n[panther]: https://panther.com/wp-content/uploads/2023/01/State-of-SIEM-2021.pdf\\n\\nSeveral factors contribute to this unpredictability: outdated benchmark data,\\nscope changes, the evolving threat landscape, and rapid digitalization. These\\nvariables often lead to unforeseen price hikes post-deployment, catching many\\nbuyers off guard.\\n\\n## SIEM Pricing: Outdated and Ineffective\\n\\nTraditional SIEM pricing models haven\'t evolved in tandem with the explosion in\\ndata volumes. While security teams now handle data ranging from Megabytes to\\nPetabytes, SIEM licensing remains anchored in a Gigabyte-centric world.\\n\\nMoreover, the true value of the vast amounts of security data remains ambiguous.\\nUntil a breach or intrusion is investigated, it\'s challenging to determine the\\nsignificance of the collected data. While the probability that the data is still\\nrequired decreases over time, we can\u2019t always be sure when it will reach 0.\\n\\n## Quantifying the Value of Security Data\\n\\nThe amount of security data an organization generates doesn\'t always correlate\\nwith its revenue. This discrepancy complicates the task of assessing the\\nbusiness value of security data. Survey data further highlights a disconnect\\nbetween the perceived and actual value of SIEM systems, with many organizations\\nlamenting rising costs and underwhelming features\\n\\nSecurity teams are grappling with [rising costs][techresearch], [underutilized\\nfeatures][panther], and a [lack of comprehensive\\ncoverage][cardinalops], with one study stating that [over 40% think they are\\noverpaying for their SIEM, and more than 50% unhappy with their current SIEM\\nproviders][panther].\\n\\n[techresearch]: https://techresearchonline.com/wp-content/uploads/2022/03/SIEM_Shift_How_the_Cloud_is_Transforming_Security_Operations_US_20211007.pdf\\n[cardinalops]: https://f.hubspotusercontent00.net/hubfs/7289101/CardinalOps%20Quantifying%20the%20Threat%20Coverage%20Gap.pdf\\n\\n## A Shift in Buyer Behavior\\n\\nRather than just endlessly expanding security budgets to combat escalating\\ncosts, organizations are adopting various strategies:\\n\\n- **Limiting Coverage**: Some are narrowing their security monitoring scope,\\n  focusing primarily on compliance. However, this approach can compromise threat\\n  visibility and increase vulnerability.\\n- **Adopting XDR**: Others are transitioning to Extended Detection and Response\\n  (XDR) solutions, prioritizing in-depth analysis over breadth. But as XDR gains\\n  traction, it may inherit SIEM\'s cost challenges.\\n- **Building Security Data Lakes**: These are becoming increasingly popular due\\n  to their cost-effectiveness and advanced analytical capabilities. However,\\n  transitioning to a data lake doesn\'t guarantee reduced consumption. Many\\n  organizations will find they are swapping Capex for Opex. Moreover, while data\\n  lakes offer certain advantages, they can\'t fully replace enterprise SIEMs.\\n\\n## Future-proofing Security Operations for Automation and AI\\n\\nEven with the improved cost efficiencies and economies of scale achieved by\\nusing security data lakes and cloud computing, we are beginning to hit\\naffordability limits again.\\n\\nThe integration of new data-intensive tools and technologies, including machine\\nlearning and AI, like large language models, further intensifies this demand.\\nWhile these advances promise enhanced cybersecurity capabilities, they\\nsimultaneously usher in a new set of financial challenges that the industry will\\nhave to grapple with. Technological advancements have made it feasible to\\nprocess vast data troves, but the question remains: is it economical?\\n\\nFinding the precarious balance between achieving cost efficiencies and\\nmaintaining robust security resilience is the conundrum facing cybersecurity\\nleaders. What they need to be able to make informed decisions is a comprehensive\\nunderstanding of these costs and their implications, so that they can\\nstrategically navigate these challenges.\\n\\n## Security FinOps with Tenzir Security Data Pipelines\\n\\nAt Tenzir, we aim to redefine how organizations manage security operations\\nexpenses. Our security data pipelines address core challenges associated with\\noptimizing SIEM, security data lake, and cloud costs.\\n\\nOur pipelines enhance data flow and processing by normalizing data formats to\\nreduce complexity and redundancy, performing in-stream enrichments, and applying\\npowerful reshaping to optimally prepare the data for consumption. By optimizing\\ndata preprocessing down to the collection point, we curtail unnecessary SIEM\\ningestion and cloud compute costs. We transfer many workloads to the edge that\\nwere previously cost-inefficiently executed centrally. By scaling vertically\\nacross cores and pipelines, and horizontally across nodes, organizations can\\nadapt to variable environments and data loads, ensuring deployment flexibility\\nand cost-efficiency.\\n\\nFurthermore, Tenzir ensures data quality, a vital component for effective\\nDataOps and automation. By filtering out redundant data and prioritizing based\\non significance, you can ensure efficient resource allocation. Tenzir\'s\\ninstrumented data flows provide clear insights into data usage, facilitating\\ntransparent cost benchmarking.\\n\\nDiscover more about our features and benefits in our [solution\\nbrief](https://tenzir.com/solution-brief.pdf) and free whitepaper on\\n[optimizing SIEM, Cloud and data costs using\\nTenzir](https://tenzir.com/whitepaper.pdf).\\n\\nStart using Tenzir right away at [app.tenzir.com](https://app.tenzir.com)."},{"id":"/a-first-look-at-esql","metadata":{"permalink":"/archive/a-first-look-at-esql","source":"@site/archive/a-first-look-at-esql/index.md","title":"A First Look at ES|QL","description":"Elastic just released their new pipeline query language called","date":"2023-08-29T00:00:00.000Z","formattedDate":"August 29, 2023","tags":[{"label":"esql","permalink":"/archive/tags/esql"},{"label":"elastic","permalink":"/archive/tags/elastic"},{"label":"tql","permalink":"/archive/tags/tql"},{"label":"kusto","permalink":"/archive/tags/kusto"},{"label":"splunk","permalink":"/archive/tags/splunk"},{"label":"spl","permalink":"/archive/tags/spl"},{"label":"pipelines","permalink":"/archive/tags/pipelines"},{"label":"language","permalink":"/archive/tags/language"}],"readingTime":7.35,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A First Look at ES|QL","authors":"mavam","date":"2023-08-29T00:00:00.000Z","last_update":"2023-12-12T00:00:00.000Z","tags":["esql","elastic","tql","kusto","splunk","spl","pipelines","language"],"comments":true},"prevItem":{"title":"We Need to Talk About the Cost of Security Operations Infrastructure","permalink":"/archive/we-need-to-talk-about-the-cost-of-security-operations-infrastructure"},"nextItem":{"title":"Slash Your SIEM, Cloud, and Data Costs with Tenzir Security Data Pipelines","permalink":"/archive/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines"}},"content":"Elastic [just released][esql-blog] their new pipeline query language called\\n**ES|QL**. This is a conscious attempt to consolidate the language zoo in the\\nElastic ecosystem\\n([queryDSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html),\\n[EQL](https://www.elastic.co/guide/en/elasticsearch/reference/current/eql.html),\\n[KQL](https://www.elastic.co/guide/en/kibana/current/kuery-query.html),\\n[SQL](https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-sql.html),\\n[Painless](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html),\\n[Canvas/Timelion](https://www.elastic.co/guide/en/kibana/current/timelion.html)).\\nElastic said that they worked on this effort for over a year. The\\n[documentation][esql-docs] is still sparse, but we still tried to read between\\nthe lines to understand what this new pipeline language has to offer.\\n\\n[esql-blog]: https://www.elastic.co/blog/elasticsearch-query-language-esql\\n[esql-docs]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql.html\\n\\n![ESQL](esql.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\nThe reason why we are excited about this announcement is because we have *also*\\ndesigned and implemented a pipeline language over the past 8 months that we just\\n[launched at BlackHat](/archive/introducing-tenzir-security-data-pipelines). First,\\nwe see the release of ES|QL as a confirmation for pipeline model. In our [blog\\npost about Splunk\'s Search Processing Language\\n(SPL)](/archive/tenzir-for-splunk-users), we briefly mentioned why SQL might not be\\nthe best choice for analysts, and why thinking about one operator at a time\\nprovides an easier user experience. Second, we\'d like to look under the hood of\\nES|QL to compare and reflect on our own Tenzir Query Language (TQL).\\n\\n## Walk-through by Example\\n\\nSo, ES|QL, how does it feel?\\n\\n```\\n  FROM employees\\n| EVAL hired_year = TO_INTEGER(DATE_FORMAT(hire_date, \\"YYYY\\"))\\n| WHERE hired_year > 1984\\n| STATS avg_salary = AVG(salary) BY languages\\n| EVAL avg_salary = ROUND(avg_salary)\\n| EVAL lang_code = TO_STRING(languages)\\n| ENRICH languages_policy ON lang_code WITH lang = language_name\\n| WHERE NOT IS_NULL(lang)\\n| KEEP avg_salary, lang\\n| SORT avg_salary ASC\\n| LIMIT 3\\n```\\n\\nThis syntax reads very straight-forward. Splunk users will immediately grasp\\nwhat it does, as there is a remarkable similarity in operator naming. Let\'s go\\nthrough each pipeline operator individually:\\n\\n- [`FROM`][esql-from] generates a table with up to 10k rows from a data stream,\\n  index, or alias. We asked ourselves why there is a hard-baked 10k limit?\\n  Shouldn\'t that be the job of [`LIMIT`][esql-limit]? The limit feels a\\n  technical limitation rather than a conscious design decision. In TQL, we have\\n  unbounded streams but also follow the single responsibility principle: one\\n  operator has exactly one job.\\n- [`EVAL`][esql-eval] appends new or replaces existing columns. We named this\\n  operator `extend` because we found the\\n  Splunk-inspired command name \\"eval\\" too generic for this use case.[^1]\\n- [`WHERE`][esql-where] filters the input with an expression. We have the same\\n  `where` in TQL.\\n- [`STATS`][esql-stats] groups its input via `BY` and applies aggregation\\n  functions on select fields of each group.  Elastic went with Splunk\\n  nomenclature for this central operation, perhaps also to make the transition\\n  from Splunk to Elastic as easy as possible.\\n- [`ENRICH`][esql-enrich] adds data from existing indexes. It\'s effectively a\\n  join operation, and the `ON` keywords makes it possible to select the join\\n  field. Interestingly, the word \\"join\\" doesn\'t appear on the documentation. We\\n  hypothesize that this was a conscious choice, as a database join may feel\\n  intimidating for beginning and intermediate users.\\n- [`KEEP`][esql-keep] selects a set of columns from the input and drops all\\n  others. It is the inverse of [`DROP`][esql-drop]. In TQL, we call these\\n  projection operators `select` and also `drop`.\\n- [`SORT`][esql-sort]\\n  sorts rows by one or more fields. `SORT height DESC, first_name ASC` sorts the\\n  field `height` in descending order and the field `first_name` in ascending\\n  order. The syntax of our `sort` is identical.\\n  Controlling the position of null values works with `NULLS FIRST` and `NULLS\\n  LAST`. In TQL, we went Kusto-like with `nulls-first` and `nulls-last`.\\n- [`LIMIT`][esql-limit] restricts the number of output rows. In TQL, we have\\n  `head` and `tail` for this purpose.\\n\\n[esql-from]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-from.html\\n[esql-eval]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-eval.html\\n[esql-where]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-where.html\\n[esql-stats]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-stats-by.html\\n[esql-enrich]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-enrich.html\\n[esql-keep]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-keep.html\\n[esql-drop]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-drop.html\\n[esql-sort]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-sort.html\\n[esql-limit]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-limit.html\\n[^1]: We took the name `extend` from Kusto. In general, we find that Kusto has\\n    very self-descriptive operator names. During the design of TQL, we compared\\n    many different languages and often favored Kusto\'s choice of name.\\n\\n## Sources, Transformations, ... but Sinks?\\n\\nES|QL differentiates two types of commands (which we call *operators* in TQL):\\n\\n1. [Source commands](https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-source-commands.html)\\n2. [Processing commands](https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-processing-commands.html)\\n\\nIn TQL, an operator is a *source*, a *transformation*, or a *sink*. Some\\noperators can be of multiple categories, like `shell`.\\n\\nMaybe this is still coming, but ES|QL doesn\'t appear to offer sinks. We\\nhypothesize that users should consume pipeline output uniformly as JSON through\\na REST API.\\n\\n## Syntax\\n\\nSyntactically, the [ES|QL language][esql-syntax] is similar to TQL. The\\nfollowing points stood out:\\n\\n[esql-syntax]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-syntax.html\\n\\n- The `|` (pipe) symbol separates commands that describe the dataflow.\\n- Comments work as in C++: `//` for single line and `/*` and `*/` for multi-line\\n  comments.\\n- Expressions can occur in `WHERE`, `STATS`, and other commands. The following\\n  relational operators exist:\\n  - Arithmetic comparisons via `<`, `<=`, `==`, `>=`, `>`\\n  - Set membership via `IN`\\n  - Glob-like wildcard search via `LIKE`\\n  - Regular expressions via `RLIKE`\\n- Date-time literals make it easier to express dates (`seconds`, `hours`, etc.)\\n  and timespans (e.g., `1 day`). We found that expressing numeric values across\\n  multiple orders of magnitude is common, e.g., when dealing with GBs. This is\\n  why we also offer SI literals in TQL, allowing you to write large numbers as\\n  `1 Mi` or `1 M`.\\n- ES|QL features multiple scalar [functions][esql-funcs].\\n  that perform value-to-value transformations. Functions can occur in `ROW`,\\n  `EVAL`, and `WHERE`.\\n- Similarly, [aggregation functions][esql-agg-funcs] perform a vector-to-scalar\\n  transformation per group in `STATS`.\\n\\n[esql-funcs]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-functions.html\\n[esql-agg-funcs]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-agg-functions.html\\n\\n## Engine\\n\\nES|QL comes with its own executor, i.e., it\'s not transpiled into any of the\\nexisting engines. A running pipelines is a *task* and there exists an\\n[API][esql-api] for querying their state, which may return something like:\\n\\n[esql-api]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-task-management.html\\n\\n```json\\n{\\n  \\"node\\" : \\"2j8UKw1bRO283PMwDugNNg\\",\\n  \\"id\\" : 5326,\\n  \\"type\\" : \\"transport\\",\\n  \\"action\\" : \\"indices:data/read/esql\\",\\n  \\"description\\" : \\"FROM test | STATS MAX(d) by a, b\\",\\n  \\"start_time\\" : \\"2023-07-31T15:46:32.328Z\\",\\n  \\"start_time_in_millis\\" : 1690818392328,\\n  \\"running_time\\" : \\"41.7ms\\",\\n  \\"running_time_in_nanos\\" : 41770830,\\n  \\"cancellable\\" : true,\\n  \\"cancelled\\" : false,\\n  \\"headers\\" : { }\\n}\\n```\\n\\n## Data Model\\n\\nThe concept of [multi-valued fields][esql-mv-fields] exists to bridge the world\\nbetween JSON records and 2D tables. This shows the heritage of the type system,\\nwhich evolved from document stores as opposed to structured data stores. In\\ndocument land, every record may have a different shape (or schema). The term\\n*multi-valued* effectively means *list*, e.g., `[1, 2, 3]`.\\n\\n[esql-mv-fields]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-multivalued-fields.html\\n\\nNoteworthy:\\n\\n- The order of multi-valued fields is undefined.\\n- It\'s possible to impose set semantics by using the `keyword` type. Specifying\\n  this type causes duplicate removal on ingest.\\n- Other types, like `long`, do not cause removal of duplicates on ingest.\\n\\nThe output is still semi-structured in that listness is something dynamic on a\\nper-value basis. Consider this output:\\n\\n```json\\n{\\n  \\"columns\\": [\\n    { \\"name\\": \\"a\\", \\"type\\": \\"long\\"},\\n    { \\"name\\": \\"b\\", \\"type\\": \\"long\\"}\\n  ],\\n  \\"values\\": [\\n    [1, [1, 2]],\\n    [2,      3]\\n  ]\\n}\\n```\\n\\nThe column `b` has the list value `[1, 2]` in the first row and `3` in the\\nsecond. In a strict type system (like TQL), the type of `b` could be\\n`list<long>` but then the second row would have value `[3]` instead of `3`. Sum\\ntypes (called `union` or `variant` in many languages) are another way to\\nrepresent heterogeneous data as in the above example. If we described `b` with\\nthe type `union<long, list<long>>` instead of `long`, then it would be perfectly\\nfine for `b` to take one value `[1, 2]` in one row and `3` in another.\\n\\nFor TQL, we built our data model on top of data frames. We express structure in\\nterms of *records* and *lists*, and arbitrarily nested combinations of them. It\\nwould be up the user to define set semantics that ensures unique values. We\\nconsider adding such a set type in the future (possible as type constraint or\\nattribute) as we gain more complete support of the underlying Arrow type system.\\nSimilarly, we plan on adding sum types in the future.\\n\\n## Summary\\n\\nThe release of ES|QL witnesses a current trend of convergence in terms of query\\nlanguages. The pipeline concept now exists for several decades. Splunk was the\\nfirst company to successfully commercialize this interface with SPL, but today\\nthere are many players in the market that have a similar language. Microsoft\\nopen-sourced their Kusto language, and we see other vendors embedding it into\\ntheir products, such as Cribl Search. Most SIEM vendors also have their own\\ninhouse pipeline language.\\n\\nThe data ecosystem has numerous languages for advanced users to offer, such as\\n[dplyr](https://dplyr.tidyverse.org/), [jq](https://stedolan.github.io/jq/),\\n[pandas](https://pandas.pydata.org/), and [polars](https://www.pola.rs/). And\\nnew ones are mushrooming everywhere, e.g., [PRQL](https://prql-lang.org/),\\n[Zed](https://zed.brimdata.io/).\\n\\nWith our own TQL, we seek to bridge the data and security analytics world, by\\noffering an intuitive language that is easy to grasp, but that internally maps\\nto vectorized execution on top of data frames that can be easily shared with other\\nruntimes.\\n\\nIf you want to look deeper at ES|QL, check out the branch\\n[`feature/esql`][esql-branch]. Find something interesting about pipelines to\\ndiscuss? Swing by our [Discord](/discord) and start a conversation.\\n\\n[esql-branch]: https://github.com/elastic/elasticsearch/tree/feature/esql/x-pack/plugin/esql"},{"id":"/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines","metadata":{"permalink":"/archive/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines","source":"@site/archive/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines/index.md","title":"Slash Your SIEM, Cloud, and Data Costs with Tenzir Security Data Pipelines","description":"Staying ahead in the realm of cybersecurity means relentlessly navigating an","date":"2023-08-17T00:00:00.000Z","formattedDate":"August 17, 2023","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"pipelines","permalink":"/archive/tags/pipelines"},{"label":"siem","permalink":"/archive/tags/siem"},{"label":"cost","permalink":"/archive/tags/cost"}],"readingTime":1.105,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"authors":"oliverrochford","date":"2023-08-17T00:00:00.000Z","tags":["tenzir","pipelines","siem","cost"],"comments":true},"prevItem":{"title":"A First Look at ES|QL","permalink":"/archive/a-first-look-at-esql"},"nextItem":{"title":"Introducing Tenzir Security Data Pipelines","permalink":"/archive/introducing-tenzir-security-data-pipelines"}},"content":"Staying ahead in the realm of cybersecurity means relentlessly navigating an\\nendless sea of emerging threats and ever-increasing data volumes. The battle to\\nstay one step ahead can often feel overwhelming, especially when your\\norganization\'s data costs are skyrocketing.\\n\\n![Slash your Costs](slash-your-costs.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n**Imagine you can simultaneously cut costs, improve security, and optimize your\\ndetection and response infrastructure.**\\n\\nIntroducing our latest whitepaper: *Slashing SIEM, Cloud, and Data Costs with\\nTenzir*\\n\\nBy reading this whitepaper, you will learn how Tenzir security data pipelines\\nmake your cybersecurity infrastructure more cost-effective and robust, and why\\nyou should consider leveraging them to future proof your security data\\noperations.\\n\\n**What you\'ll find inside:**\\n* A clear breakdown of the challenges around skyrocketing SIEM, cloud and data\\n  costs (and why they\'re eating into your security budget).\\n* An expert\'s take on why solid security data management is your secret weapon\\n  against evolving cyber threats.\\n* A sneak peek at [Security Data Operations\\n  (SecDataOps)](https://tenzir.com/secdataops?utm_source=Blog)\u2014a fresh approach\\n  to reduce data costs and data wrangling, while maximizing data utility.\\n* A detailed look at how Tenzir\'s clever security data pipelines make data\\n  filtering, reduction, and deduplication a breeze.\\n\\nEquip yourself with the knowledge to transform your security data operations.\\nDownload Tenzir\'s whitepaper today and step into the future of cybersecurity\\nwith confidence.\\n\\n<div align=\\"center\\">\\n  <a class=\\"button button--primary\\" href=\\"https://tenzir.com/whitepaper.pdf\\">Download Now</a>\\n</div>"},{"id":"/introducing-tenzir-security-data-pipelines","metadata":{"permalink":"/archive/introducing-tenzir-security-data-pipelines","source":"@site/archive/introducing-tenzir-security-data-pipelines/index.md","title":"Introducing Tenzir Security Data Pipelines","description":"We\'re overjoyed to announce our highly-anticipated security data pipeline","date":"2023-08-09T00:00:00.000Z","formattedDate":"August 9, 2023","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"pipelines","permalink":"/archive/tags/pipelines"}],"readingTime":4.3,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"title":"Introducing Tenzir Security Data Pipelines","authors":"oliverrochford","date":"2023-08-09T00:00:00.000Z","tags":["tenzir","pipelines"],"comments":true},"prevItem":{"title":"Slash Your SIEM, Cloud, and Data Costs with Tenzir Security Data Pipelines","permalink":"/archive/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines"},"nextItem":{"title":"Tenzir for Splunk Users","permalink":"/archive/tenzir-for-splunk-users"}},"content":"We\'re overjoyed to [announce][pr] our highly-anticipated security data pipeline\\nplatform at the renowned BlackHat conference in Las Vegas. The launch marks a\\nmilestone in our journey to bring simplicity to data engineering for\\ncybersecurity operations, and to bring a cost-efficient way to tackle the\\nincreasingly complex data engineering challenges that security teams confront\\ndaily.\\n\\n[pr]: https://tenzir.com/press/tenzir-launches-security-data-pipeline-platform?utm_source=Blog&utm_campaign=launch\\n\\n![Tenzir Launch](tenzir-launch.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Security Data Operations for the Automation Age\\n\\nThe volume of data that needs to be collected, analyzed, and stored by security\\nteams has skyrocketed. Traditional security operations tools are increasingly\\noverwhelmed, leading to an urgent need for more efficient and effective\\nsolutions. Tenzir addresses this challenge head-on, simplifying data management\\nso that security teams can focus more on identifying and mitigating threats.\\n\\nIn the words of our CEO and founder, [Matthias\\nVallentin](https://www.linkedin.com/in/matthias-vallentin/), \\"To survive in\\ntoday\'s unforgiving threat landscape you need fast, near real-time *and*\\nextensive historical data. Tenzir pipelines help security teams speed up and\\nsimplify managing the data they need, so that they can spend more time doing\\nwhat is most crucial\u2014hunting threats.\\"\\n\\n## Why Data Pipelines for Security?\\n\\nCybersecurity has become an increasingly data-driven field. From network traffic\\nto cloud telemetry, the amount of information that security teams need to\\nanalyze is staggering. A single security incident can generate billions of data\\npoints that need to be reviewed, analyzed, and actioned. Traditional methods of\\ncollecting, aggregating, and analyzing this data are not just insufficient, they\\nare obsolete, leading to security gaps and cost inefficiencies.\\n\\nNavigating the modern security data stack is no small feat. Today\'s security\\nteams are faced not just with the management of a horde of advanced security\\nsolutions including SIEM, SOAR, UEBA, and threat intelligence platforms, but\\nalso the challenge of integrating these systems with diverse data technologies,\\nsuch as databases, data lakes, and data warehouses. Compounding this complexity\\nis a growing reliance on cloud microservices and increasingly AI services. This\\nhas made security operations less of a routine process and more a strategic\\nexercise in continuously mastering emergent complexities and optimizing\\nperformance.\\n\\nThis is where Tenzir\'s security data pipelines come in. Our unique platform\\ninstigates a shift from centralized security information and event management to\\na more adaptive and decentralized operating model more aligned with DevOps and\\ndata engineering principles: security data operations\\n([SecDataOps](https://tenzir.com/secdataops?utm_source=Blog)). It transcends\\nmere collection of events and logs, instead building resilient and robust data\\nflows that optimize data for further use, whether for detection and correlation,\\nthreat hunting, or machine learning. Data pipelines are already common in data\\nengineering and DevOps. They are designed to provide a seamless, efficient, and\\nflexible way to manage and move data. But there are a number of reasons why data\\npipelines are also the ideal solution for today\'s cybersecurity challenges.\\n\\n- Firstly, security data pipelines optimize and formalize data management. They\\n  allow for the standardized collection, shaping, enrichment, and routing of\\n  data between any security and data technology. They also provide a measurable,\\n  repeatable and more cost-effective approach to solve the growing data\\n  engineering challenges typically faced by security teams.  As they are\\n  designed specifically for security use-cases, they also allow security teams\\n  to meet their own data needs.\\n- Secondly, and in today\'s economic climate more crucially, security data\\n  pipelines reduce consumption-based costs. By moving only the right data to the\\n  right place at the right time in the most efficient way, and by pushing\\n  detection and enrichment workloads to the network edge, businesses can\\n  drastically reduce their SIEM, cloud, and other data costs. Security\\n  operations become more efficient and cost-effective, ultimately allowing more\\n  data to be collected, and scarce money to be reallocated.\\n- Thirdly, security data pipelines help avoid vendor lock-in. Tenzir is built on\\n  open data and security standards, making data exchange between different\\n  technologies trivial. Pipelines also connect diverse tools and solutions as\\n  needed, enabling organizations to choose whatever solutions fit best for them,\\n  and to better adapt to evolving.\\n- Finally, the flexibility and scalability of security data pipelines are\\n  unmatched. They can scale up or down according to need. They also make it easy\\n  to support new data types and security scenarios, helping to future-proof your\\n  security architecture, and providing operational plasticity and resilience.\\n\\nSecurity data pipelines are transforming the security operations landscape by\\nproviding a more effective and efficient way to manage the ever-growing volumes\\nof security data. As the volume, variety, and velocity of security data continue\\nto increase, the need for more effective data management and analysis tools will\\nonly grow as well.\\n\\nAt Tenzir, we are leading this transformation, building an open platform that\\nempowers security teams to build and deploy efficient security data pipelines\\nusing plug-and-play building blocks. Our goal is simple\u2014more time for threat\\nhunting, less time and money on data engineering, and a more robust\\ncybersecurity posture overall.\\n\\nIn today\'s complex cybersecurity landscape, data pipelines are not just for data\\nengineers anymore. They have become indispensable for security teams. The era\\nfor security data pipelines isn\'t on the horizon, it\'s already here.\\n\\nJoin us on this exciting journey to revamp cybersecurity operations.\\n\\nStart using Tenzir by visiting our website at\\n[https://tenzir.com](https://tenzir.com?utm_source=Blog), or get in touch with\\nus at [info@tenzir.com](mailto:info@tenzir.com)."},{"id":"/tenzir-for-splunk-users","metadata":{"permalink":"/archive/tenzir-for-splunk-users","source":"@site/archive/tenzir-for-splunk-users/index.md","title":"Tenzir for Splunk Users","description":"Our Tenzir Query Language (TQL) is a pipeline language that works by chaining","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"zeek","permalink":"/archive/tags/zeek"},{"label":"threat hunting","permalink":"/archive/tags/threat-hunting"},{"label":"pipelines","permalink":"/archive/tags/pipelines"},{"label":"tql","permalink":"/archive/tags/tql"},{"label":"splunk","permalink":"/archive/tags/splunk"},{"label":"spl","permalink":"/archive/tags/spl"}],"readingTime":8.005,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir for Splunk Users","authors":"mavam","date":"2023-08-03T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["zeek","threat hunting","pipelines","tql","splunk","spl"],"comments":true},"prevItem":{"title":"Introducing Tenzir Security Data Pipelines","permalink":"/archive/introducing-tenzir-security-data-pipelines"},"nextItem":{"title":"Native Zeek Log Rotation & Shipping","permalink":"/archive/native-zeek-log-rotation-and-shipping"}},"content":"Our Tenzir Query Language (TQL) is a pipeline language that works by chaining\\noperators into data flows. When we designed TQL, we specifically studied\\nSplunk\'s [Search Processing Language (SPL)][spl], as it generally leaves a\\npositive impression for security analysts that are not data engineers. Our goal\\nwas to take all the good things of SPL, but provide a more powerful language\\nwithout compromising simplicity. In this blog post, we explain how the two\\nlanguages differ using concrete threat hunting examples.\\n\\n[spl]: https://docs.splunk.com/Documentation/SplunkCloud/latest/Search/Aboutthesearchlanguage\\n\\n![SPL versus TQL](spl-vs-tql.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why not SQL?\\n\\nSplunk was the first tool that provided an integrated solution from interactive\\ndata exploration to management-grade dashboards\u2014all powered by dataflow\\npipelines. The success of Splunk is not only resulting from their first-mover\\nadvantage in the market, but also because their likable user experience: it is\\n*easy* to get things done.\\n\\nAt Tenzir, we have a very clear target audience: security practitioners. They\\nare not necessarily data engineers and fluent in SQL and low-level data tools,\\nbut rather identify as blue teamers, incident responders, threat hunters,\\ndetection engineers, threat intelligence analysts, and other domain experts. Our\\ngoal is cater to these folks, without requiring them to have deep understanding\\nof relational algebra.\\n\\nWe opted for a dataflow language because it simplifies reasoning\u2014one step at a\\ntime. At least conceptually, because a smart system optimizes the execution\\nunder the hood. As long as the observable behavior remains the same, the\\nunderlying implementation can optimize the actual computation at will. This is\\nespecially noticeable with declarative languages, such as SQL, where the user\\ndescribes the *what* instead of the *how*. A dataflow language is a bit more\\nconcrete in that it\'s closer to the *how*, but that\'s precisely the trade-off\\nthat simplifies the reasoning: the focus is on a single operation at a time as\\nopposed to an entire large expression.\\n\\nThis dataflow pipeline style is becoming more and more popular. Most SIEMs have\\na language of their own, like Splunk. [Kusto][kusto] is another great example\\nwith a wide user base in security. Even in the data space,\\n[PRQL](https://prql-lang.org) witnesses a strong support for this way of\\nthinking.\\n\\n[kusto]: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\\n\\nIn fact, for a given dataflow pipeline there\'s often an equivalent SQL\\nexpression, because the underlying engines frequently map to the same execution\\nmodel. This gives rise to [transpiling dataflow languages to other execution\\nplatforms][splunk-transpiler]. Ultimately, our goal is that security\\npractitioners do not have to think about *any* of this and stay in their happy\\nplace, which means avoiding context switches to lower-level data primitives.\\n\\n[splunk-transpiler]: https://www.databricks.com/blog/2022/12/16/accelerating-siem-migrations-spl-pyspark-transpiler.html\\n\\nNow that we got the SQL topic out of the way, let\'s dive into some hands-on\\nexamples that illustrate the similarities and differences between SPL and TQL.\\n\\n## Examples\\n\\nBack in 2020, Eric Ooi wrote about [threat hunting with\\nZeek](https://www.ericooi.com/zeekurity-zen-part-iv-threat-hunting-with-zeek/),\\nproviding a set of Splunk queries that are corner stones for threat hunting.\\n\\n### Connections to destination port > 1024\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn id.resp_p > 1024\\n| chart count over service by id.resp_p\\n```\\n\\nTenzir:\\n\\n```\\nexport\\n| where #schema == \\"zeek.conn\\" && id.resp_p > 1024\\n| summarize count(.) by service, id.resp_p\\n```\\n\\nAnalysis:\\n\\n- In SPL, you typically start with an `index=X` to specify your dataset. In\\n  TQL, you start with a source operator. To run a query over historical data, we\\n  use the `export` operator.\\n\\n- The subsequent `where` operator is a transformation to filter the stream of\\n  events with the expression `#schema == \\"zeek.conn\\" && id.resp_p > 1024`. In\\n  SPL, you write that expression directly into `index`. In TQL, we logically\\n  separate this because one operator should have exactly one purpose. Under the\\n  hood, the TQL optimizer does predicate pushdown to avoid first exporting the\\n  entire database and only then applying the filter.\\n\\n  Why does this single responsibility principle matter? Because it\'s critical\\n  for *composition*: we can now replace `export` with another data source, like\\n  `from`, `kafka`, and the rest of the pipeline stays the same.\\n\\n- TQL\'s `#schema` is an expression that is responsible for filtering the data\\n  sources. This is because all TQL pipelines are *multi-schema*, i.e., they can\\n  process more than a single type of data. The ability to specify a regular\\n  expression makes for a powerful way to select the desired input.\\n\\n- SPL\'s [`chart X by Y, Z`][chart] (or equivalently `chart X over Y by Z`)\\n  performs an implicit\\n  [pivot-wider](https://epirhandbook.com/en/pivoting-data.html) operation on\\n  `Z`. This different tabular format has the same underlying data produced by\\n  `summarize X by Y, Z`, which is why we are replacing it accordingly in our\\n  examples.\\n\\n[chart]: https://www.splunk.com/en_us/blog/tips-and-tricks/search-commands-stats-chart-and-timechart.html\\n\\n### Top 10 sources by number of connections\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn\\n| top id.orig_h\\n| head 10\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| top id.orig_h\\n| head 10\\n```\\n\\nNote the similarity. We opted to add `top` and `rare` to make SPL users feel at\\nhome.\\n\\n### Top 10 sources by bytes sent\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn\\n| stats values(service) as Services sum(orig_bytes) as B by id.orig_h\\n| sort -B\\n| head 10\\n| eval MB = round(B/1024/1024,2)\\n| eval GB = round(MB/1024,2)\\n| rename id.orig_h as Source\\n| fields Source B MB GB Services\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| summarize Services=distinct(service), B=sum(orig_bytes) by id.orig_h\\n| sort B desc\\n| head 10\\n| extend MB=round(B/1024/1024,2)\\n| extend GB=round(MB/1024,2)\\n| put Source=id.orig_h, B, MB, GB, Services\\n```\\n\\nAnalysis:\\n\\n- We opted for Kusto\'s syntax of sorting (for technical reasons), by appending\\n  an `asc` or `desc` qualifier after the field name. `sort -B` translates into\\n  `sort B desc`, whereas `sort B` into `sort B asc`. However, we want to adopt\\n  the SPL syntax in the future.\\n\\n- SPL\'s `eval` maps to `extend`.\\n\\n- The difference between `extend` and `put` is that `extend` keeps all fields as\\n  is, whereas `put` reorders fields and performs an explicit projection with the\\n  provided fields.\\n\\n- We don\'t have functions in TQL. *Yet*. It\'s one of our most important roadmap\\n  items at the time of writing, so stay tuned.\\n\\n### Bytes transferred over time by service\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=\\"zeek_conn\\" OR sourcetype=\\"zeek_conn_long\\"\\n| eval orig_megabytes = round(orig_bytes/1024/1024,2)\\n| eval resp_megabytes = round(resp_bytes/1024/1024,2)\\n| eval orig_gigabytes = round(orig_megabytes/1024,2)\\n| eval resp_gigabytes = round(resp_megabytes/1024,2)\\n| timechart sum(orig_gigabytes) AS \'Outgoing\',sum(resp_gigabytes) AS \'Incoming\' by service span=1h\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == /zeek\\\\.conn.*/\\n| extend orig_megabytes=round(orig_bytes/1024/1024, 2)\\n| extend resp_megabytes=round(orig_bytes/1024/1024, 2)\\n| extend orig_gigabytes=round(orig_megabytes/1024, 2)\\n| extend resp_gigabytes=round(orig_megabytes/1024, 2)\\n| summarize Outgoing=sum(orig_gigabytes), Incoming=sum(resp_gigabytes) by ts, service resolution 1h\\n```\\n\\nAnalysis:\\n\\n- SPL\'s `timechart` does an implicit group by timestamp. As we use TQL\'s\\n  `summarize` operator, we need to explicitly provide the grouping field `ts`.\\n  In the future, you will be able to use `:timestamp` in a grouping expression,\\n  i.e., group by the field with the type named `timestamp`.\\n\\n- This query spreads over two data sources: the event `zeek.conn` and\\n  `zeek.conn_long`. The latter tracks long-running connections and is available\\n  as [separate package](https://github.com/corelight/zeek-long-connections).\\n\\n### Rare JA3 hashes\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_ssl\\n| rare ja3\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.ssl\\"\\n| rare ja3\\n| head 10\\n```\\n\\nAnalysis:\\n\\n- This example shows again how to select a specific data source and perform\\n  \\"stack counting\\". Unlike SPL, our version of `rare` does not limit the output\\n  to 10 events by default, which is why add `head 10`. This goes back to the\\n  single responsibility principle: one operator should do exactly one thing. The\\n  act of limiting the output should always be associated with `head`.\\n\\n### Expired certificates\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_x509\\n| convert num(certificate.not_valid_after) AS cert_expire\\n| eval current_time = now(), cert_expire_readable = strftime(cert_expire,\\"%Y-%m-%dT%H:%M:%S.%Q\\"), current_time_readable=strftime(current_time,\\"%Y-%m-%dT%H:%M:%S.%Q\\")\\n| where current_time > cert_expire\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where certificate.not_valid_after > now()\\n```\\n\\nAnalysis:\\n\\n- This example shows the benefit of native time types (and Tenzir\'s rich type\\n  system in general).\\n\\n- TQL\'s type system has first-class support for times and durations.\\n\\n- TQL\'s `zeek-tsv` parser preserves `time` types natively, so you don\'t have to\\n  massage strings at query-time.\\n\\n### Large DNS queries\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_dns\\n| eval query_length = len(query)\\n| where query_length > 75\\n| table _time id.orig_h id.resp_h proto query query_length answer\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.dns\\"\\n| extend query_length = length(query)\\n| where query_length > 75\\n| select :timestamp, id.orig_h, id.resp_h, proto, query, query_length, answer\\n```\\n\\nAnalysis:\\n\\n- As mentioned above, we don\'t have functions in TQL yet. Once we have them,\\n  SPL\'s `len` will map to `length` in TQL.\\n\\n- The SPL-generated `_time` field maps to the `:timestamp` type extractor in\\n  TQL.\\n\\n### Query responses with NXDOMAIN\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_dns rcode_name=NXDOMAIN\\n| table _time id.orig_h id.resp_h proto query\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.dns\\" && rcode_name == \\"NXDOMAIN\\"\\n| select :timestamp, id.orig_h, id.resp_h, proto, query\\n```\\n\\nAnalysis:\\n\\n- The `table` operator in splunk outputs the data in tabular form. This is the\\n  default for our [app](https://app.tenzir.com).\\n\\n- There\'s also an [upcoming](https://github.com/tenzir/tenzir/pull/3113) `write\\n  table` format to generate a tabular representation outside the app.\\n\\n## Summary\\n\\nIn this blog post we\'ve juxtaposed the languages of Splunk (SPL) and Tenzir\\n(TQL). They are remarkably similar\u2014and that\'s not accidental. When we talked to\\nsecurity analysts we often heard that Splunk has a great UX. Even our own\\nengineers that live on the command line find this mindset natural. But Splunk\\nwas not our only inspiration, we also drew inspiration from Kusto and others.\\n\\nAs we created TQL, we wanted to learn from missed opportunities while doubling\\ndown on SPL\'s great user experience.\\n\\nIf you\'d like to give Tenzir a spin, [try our community\\nedition](https://app.tenzir.com) for free. A demo node with example pipelines is\\nwaiting for you."},{"id":"/native-zeek-log-rotation-and-shipping","metadata":{"permalink":"/archive/native-zeek-log-rotation-and-shipping","source":"@site/archive/native-zeek-log-rotation-and-shipping/index.md","title":"Native Zeek Log Rotation & Shipping","description":"Did you know that Zeek supports log rotation triggers, so","date":"2023-07-27T00:00:00.000Z","formattedDate":"July 27, 2023","tags":[{"label":"zeek","permalink":"/archive/tags/zeek"},{"label":"logs","permalink":"/archive/tags/logs"},{"label":"shipping","permalink":"/archive/tags/shipping"},{"label":"rotation","permalink":"/archive/tags/rotation"},{"label":"pipelines","permalink":"/archive/tags/pipelines"}],"readingTime":4.75,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Native Zeek Log Rotation & Shipping","authors":"mavam","date":"2023-07-27T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["zeek","logs","shipping","rotation","pipelines"],"comments":true},"prevItem":{"title":"Tenzir for Splunk Users","permalink":"/archive/tenzir-for-splunk-users"},"nextItem":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","permalink":"/archive/shell-yeah-supercharging-zeek-and-suricata-with-tenzir"}},"content":"Did you know that [Zeek](http://zeek.org) supports log rotation triggers, so\\nthat you can do anything you want with a newly rotated batch of logs?\\n\\n![Zeek Log Rotation](zeek-log-rotation.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nThis blog post shows you how to use Zeek\'s native log rotation feature to\\nconveniently invoke any post-processor, such as a log shipper. In our examples\\nwe show how to to ingest data into Tenzir, but you can plug in any downstream\\ntooling.\\n\\n## External Log Shipping (pull)\\n\\nIn case you\'re not using Zeek\'s native log rotation trigger, you may observe a\\ndirectory to which Zeek periodically writes files. For example, the utility\\n[zeek-archiver](https://github.com/zeek/zeek-archiver) does that.\\n\\nGeneric log shippers can take care of that as well. Your mileage may vary. For\\nexample, [Filebeat][filebeat] works for stock Zeek only. The parsing logic is\\nhard-coded for every log type. If you have custom scripts or extend some logs,\\nyou\'re left alone. Filebeat also uses the stock Zeek JSON output, which has no\\ntype information. Filebeat then brings the typing back manually later as it\\nconverts the logs to the Elastic Common Schema (ECS).\\n\\n[filebeat]: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-zeek.html\\n\\n## Native Log Shipping (push)\\n\\nThere\'s also a lesser known, push-based option using [Zeek\'s logging\\nframework](https://docs.zeek.org/en/master/frameworks/logging.html). You can\\nprovide a shell script that Zeek invokes *whenever it rotates a file*. The shell\\nscript receives the filename of the rotated file plus some additional metadata\\nas arguments.\\n\\nFirst, to activate log rotation, you need to set\\n`Log::default_rotation_interval` to a non-zero value. The default of `0 secs`\\nmeans that log rotation is disabled.\\n\\nSecond, to customize what\'s happening on rotation you can redefine\\n[`Log::default_rotation_postprocessor_cmd`](https://docs.zeek.org/en/master/scripts/base/frameworks/logging/main.zeek.html#id-Log::default_rotation_postprocessor_cmd)\\nto point to a shell script.\\n\\nFor example, to rotate all log files every 10 minutes with a custom `ingest`\\nscript, you can invoke Zeek as follows:\\n\\n```bash\\nzeek -r trace.pcap \\\\\\n  Log::default_rotation_postprocessor_cmd=ingest \\\\\\n  Log::default_rotation_interval=10mins\\n```\\n\\nLet\'s take a look at this `ingest` shell script in more detail. Zeek always\\npasses 6 arguments to the post-processing script:\\n\\n1. The filename of the log, e.g., `/path/to/conn.log`\\n2. The type of the log (aka. `path`), such as `conn` or `http`\\n3. Timestamp when Zeek opened the log file\\n4. Timestamp when Zeek closed (= rotated) the log file\\n5. A flag that is true when rotation occurred due to Zeek terminating\\n6. The format of the log, which is either `ascii` (=\\n   [`zeek-tsv`](/formats/zeek-tsv)) or [`json`](/formats/json)\\n\\nHere\'s a complete example that uses (1), (2), and (6):\\n\\n```bash title=\\"ingest\\"\\n#!/bin/sh\\n\\nfile_name=\\"$1\\"\\nbase_name=\\"$2\\"\\nfrom=\\"$3\\"\\nto=\\"$4\\"\\nterminating=\\"$5\\"\\nwriter=\\"$6\\"\\n\\nif [ \\"$writer\\" = \\"ascii\\" ]; then\\n  format=\\"zeek-tsv\\"\\nelif [ \\"$writer\\" = \\"json\\" ]; then\\n  format=\\"json --schema zeek.$base_name\\"\\nelse\\n  echo \\"unsupported Zeek writer: $writer\\"\\n  exit 1\\nfi\\n\\npipeline=\\"from file $file_name read $format | import\\"\\n\\ntenzir \\"$pipeline\\"\\n```\\n\\n### Post-processing with Tenzir pipelines\\n\\nWhen you run Zeek as above, the `ingest` script dynamically constructs an\\ningestion pipeline based on the type of the Zeek log at hand. Given your logging\\nformat (TSV or JSON), the pipelines for a rotated `conn.log` file may look like\\nthis:\\n\\n```\\nfrom file /path/to/conn.log read zeek-tsv | import\\nfrom file /path/to/conn.log read json --schema zeek.conn | import\\n```\\n\\nThis pipeline reads the Zeek log and pipes it to the `import` operator, which\\nstores all your logs at a running Tenzir node. You could also use the `extend`\\noperator to include the filename in the data:\\n\\n```bash\\npipeline=\\"from file $file_name read $format \\\\\\n          | extend filename=$file_name \\\\\\n          | import\\"\\n```\\n\\n### Zeek package\\n\\nIf you want post-processing with Tenzir pipelines out of the box, use our\\nofficial [Zeek package](https://github.com/tenzir/zeek-tenzir):\\n\\n```bash\\nzkg install zeek-tenzir\\n```\\n\\nAfter installing the package, you have two options to run pipelines on rotated\\nZeek logs:\\n\\n1. Load the `tenzir-import` Zeek script to ship logs to a local Tenzir node\\n\\n   ```bash\\n   # Start a node.\\n   tenzir-node\\n   # Ship logs to it and delete the original files.\\n   zeek -r trace.pcap tenzir/import\\n   ```\\n\\n  Pass `Tenzir::delete_after_postprocesing=F` to `zeek` to keep the original\\n  logs.\\n\\n2. Write Zeek scripts to register pipelines manually:\\n\\n   ```zeek\\n   # Activate log rotation by setting a non-zero value.\\n   redef Log::default_rotation_interval = 10 mins;\\n \\n   event zeek_init()\\n     {\\n     Tenzir::postprocess(\\"import\\");\\n     Tenzir::postprocess(\\"to directory /tmp/logs write parquet\\");\\n     }\\n   ```\\n\\n   The above Zeek script hooks up two pipelines via the function\\n   `Tenzir::postprocess`. Each pipeline executes upon log rotation and receives\\n   the Zeek log file as input. The first imports all data via `import` and the\\n   second writes the logs as `parquet` files using `to`.\\n\\n## Reliability\\n\\nZeek implements the log rotation logic by spawning a separate child process.\\nWhen the (parent) Zeek process dies, the children become orphaned and keep\\nrunning until completion.\\n\\nThe implication is that Zeek cannot re-trigger a failed post-processing command.\\nSo you have exactly one shot. This may not be a problem for trace file analysis,\\nbut live deployments may require higher reliability guarantees. For such\\nscenarios, we recommend to use the post-processing script as a notifier, e.g.,\\nto signal another tool that it can now process a file.\\n\\nFor ultimate control over logging, you can always develop your own [writer\\nplugin](/archive/mobilizing-zeek-logs#writer-plugin) that immediately ship logs\\ninstead of going through the file system.\\n\\n## Conclusion\\n\\nThis blog post shows how you can use Zeek\'s native log rotation feature to\\ninvoke an arbitrary command as soon as a log file gets rotated. This approach\\nprovides an attractive alternative that turns pull-based file monitoring into\\nmore flexible push-based delivery.\\n\\n|              |   Push   |     Pull     |\\n| ------------ |:--------:|:------------:|\\n| Trigger      | rotation | new file/dir |\\n| Complexity   |   low    |    medium    |\\n| Reliability  |   low    |    high      |\\n\\nIf you are looking for an efficient way to get your Zeek logs flowing, [give\\nTenzir a try](/overview). [Our Zeek\\npackage](https://github.com/tenzir/zeek-tenzir) makes it easy to launch\\npost-processing pipelines natively from Zeek. And don\'t forget to check out our\\n[other Zeek blogs](/archive/tags/zeek)."},{"id":"/shell-yeah-supercharging-zeek-and-suricata-with-tenzir","metadata":{"permalink":"/archive/shell-yeah-supercharging-zeek-and-suricata-with-tenzir","source":"@site/archive/shell-yeah-supercharging-zeek-and-suricata-with-tenzir/index.md","title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","description":"As an incident responder, threat hunter, or detection engineer, getting quickly","date":"2023-07-20T00:00:00.000Z","formattedDate":"July 20, 2023","tags":[{"label":"zeek","permalink":"/archive/tags/zeek"},{"label":"suricata","permalink":"/archive/tags/suricata"},{"label":"logs","permalink":"/archive/tags/logs"},{"label":"shell","permalink":"/archive/tags/shell"}],"readingTime":4.125,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","authors":"mavam","date":"2023-07-20T00:00:00.000Z","last_updated":"2024-02-23T00:00:00.000Z","tags":["zeek","suricata","logs","shell"],"comments":true},"prevItem":{"title":"Native Zeek Log Rotation & Shipping","permalink":"/archive/native-zeek-log-rotation-and-shipping"},"nextItem":{"title":"Zeek and Ye Shall Pipe","permalink":"/archive/zeek-and-ye-shall-pipe"}},"content":"As an incident responder, threat hunter, or detection engineer, getting quickly\\nto your analytics is key for productivity. For network-based visibility and\\ndetection, [Zeek](https://zeek.org) and [Suricata](https://suricata.io) are the\\nbedrock for many security teams. But operationalizing these tools can take a\\ngood chunk of time.\\n\\nSo we asked ourselves: **How can we make it super easy to work with Zeek and\\nSuricata logs?**\\n\\n![Shell Operator](shell-operator.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Purring like a Suricat\\n\\nIn our [previous blog post](/archive/zeek-and-ye-shall-pipe/) we adapted Zeek to\\nbehave like a good ol\' Unix tool, taking input via stdin and producing output\\nvia stdout. Turns out you can do the same fudgery with Suricata:[^1]\\n\\n[^1]: Suricata outputs [EVE\\nJSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html)\\nby default, which is equivalent to Zeek\'s streaming JSON output, with the\\ndifference being that Zeek\'s `_path` field is called `event_type` in Suricata\\nlogs.\\n\\n```bash title=suricatify\\n#!/bin/sh\\nsuricata -r /dev/stdin \\\\\\n  --set outputs.1.eve-log.filename=/dev/stdout \\\\\\n  --set logging.outputs.0.console.enabled=no\\n```\\n\\nLet\'s break this down:\\n\\n- The `--set` option take a `name=value` parameter that overrides the settings\\n  in your `suricata.yaml` config file.\\n- The key `outputs.1.eve-log.filename` refers to the `outputs` array, takes\\n  element at index `1`, treats that as object and goes to the nested field\\n  `eve-log.filename`. Setting `/dev/stdout` as filename makes Suricata write to\\n  stdout.\\n- We must set `logging.outputs.0.console.enabled` to `no` because Suricata\\n  writes startup log messages to stdout. Since they are not valid JSON, we\\n  would otherwise create an invalid JSON output stream.\\n\\n## User-defined Operators\\n\\nNow that we have both Zeek and Suricata at our fingertips, how can we work with\\ntheir output more easily? This is where Tenzir comes into play\u2014easy pipelines\\nfor security teams to acquire, shape, and route event data.\\n\\nHere are two examples that count the number of unique source IP addresses per\\ndestination IP address, on both Zeek and Suricata data:\\n\\n```bash\\n# Zeek\\nzcat pcap.gz | zeekify | tenzir \\\\\\n  \'read zeek-json\\n   | where #schema == \\"zeek.conn\\"\\n   | summarize n=count_distinct(id.orig_h) by id.resp_h\\n   | sort n desc\'\\n# Suricata\\nzcat pcap.gz | suricatify | tenzir \\\\\\n  \'read suricata\\n   | where #schema == \\"suricata.flow\\"\\n   | summarize n=count_distinct(src_ip) by dst_ip\\n   | sort n desc\'\\n```\\n\\nIt\'s a bit unwieldy to write such a command line that requires an external shell\\nscript to work. This is where user-defined operators come into play. In\\ncombination with the `shell` operator, you can write a custom `zeek` and\\n`suricata` operator and ditch the shell script:\\n\\n```yaml title=\\"tenzir.yaml\\"\\ntenzir:\\n  operators:\\n    zeek:\\n     shell \\"zeek -r - LogAscii::output_to_stdout=T\\n            JSONStreaming::disable_default_logs=T\\n            JSONStreaming::enable_log_rotation=F\\n            json-streaming-logs\\"\\n     | read zeek-json\\n    suricata:\\n     shell \\"suricata -r /dev/stdin\\n            --set outputs.1.eve-log.filename=/dev/stdout\\n            --set logging.outputs.0.console.enabled=no\\"\\n     | read suricata\\n```\\n\\nThe difference stands out when you look now at the pipeline definition:\\n\\n```text title=Zeek\\nzeek\\n| where #schema == \\"zeek.conn\\"\\n| summarize n=count_distinct(id.orig_h) by id.resp_h\\n| sort n desc\\n```\\n\\n```text bash title=Suricata\\nsuricata\\n| where #schema == \\"suricata.flow\\"\\n| summarize n=count_distinct(src_ip) by dst_ip\\n| sort n desc\\n```\\n\\nIt\'s pretty convenient to drop packets into a Tenzir pipeline, process them with\\nour favorite tools, and then perform fast in-situ analytics on them. The nice\\nthing is that operators compose: a new operator automatically works with all\\nexisting ones.\\n\\n## How does it work?\\n\\nFirst, let\'s take a look at the standard approach where one process pipes the\\noutput into the next:\\n\\n![Piping Zeek to Tenzir](zeek-to-tenzir-pipe.excalidraw.svg)\\n\\nWhen using the `shell` operator, the `tenzir` process spawns `zeek` or\\n`suricata` as child process. The operator then forwards the bytes from stdin of\\nthe `tenzir` process to the child\'s stdin, and uses the child\'s stdout as input\\nto the subsequent `read` operator.\\n\\n![Shelling out to Zeek](zeek-to-tenzir-shell.excalidraw.svg)\\n\\nIn the above example, `shell` acts as a *source* operator, i.e., it does not\\nconsume input and only produces output. The `shell` operator can also act as\\n*transformation*, i.e., additionally accept input. This makes it possible to use\\nit more flexibly in combination with other operators, e.g., the `load` operator\\nemitting bytes from a loader:\\n\\n```\\nload file trace.pcap\\n| zeek\\n| where 6.6.6.6\\n| write json\\n```\\n\\nGot a PCAP trace via Kafka? Just exchange the `file` loader with the `kafka`\\nloader:\\n\\n```\\nload kafka -t artifact\\n| zeek\\n| where 6.6.6.6\\n| write json\\n```\\n\\nYou may not always sit in front of a command line and are able to pipe data from\\na Unix tool into a Tenzir pipeline. For example, when you use our\\n[app](https://app.tenzir.com) or the [REST API](/rest-api). This is where the\\n`shell` operator shines. The diagram above shows how `shell` shifts the entry\\npoint of data from a tool to the Tenzir process. You can consider `shell` your\\nescape hatch to reach deeper into a specific Tenzir node, as if you had a native\\nshell.\\n\\n## Conclusion\\n\\nIn this blog post we showed you the `shell` operator and how you can use it to\\nintegrate third-party tooling into a Tenzir pipeline when coupled with\\nuser-defined operators.\\n\\nUsing Zeek or Suricata? Tenzir makes \'em fun to work with. Check out our other\\nblogs tagged with [`#zeek`](/archive/tags/zeek) and\\n[`#suricata`](/archive/tags/suricata), and [give it a shot](/overview) yourself."},{"id":"/zeek-and-ye-shall-pipe","metadata":{"permalink":"/archive/zeek-and-ye-shall-pipe","source":"@site/archive/zeek-and-ye-shall-pipe/index.md","title":"Zeek and Ye Shall Pipe","description":"Zeek turns packets into structured logs. By default, Zeek","date":"2023-07-13T00:00:00.000Z","formattedDate":"July 13, 2023","tags":[{"label":"zeek","permalink":"/archive/tags/zeek"},{"label":"logs","permalink":"/archive/tags/logs"},{"label":"json","permalink":"/archive/tags/json"},{"label":"pipelines","permalink":"/archive/tags/pipelines"}],"readingTime":2.46,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Zeek and Ye Shall Pipe","authors":"mavam","date":"2023-07-13T00:00:00.000Z","tags":["zeek","logs","json","pipelines"],"comments":true},"prevItem":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","permalink":"/archive/shell-yeah-supercharging-zeek-and-suricata-with-tenzir"},"nextItem":{"title":"Mobilizing Zeek Logs","permalink":"/archive/mobilizing-zeek-logs"}},"content":"[Zeek](https://zeek.org) turns packets into structured logs. By default, Zeek\\ngenerates one file per log type and per rotation timeframe. If you don\'t want to\\nwrangle files and directly process the output, this short blog post is for you.\\n\\n![Zeek as Pipeline](zeek-as-pipeline.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nZeek requires a bit of adaptation to fit in the Unix pipeline model, by which we\\nmean *take your input on stdin and produce your output to stdout*:\\n\\n```\\n<upstream> | zeek | <downstream>\\n```\\n\\nIn this example, `<upstream>` produces packets in PCAP format and `<downstream>`\\nprocesses the Zeek logs. Let\'s work towards this.\\n\\nSolving the upstream part is easy: just use `zeek -r -` to read from stdin. So\\nlet\'s focus on the logs downstream. [Our last blog](/archive/mobilizing-zeek-logs)\\nintroduced the various logging formats, such as tab-separated values (TSV),\\nJSON, and Streaming JSON with an extra `_path` discriminator field. The only\\nformat conducive to multiplexing different log types is Streaming JSON.\\n\\nLet\'s see what we get:\\n\\n```bash\\nzcat < trace.pcap | zeek -r - json-streaming-logs\\n```\\n\\n```\\n\u276f ls\\njson_streaming_analyzer.1.log       json_streaming_packet_filter.1.log\\njson_streaming_conn.1.log           json_streaming_pe.1.log\\njson_streaming_dce_rpc.1.log        json_streaming_reporter.1.log\\njson_streaming_dhcp.1.log           json_streaming_sip.1.log\\njson_streaming_dns.1.log            json_streaming_smb_files.1.log\\njson_streaming_dpd.1.log            json_streaming_smb_mapping.1.log\\njson_streaming_files.1.log          json_streaming_snmp.1.log\\njson_streaming_http.1.log           json_streaming_ssl.1.log\\njson_streaming_kerberos.1.log       json_streaming_tunnel.1.log\\njson_streaming_ntlm.1.log           json_streaming_weird.1.log\\njson_streaming_ntp.1.log            json_streaming_x509.1.log\\njson_streaming_ocsp.1.log\\n```\\n\\nThe `json-streaming-package` prepends a distinguishing prefix to the filename.\\nThe `*.N.log` suffix counts the rotations, e.g., `*.1.log` means the logs from\\nthe first batch.\\n\\nLet\'s try to avoid the files altogether and send the contents of these file to\\nstdout. This requires a bit of option fiddling to achieve the desired result:\\n\\n```bash\\nzcat < trace.pcap |\\n  zeek -r - \\\\\\n    LogAscii::output_to_stdout=T \\\\\\n    JSONStreaming::disable_default_logs=T \\\\\\n    JSONStreaming::enable_log_rotation=F \\\\\\n    json-streaming-logs\\n```\\n\\nThis requires a bit explanation:\\n\\n- `LogAscii::output_to_stdout=T` redirects the log output to stdout.\\n- `JSONStreaming::disable_default_logs=T` disables the default TSV logs.\\n  Without this option, Zeek will print *both* TSV and NDJSON to stdout.\\n- `JSONStreaming::enable_log_rotation=F` disables log rotation. This is needed\\n  because the option `output_to_stdout=T` sets the internal filenames to\\n  `/dev/stdout`, which Zeek then tries to rotate away. Better not.\\n\\nHere\'s the result you\'d expect, which is basically a `cat *.log`:\\n\\n```json\\n{\\"_path\\":\\"files\\",\\"_write_ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"fuid\\":\\"FhEFqzHx1hVpkhWci\\",\\"uid\\":\\"CHhfpE1dTbPgBTR24\\",\\"id.orig_h\\":\\"128.14.134.170\\",\\"id.orig_p\\":57468,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":80,\\"source\\":\\"HTTP\\",\\"depth\\":0,\\"analyzers\\":[],\\"mime_type\\":\\"text/html\\",\\"duration\\":0.0,\\"is_orig\\":false,\\"seen_bytes\\":51,\\"total_bytes\\":51,\\"missing_bytes\\":0,\\"overflow_bytes\\":0,\\"timedout\\":false}\\n{\\"_path\\":\\"http\\",\\"_write_ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"ts\\":\\"2021-11-17T13:32:43.249475Z\\",\\"uid\\":\\"CHhfpE1dTbPgBTR24\\",\\"id.orig_h\\":\\"128.14.134.170\\",\\"id.orig_p\\":57468,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"GET\\",\\"host\\":\\"198.71.247.91\\",\\"uri\\":\\"/\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36 \\",\\"request_body_len\\":0,\\"response_body_len\\":51,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[],\\"resp_fuids\\":[\\"FhEFqzHx1hVpkhWci\\"],\\"resp_mime_types\\":[\\"text/html\\"]}\\n{\\"_path\\":\\"packet_filter\\",\\"_write_ts\\":\\"1970-01-01T00:00:00.000000Z\\",\\"ts\\":\\"2023-07-11T03:30:17.189787Z\\",\\"node\\":\\"zeek\\",\\"filter\\":\\"ip or not ip\\",\\"init\\":true,\\"success\\":true}\\n{\\"_path\\":\\"conn\\",\\"_write_ts\\":\\"2021-11-17T13:33:01.457108Z\\",\\"ts\\":\\"2021-11-17T13:32:46.565338Z\\",\\"uid\\":\\"CD868huwhDP636oT\\",\\"id.orig_h\\":\\"89.248.165.145\\",\\"id.orig_p\\":43831,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":52806,\\"proto\\":\\"tcp\\",\\"conn_state\\":\\"S0\\",\\"missed_bytes\\":0,\\"history\\":\\"S\\",\\"orig_pkts\\":1,\\"orig_ip_bytes\\":40,\\"resp_pkts\\":0,\\"resp_ip_bytes\\":0}\\n{\\"_path\\":\\"tunnel\\",\\"_write_ts\\":\\"2021-11-17T13:40:34.891453Z\\",\\"ts\\":\\"2021-11-17T13:40:34.891453Z\\",\\"uid\\":\\"CsqzCG2F8VDR4gM3a8\\",\\"id.orig_h\\":\\"49.213.162.198\\",\\"id.orig_p\\":0,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":0,\\"tunnel_type\\":\\"Tunnel::GRE\\",\\"action\\":\\"Tunnel::DISCOVER\\"}\\n```\\n\\nNobody can remember this invocation. Especially during firefighting when you\\nquickly need to plow through a trace to understand it. So we want to wrap this\\nsomehow:\\n\\n```bash title=zeekify\\n#!/bin/sh\\nzeek -r - \\\\\\n  LogAscii::output_to_stdout=T \\\\\\n  JSONStreaming::disable_default_logs=T \\\\\\n  JSONStreaming::enable_log_rotation=F \\\\\\n  json-streaming-logs \\\\\\n  \\"$@\\"\\n```\\n\\nNow we\'re in pipeline land:\\n\\n```bash\\nzcat pcap.gz | zeekify | head | jq -r ._path\\n```\\n\\n```\\npacket_filter\\nfiles\\nntp\\ntunnel\\nconn\\nntp\\nhttp\\nconn\\nntp\\nconn\\n```\\n\\nOkay, we got Zeek as a Unix pipe. But now you have to wrangle the JSON with\\n`jq`. Unless you\'re a die-hard fan, even simple analytics, like filtering or\\naggregating, have a steep learning curve. In the next blog post, we\'ll double\\ndown on the elegant principle of pipelines and show how you can take do easy\\nin-situ analytics with Tenzir."},{"id":"/mobilizing-zeek-logs","metadata":{"permalink":"/archive/mobilizing-zeek-logs","source":"@site/archive/mobilizing-zeek-logs/index.md","title":"Mobilizing Zeek Logs","description":"Zeek offers many ways to produce and consume logs. In this","date":"2023-07-06T00:00:00.000Z","formattedDate":"July 6, 2023","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"zeek","permalink":"/archive/tags/zeek"},{"label":"logs","permalink":"/archive/tags/logs"},{"label":"json","permalink":"/archive/tags/json"},{"label":"kafka","permalink":"/archive/tags/kafka"}],"readingTime":7.695,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Mobilizing Zeek Logs","authors":"mavam","date":"2023-07-06T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["tenzir","zeek","logs","json","kafka"],"comments":true},"prevItem":{"title":"Zeek and Ye Shall Pipe","permalink":"/archive/zeek-and-ye-shall-pipe"},"nextItem":{"title":"Migrating from VAST to Tenzir","permalink":"/archive/migrating-from-vast-to-tenzir"}},"content":"[Zeek](https://zeek.org) offers many ways to produce and consume logs. In this\\nblog, we explain the various Zeek logging formats and show how you can get the\\nmost out of Zeek with Tenzir. We conclude with recommendations for when to use\\nwhat Zeek format based on your use case.\\n\\n![Packet to Logs](zeek-packets-to-logs.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Zeek Logging 101\\n\\nZeek\'s main function is turning live network traffic or trace files into\\nstructured logs.[^1] Zeek logs span the entire network stack, including\\nlink-layer analytics with MAC address to application-layer fingerprinting of\\napplications. These rich logs are invaluable for any network-based detection and\\nresponse activities. Many users also simply appreciate the myriad of protocol\\nanalyzers, each of which generates a dedicated log file, like `smb.log`,\\n`http.log`, `x509.log`, and others.\\n\\n[^1]: Zeek also comes with a Turing-complete scripting language for executing\\narbitrary logic. The event-based language resembles Javascript and is\\nespecially useful for performing in-depth protocol analysis and engineering\\ndetections.\\n\\nIn the default configuration, Zeek writes logs into the current directory, one\\nfile per log type. There are various file formats to choose from, such as TSV,\\nJSON, and others. Let\'s take a look.\\n\\n### Tab-Separated Values (TSV)\\n\\nZeek\'s custom tab-separated value (TSV) format is variant of CSV with additional\\nmetadata, similar to a data frame.\\n\\nHere\'s how you create TSV logs from a trace:\\n\\n```bash\\nzeek -C -r trace.pcap [scripts]\\n```\\n\\n:::info disable checksumming\\nWe add `-C` to disable checksumming, telling Zeek to ignore mismatches and use\\nall packets in the trace. This is good practice to process all packets in a\\ntrace, as some capturing setups may perturb the checksums.\\n:::\\n\\nAnd here\'s a snippet of the corresponding `conn.log`:\\n\\n```\\n#separator \\\\x09\\n#set_separator\\t,\\n#empty_field\\t(empty)\\n#unset_field\\t-\\n#path\\tconn\\n#open\\t2019-06-07-14-30-44\\n#fields\\tts\\tuid\\tid.orig_h\\tid.orig_p\\tid.resp_h\\tid.resp_p\\tproto\\tservice\\tduration\\torig_bytes\\tresp_bytes\\tconn_state\\tlocal_orig\\tlocal_resp\\tmissed_bytes\\thistory\\torig_pkts\\torig_ip_bytes\\tresp_pkts\\tresp_ip_bytes\\ttunnel_parents\\tcommunity_id\\n#types\\ttime\\tstring\\taddr\\tport\\taddr\\tport\\tenum\\tstring\\tinterval\\tcount\\tcount\\tstring\\tbool\\tbool\\tcount\\tstring\\tcount\\tcount\\tcount\\tcount\\tset[string]\\tstring\\n1258531221.486539\\tCz8F3O3rmUNrd0OxS5\\t192.168.1.102\\t68\\t192.168.1.6\\t7\\tudp\\tdhcp\\t0.163820\\t301\\t300\\tSF\\t-\\t-\\t0\\tDd\\t1\\t329\\t1\\t328\\t-\\t1:aWZfLIquYlCxKGuJ62fQGlgFzAI=\\n1258531680.237254\\tCeJFOE1CNssyQjfJo1\\t192.168.1.103\\t137\\t192.168.1.255\\t137\\tudp\\tdns\\t3.780125\\t350\\t0\\tS0\\t-\\t-\\t546\\t0\\t0\\t-\\t1:fLbpXGtS1VgDhqUW+WYaP0v+NuA=\\n```\\n\\nAnd here\'s a `http.log` with a different header:\\n\\n```\\n#separator \\\\x09\\n#set_separator\\t,\\n#empty_field\\t(empty)\\n#unset_field\\t-\\n#path\\thttp\\n#open\\t2019-06-07-14-30-44\\n#fields\\tts\\tuid\\tid.orig_h\\tid.orig_p\\tid.resp_h\\tid.resp_p\\ttrans_depth\\tmethod\\thost\\turi\\treferrer\\tversion\\tuser_agent\\trequest_body_len\\tresponse_body_len\\tstatus_code\\tstatus_msg\\tinfo_code\\tinfo_msg\\ttags\\tusername\\tpassword\\tproxied\\torig_fuids\\torig_filenames\\torig_mime_types\\tresp_fuids\\tresp_filenames\\tresp_mime_types\\n#types\\ttime\\tstring\\taddr\\tport\\taddr\\tport\\tcount\\tstring\\tstring\\tstring\\tstring\\tstring\\tstring\\tcount\\tcount\\tcount\\tstring\\tcount\\tstring\\tset[enum]\\tstring\\tstring\\tset[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\n1258535653.087137\\tCUk3vSsgfU9oCghL4\\t192.168.1.104\\t1191\\t65.54.95.680\\t1\\tHEAD\\tdownload.windowsupdate.com\\t/v9/windowsupdate/redir/muv4wuredir.cab?0911180916\\t-\\t1.1\\tWindows-Update-Agent\\t0\\t0\\t20OK\\t-\\t-\\t(empty)\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\n1258535655.525107\\tCc6Alh3FtTOAqNSIx2\\t192.168.1.104\\t1192\\t65.55.184.16\\t80\\t1\\tHEAD\\twww.update.microsoft.com\\t/v9/windowsupdate/selfupdate/wuident.cab?0911180916\\t-\\t1.1\\tWindows-Update-Agent\\t0\\t200\\tOK\\t-\\t-\\t(empty)\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\n```\\n\\nMany Zeek users would now resort to their downstream log management tool,\\nassuming it supports the custom TSV format. Zeek also comes with small helper\\nutility `zeek-cut` for light-weight reshaping of this TSV format. For example:\\n\\n```bash\\nzeek-cut id.orig_h id.resp_h < conn.log\\n```\\n\\nThis selects the columns `id.orig_h` and `id.resp_h`. Back in the days, many\\nfolks used `awk` to extract fields by their position, e.g., with `$4`, `$7`,\\n`$9`. This is not only difficult to understand, but also brittle, since Zeek\\nschemas can change based on configuration. With `zeek-cut`, it\'s at least a bit\\nmore robust.\\n\\nTenzir\'s data pipelines make it easy to process Zeek logs. The native\\n[`zeek-tsv`](/next/formats/zeek-tsv) parser converts them into data frames, so\\nthat you can process them with a wide range of [operators](/next/operators):\\n\\n```bash\\ncat *.log | tenzir \'read zeek-tsv | select id.orig_h, id.resp_h\'\\n```\\n\\nTenzir takes care of parsing the type information properly and keeps IP\\naddresses and timestamps as native data types. You can also see in the examples\\nthat Tenzir handles multiple concatenated TSV logs of different schemas as you\'d\\nexpect.\\n\\nNow that Zeek logs are flowing, you can do a lot more than selecting specific\\ncolumns. Check out the [shaping guide](/next/usage/shape-data) for\\nfiltering rows, performing aggregations, and routing them elsewhere. Or [store\\nthe logs](/next/usage/import-into-a-node) locally at a Tenzir node in\\n[Parquet](https://parquet.apache.org) to process them with other data tools.\\n\\n### JSON\\n\\nZeek can also render logs as JSON by setting\\n[`LogAscii::use_json=T`](https://docs.zeek.org/en/master/frameworks/logging.html):\\n\\n```bash\\nzeek -r trace.pcap LogAscii::use_json=T\\n```\\n\\nAs with TSV, this generates one file per log type containing the NDJSON records.\\nHere are the same two entries from above:\\n\\n```json\\n{\\"ts\\":1258531221.486539,\\"uid\\":\\"C8b0xF1gjm7rOZXemg\\",\\"id.orig_h\\":\\"192.168.1.102\\",\\"id.orig_p\\":68,\\"id.resp_h\\":\\"192.168.1.1\\",\\"id.resp_p\\":67,\\"proto\\":\\"udp\\",\\"service\\":\\"dhcp\\",\\"duration\\":0.1638200283050537,\\"orig_bytes\\":301,\\"resp_bytes\\":300,\\"conn_state\\":\\"SF\\",\\"missed_bytes\\":0,\\"history\\":\\"Dd\\",\\"orig_pkts\\":1,\\"orig_ip_bytes\\":329,\\"resp_pkts\\":1,\\"resp_ip_bytes\\":328}\\n{\\"ts\\":1258531680.237254,\\"uid\\":\\"CMsxKW3uTZ3tSLsN0g\\",\\"id.orig_h\\":\\"192.168.1.103\\",\\"id.orig_p\\":137,\\"id.resp_h\\":\\"192.168.1.255\\",\\"id.resp_p\\":137,\\"proto\\":\\"udp\\",\\"service\\":\\"dns\\",\\"duration\\":3.780125141143799,\\"orig_bytes\\":350,\\"resp_bytes\\":0,\\"conn_state\\":\\"S0\\",\\"missed_bytes\\":0,\\"history\\":\\"D\\",\\"orig_pkts\\":7,\\"orig_ip_bytes\\":546,\\"resp_pkts\\":0,\\"resp_ip_bytes\\":0}\\n```\\n\\nAnd `http.log`:\\n\\n```json\\n{\\"ts\\":1258535653.087137,\\"uid\\":\\"CDsoEy4cHSHJRBvilg\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1191,\\"id.resp_h\\":\\"65.54.95.64\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"HEAD\\",\\"host\\":\\"download.windowsupdate.com\\",\\"uri\\":\\"/v9/windowsupdate/redir/muv4wuredir.cab?0911180916\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Windows-Update-Agent\\",\\"request_body_len\\":0,\\"response_body_len\\":0,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[]}\\n{\\"ts\\":1258535655.525107,\\"uid\\":\\"C8muAY3KSDGScVUrO4\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1192,\\"id.resp_h\\":\\"65.55.184.16\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"HEAD\\",\\"host\\":\\"www.update.microsoft.com\\",\\"uri\\":\\"/v9/windowsupdate/selfupdate/wuident.cab?0911180916\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Windows-Update-Agent\\",\\"request_body_len\\":0,\\"response_body_len\\":0,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[]}\\n```\\n\\nUse the regular [`json`](/next/formats/json) parser to get the data flowing:\\n\\n```bash\\ncat conn.log | tenzir \'read json --schema \\"zeek.conn\\" | head\'\\ncat http.log | tenzir \'read json --schema \\"zeek.http\\" | head\'\\n```\\n\\nThe option `--schema` of the `json` reader passes a name of a known schema that\\nbrings back the lost typing, e.g., the schema knows that the `duration` field in\\n`conn.log` is not a floating-point number, but a duration type, so that you can\\nfilter connections with `where duration < 4 mins`.\\n\\n### Streaming JSON\\n\\nThe above one-file-per-log format is not conducive to stream processing because\\na critical piece of information is missing: the type of the log (or *schema*),\\nwhich is only contained in the file name. So you can\'t just ship the data away\\nand infer the type later at ease. And passing the filename around through a side\\nchannel is cumbersome. Enter [JSON streaming\\nlogs](https://github.com/corelight/json-streaming-logs). This package adds two\\nnew fields: `_path` with the log type and `_write_ts` with the timestamp when\\nthe log was written. For example, `http.log` now gets an additional field\\n`{\\"_path\\": \\"http\\" , ...}`. This makes it a lot easier to consume, because you\\ncan now concatenate the entire output and multiplex it over a single stream.\\n\\nThis format doesn\'t come with stock Zeek. Use Zeek\'s package manager `zkg` to\\ninstall it:\\n\\n```bash\\nzkg install json-streaming-logs\\n```\\n\\nThen pass the package name to the list of scripts on the command line:\\n\\n```bash\\nzeek -r trace.pcap json-streaming-logs\\n```\\n\\nAnd now you get JSON logs in the current directory. Here\'s the same `conn.log`\\nand `http.log` example from above, this time with added `_path` and `_write_ts`\\nfields:\\n\\n```json title=\\"conn.log\\"\\n{\\"_path\\":\\"conn\\",\\"_write_ts\\":\\"2009-11-18T16:45:06.678526Z\\",\\"ts\\":\\"2009-11-18T16:43:56.223671Z\\",\\"uid\\":\\"CzFMRp2difzeGYponk\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1387,\\"id.resp_h\\":\\"74.125.164.85\\",\\"id.resp_p\\":80,\\"proto\\":\\"tcp\\",\\"service\\":\\"http\\",\\"duration\\":65.45066595077515,\\"orig_bytes\\":694,\\"resp_bytes\\":11708,\\"conn_state\\":\\"SF\\",\\"missed_bytes\\":0,\\"history\\":\\"ShADadfF\\",\\"orig_pkts\\":9,\\"orig_ip_bytes\\":1062,\\"resp_pkts\\":14,\\"resp_ip_bytes\\":12276}\\n```\\n\\n```json title=\\"http.log\\"\\n {\\"_path\\":\\"http\\",\\"_write_ts\\":\\"2009-11-18T17:00:51.888304Z\\",\\"ts\\":\\"2009-11-18T17:00:51.841527Z\\",\\"uid\\":\\"CgdQsm2eBBV8T8GjUk\\",\\"id.orig_h\\":\\"192.168.1.103\\",\\"id.orig_p\\":1399,\\"id.resp_h\\":\\"74.125.19.104\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"GET\\",\\"host\\":\\"www.google.com\\",\\"uri\\":\\"/\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)\\",\\"request_body_len\\":0,\\"response_body_len\\":10205,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[],\\"resp_fuids\\":[\\"FI1gWL1b9SuIA8HAv3\\"],\\"resp_mime_types\\":[\\"text/html\\"]}\\n```\\n\\nMany tools have have logic to disambiguate based on a field like `_path`. That\\nsaid, JSON is always \\"dumbed down\\" compared to TSV, which contains additional\\ntype information, such as timestamps, durations, IP addresses, etc. This type\\ninformation is lost in the JSON output and up to the downstream tooling to bring\\nback.\\n\\nWith JSON Streaming logs, you can simply concatenate all logs Zeek generated and\\npass them to a tool of your choice. Tenzir has native support for these logs via\\nthe [`zeek-json`](/next/formats/zeek-json) parser:\\n\\n```bash\\ncat *.log | tenzir \'read zeek-json | taste 1\'\\n```\\n\\nIn fact, the `zeek-json` parser just an alias for `json --selector=zeek:_path`,\\nwhich extracts the schema name from the `_path` field to demultiplex the JSON\\nstream and assign the corresponding schema.\\n\\n### Writer Plugin\\n\\nIf the stock options of Zeek\'s logging framework do not work for you, you can\\nstill write a C++ *writer plugin* to produce any output of your choice.\\n\\nFor example, the [zeek-kafka](https://github.com/SeisoLLC/zeek-kafka) plugin\\nwrites incoming Zeek data to Kafka topics. For this use case, you can also\\nleverage Tenzir\'s `kafka` connector and write:\\n\\n```bash\\ncat *.log | tenzir \'\\n  read zeek-tsv\\n  | extend _path=#schema\\n  | to kafka -t zeek write json\\n  \'\\n```\\n\\nThis pipeline starts by reading Zeek TSV, appends the `_path` field to emulate\\nStreaming JSON, and then writes the events to the Kafka topic `zeek`. The\\nexample is not equivalent to the Zeek Kafka plugin, because concatenate existing\\nfields and apply a (one-shot) pipeline, as opposed to continuously streaming to\\na Kafka topic. We\'ll elaborate on this in the next blog post, stay tuned.\\n\\n## Conclusion\\n\\nIn this blog, we presented the most common Zeek logging formats. We also\\nprovided examples how you can mobilize any of them in a Tenzir pipeline. If\\nyou\'re unsure when to use what Zeek logging format, here are our\\nrecommendations:\\n\\n:::tip Recommendation\\n- **Use TSV when you can.** If your downstream tooling can parse TSV, it is the\\n  best choice because it retains Zeek\'s rich type annotations as\\n  metadata\u2014without the need for downstream schema wrangling.\\n- **Use Streaming JSON for the easy button**. The single stream of NDJSON\\n  logs is most versatile, since most downstream tooling supports it well. Use it\\n  when you need to get in business quickly.\\n- **Use stock JSON when you must**. There\'s marginal utility in the\\n  one-JSON-file-per-log format. It requires extra effort in keeping track of\\n  filenames and mapping fields to their corresponding types.\\n- **Use plugins for everything else**. If none of these fit the bill or you\\n  need a tighter integration, leverage Zeek\'s writer plugins to create a custom\\n  logger.\\n:::\\n\\nIf you\'re a Zeek power user and need power tools for data processing, take a\\ncloser look at what we do at [Tenzir](https://tenzir.com). There\'s a lot more\\nyou can do!"},{"id":"/migrating-from-vast-to-tenzir","metadata":{"permalink":"/archive/migrating-from-vast-to-tenzir","source":"@site/archive/migrating-from-vast-to-tenzir/index.md","title":"Migrating from VAST to Tenzir","description":"VAST is now Tenzir. This blog post describes what changed when [we renamed the","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"vast","permalink":"/archive/tags/vast"},{"label":"community","permalink":"/archive/tags/community"},{"label":"project","permalink":"/archive/tags/project"}],"readingTime":1.55,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Migrating from VAST to Tenzir","authors":"dominiklohmann","date":"2023-06-26T00:00:00.000Z","tags":["tenzir","vast","community","project"]},"prevItem":{"title":"Mobilizing Zeek Logs","permalink":"/archive/mobilizing-zeek-logs"},"nextItem":{"title":"Visibility Across Space and Time is now Tenzir","permalink":"/archive/vast-to-tenzir"}},"content":"VAST is now Tenzir. This blog post describes what changed when [we renamed the\\nproject](/archive/vast-to-tenzir).\\n\\n![VAST to Tenzir](vast-to-tenzir.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## TL;DR\\n\\n- Use `tenzir-node` instead of `vast start`.\\n- Use `tenzir` instead of `vast exec`.\\n- Use `tenzir-ctl` for all other commands.\\n- Move your configuration from `<prefix>/etc/vast/vast.yaml` to\\n  `<prefix>/etc/tenzir/tenzir.yaml`.\\n- Move your configuration from `$XDG_CONFIG_HOME/vast/vast.yaml` to\\n  `$XDG_CONFIG_HOME/tenzir/tenzir.yaml`.\\n- In your configuration, replace `vast:` with `tenzir:`.\\n- Prefix environment variables with `TENZIR_` instead of `VAST_`.\\n\\nIn addition to that, the following things have changed.\\n\\n## Project\\n\\n- The repository moved from `tenzir/vast` to `tenzir/tenzir`.\\n- Our Discord server is now the *Tenzir Community*. Join us at\\n  <https://discord.tenzir.com>!\\n- The documentation moved from [vast.io](https://vast.io) to\\n  [docs.tenzir.com](https://docs.tenzir.com).\\n\\n## Usage\\n\\n- We\'re making the split between starting a node and starting a pipeline more\\n  obvious:\\n  - `tenzir` executes pipelines (previously `vast exec`).\\n  - `tenzir-node` starts a node (previously `vast start`).\\n  - Some commands have not yet been ported over to pipelines, and are accessible\\n    under `tenzir-ctl`; this will be phased out over time without deprecation\\n    notices as commands are moving into pipeline operators.\\n  - The `vast` executable exists for drop-in backwards compatibility and is\\n    equivalent to running `tenzir-ctl`.\\n- Configuration moved to use `tenzir` over `vast` where possible.\\n- Packages are now called *Tenzir* instead of *VAST*.\\n- The default install prefix of packages moved from `/opt/vast` to `/opt/tenzir`.\\n- The Docker image now includes the proprietary plugins\\n- There exit separate Docker images `tenzir/tenzir` and `tenzir/tenzir-node` to\\n  match the new binaries `tenzir` and `tenzir-node`, respectively.\\n- The PyVAST package is deprecated and now called Tenzir. We will bring it back\\n  with the Tenzir v4.0 release.\\n- The interop with Apache Arrow uses `tenzir.` prefixes for the extension type\\n  names now. We support reading the old files transparently, but tools\\n  interfacing will need to adapt to the new names `tenzir.ip`, `tenzir.subnet`,\\n  and `tenzir.enumeration`."},{"id":"/vast-to-tenzir","metadata":{"permalink":"/archive/vast-to-tenzir","source":"@site/archive/vast-to-tenzir/index.md","title":"Visibility Across Space and Time is now Tenzir","description":"Renaming our project from VAST to Tenzir","date":"2023-06-20T00:00:00.000Z","formattedDate":"June 20, 2023","tags":[{"label":"tenzir","permalink":"/archive/tags/tenzir"},{"label":"vast","permalink":"/archive/tags/vast"},{"label":"community","permalink":"/archive/tags/community"},{"label":"project","permalink":"/archive/tags/project"}],"readingTime":1.715,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Visibility Across Space and Time is now Tenzir","description":"Renaming our project from VAST to Tenzir","authors":"mavam","date":"2023-06-20T00:00:00.000Z","tags":["tenzir","vast","community","project"]},"prevItem":{"title":"Migrating from VAST to Tenzir","permalink":"/archive/migrating-from-vast-to-tenzir"},"nextItem":{"title":"From Slack to Discord","permalink":"/archive/from-slack-to-discord"}},"content":"After 5 years of developing two identities, the VAST project and Tenzir\\nthe company, we decided to streamline our efforts and **rename VAST to Tenzir**.\\n\\n\x3c!--truncate--\x3e\\n\\nVAST is the open-source software project Tenzir\'s Founder and CEO, Matthias\\nVallentin, created originally during his Master\'s at Technical University Munich\\nin 2006, and then continued to work on throughout his PhD from 2008 to 2016 at\\nthe University of California, Berkeley. The name is an acronym for Visibility\\nAcross Space and Time and originates from the HotSec\'08 paper about [Principles\\nfor Developing Comprehensive Network\\nVisibility](https://www.icir.org/mallman/papers/awareness-hotsec08.pdf) by Mark\\nAllman, Christian Kreibich, Vern Paxson, Robin Sommer, and Nicholas Weaver.\\n\\nThe following reasons ultimately drove our decision to rename VAST:\\n\\n- **Building two brands is double the effort**: we found that building two\\n  brands\u2014one for the VAST project and one for Tenzir as a company\u2014is a challenge\\n  for an early-stage startup like ourselves.\\n- **Taking a broader view when it comes to open source**: we also don\'t see our\\n  enterprise and open-source users as separate and isolated groupings. Instead\\n  we see a continuum. So we want to put all of our energy into a single, unified\\n  community.\\n- **Avoiding confusion and conflicts**: over the past years the term VAST has\\n  become very popular and widely used for a number of different projects and in\\n  adjacent industries (VAST Data, VAST.ai, anyone?) to avoid existing or future\\n  confusion we decided that it was better to retire the name but keep it part of\\n  our legacy and our story.\\n\\nOur next release will be Tenzir v4.0, continuing from the VAST v3.x series. In\\nother respects as well, the transition is an evolution, not a revolution. All\\nproject-related infrastructure, including the SemVer versioning, Git history,\\nGitHub repository, and so forth will remain the same. Stay tuned for a follow-up\\nblog post where we discuss the technical changes in depth and provide migration\\ninstructions.\\n\\nOh, and if you have any nostalgic anecdotes or stories to share, we\'d love to\\nhear them! Chime in on our [Discord chat](/discord) if you have any questions or\\nfeedback."},{"id":"/from-slack-to-discord","metadata":{"permalink":"/archive/from-slack-to-discord","source":"@site/archive/from-slack-to-discord/index.md","title":"From Slack to Discord","description":"Moving our community chat from Slack to Discord","date":"2023-02-09T00:00:00.000Z","formattedDate":"February 9, 2023","tags":[{"label":"community","permalink":"/archive/tags/community"},{"label":"chat","permalink":"/archive/tags/chat"},{"label":"discord","permalink":"/archive/tags/discord"}],"readingTime":0.785,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"From Slack to Discord","description":"Moving our community chat from Slack to Discord","authors":"mavam","image":"/img/blog/slack-to-discord.excalidraw.svg","date":"2023-02-09T00:00:00.000Z","tags":["community","chat","discord"]},"prevItem":{"title":"Visibility Across Space and Time is now Tenzir","permalink":"/archive/vast-to-tenzir"},"nextItem":{"title":"The New REST API","permalink":"/archive/the-new-rest-api"}},"content":"We are moving our community chat from Slack to Discord. Why? TL;DR: because\\nDiscord has better support for community building. VAST is not the first project\\nthat abandons Slack. [Numerous][meilisearch] [open-source][appwrite]\\n[projects][deepset] [have][sst] [done][qovery] [the][neo4j] [same][discord-oss].\\n\\n[meilisearch]: https://blog.meilisearch.com/from-slack-to-discord-our-migration/\\n[appwrite]: https://appwrite.io/\\n[deepset]: https://www.deepset.ai/blog/migration-to-discord\\n[sst]: https://sst.dev/blog/moving-to-discord.html\\n[qovery]: https://www.qovery.com/blog/feedback-from-slack-to-discord-13-months-later\\n[neo4j]: https://neo4j.com/blog/neo4j-community-is-migrating-from-slack-to-discord/\\n[discord-oss]: https://discord.com/open-source\\n\\n\x3c!--truncate--\x3e\\n\\n![Slack-to-Discord](/img/blog/slack-to-discord.excalidraw.svg)\\n\\n:::info Discord Invite Link\\nYou can join our Discord community chat via <https://docs.tenzir.com/discord>.\\n:::\\n\\nHere are the top four reasons why we are switching:\\n\\n- **Retention**: Slack\'s free plan has only 90 days message retention. We prefer\\n  permanence of our community discussion.\\n\\n- **Moderation**: Discord has solid moderation tools that rely on role-based\\n  access, and makes it possible adhere to our Code of Conduct upon joining.\\n\\n- **Invitation**: Unlimited invite links that do not expire.\\n\\n- **Inclusion**: Users can self-assign their preferred pronouns.\\n\\nWe hope that the majority of our Slack users understand these concerns and\\nwill join us over at Discord. See you there!"},{"id":"/the-new-rest-api","metadata":{"permalink":"/archive/the-new-rest-api","source":"@site/archive/the-new-rest-api/index.md","title":"The New REST API","description":"As of v2.4 VAST ships with a new web plugin that","date":"2023-01-26T00:00:00.000Z","formattedDate":"January 26, 2023","tags":[{"label":"frontend","permalink":"/archive/tags/frontend"},{"label":"rest","permalink":"/archive/tags/rest"},{"label":"api","permalink":"/archive/tags/api"},{"label":"architecture","permalink":"/archive/tags/architecture"}],"readingTime":6.89,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"The New REST API","authors":["lava","mavam"],"date":"2023-01-26T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","image":"/img/blog/rest-api-deployment-single.excalidraw.svg","tags":["frontend","rest","api","architecture"]},"prevItem":{"title":"From Slack to Discord","permalink":"/archive/from-slack-to-discord"},"nextItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/archive/parquet-and-feather-data-engineering-woes"}},"content":"As of [v2.4](/releases/vast-v2.4) VAST ships with a new `web` plugin that\\nprovides a REST API. The [API documentation](/api) describes the\\navailable endpoints also provides an\\n[OpenAPI](https://spec.openapis.org/oas/latest.html) spec for download. This\\nblog post shows how we built the API and what you can do with it.\\n\\n\x3c!--truncate--\x3e\\n\\nWhy does VAST need a REST API? Two reasons:\\n\\n1. **Make it easy to integrate with VAST**. To date, the only interface to VAST\\n   is the command line. This is great for testing and ad-hoc use cases, but to\\n   make it easy for other tools to integrate with VAST, a REST API is the common\\n   expectation.\\n\\n2. **Develop our own web frontend**. We are in the middle of building a\\n   [Svelte](https://svelte.dev/) frontend that delivers a web-based experience\\n   of interacting with VAST through the browser. This frontend interacts with\\n   VAST through the REST API.\\n\\nTwo architectural features of VAST made it really smooth to design the REST API:\\nPlugins and Actors.\\n\\nFirst, VAST\'s plugin system offers a flexible extension mechanism to add\\nadditional functionality without bloating the core. Specifically, we chose\\n[RESTinio](https://github.com/Stiffstream/restinio) as C++ library that\\nimplements an asynchronous HTTP and WebSocket server. Along with it comes a\\ndependency on Boost ASIO. We deem it acceptable to have this dependency of the\\n`web` plugin, but would feel less comfortable with adding dependencies to the\\nVAST core, which we try to keep as lean as possible.\\n\\nSecond, the actor model architecture of VAST makes it easy to\\nintegrate new \\"microservices\\" into the system. The `web` plugin is a *component\\nplugin* that provides a new actor with a typed messaging interface. It neatly\\nfits into the existing architecture and thereby inherits the flexible\\ndistribution and scaling properties. Concretely, there exist two ways to run the\\nREST API actor: either as a separate process or embedded inside a VAST server\\nnode:\\n\\n![REST API - Single Deployment](rest-api-deployment-single.excalidraw.svg)\\n\\nRunning the REST API as dedicated process gives you more flexibility with\\nrespect to deployment, fault isolation, and scaling. An embedded setup offers\\nhigher throughput and lower latency between the REST API and the other VAST\\ncomponents.\\n\\nThe REST API is also a *command plugin* and exposes the\u2014you guessed it\u2014`web`\\ncommand. To run the REST API as dedicated process, spin up a VAST node as\\nfollows:\\n\\n```bash\\nvast web server --certfile=/path/to/server.certificate --keyfile=/path/to/private.key\\n```\\n\\nTo run the server within the main VAST process, use a `start` command:\\n\\n```bash\\nvast start --commands=\\"web server [...]\\"\\n```\\n\\nThe server will only accept TLS requests by default. To allow clients to connect\\nsuccessfully, you need to pass a valid certificate and corresponding private key\\nwith the `--certfile` and `--keyfile` arguments.\\n\\n## Authentication\\n\\nClients must authenticate all requests with a valid token. The token is a short\\nstring that clients put in the `X-VAST-Token` request header.\\n\\nYou can generate a valid token on the command line as follows:\\n\\n```bash\\nvast web generate-token\\n```\\n\\nFor local testing and development, generating suitable certificates and tokens\\ncan be a hassle. For this scenario, you can start the server in [developer\\nmode](#developer-mode) where it accepts plain HTTP connections and does not\\nperform token authentication.\\n\\n## TLS Modes\\n\\nThere exist four modes to start the REST API, each of which suits a slightly\\ndifferent use case.\\n\\n### Developer Mode\\n\\nThe developer mode bypasses encryption and authentication token verification.\\n\\n![REST API - Developer Mode](rest-api-mode-developer.excalidraw.svg)\\n\\nPass `--mode=dev` to start the REST API in developer mode:\\n\\n```bash\\nvast web server --mode=dev\\n```\\n\\n### Server Mode\\n\\nThe server mode reflects the \\"traditional\\" mode of operation where VAST binds to\\na network interface. This mode only accepts HTTPS connections and requires a\\nvalid authentication token for every request. This is the default mode of\\noperation.\\n\\n![REST API - Server Mode](rest-api-mode-server.excalidraw.svg)\\n\\nPass `--mode=server` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=server\\n```\\n\\n### Upstream TLS Mode\\n\\nThe upstream TLS mode is suitable when VAST sits upstream of a separate\\nTLS terminator that is running on the same machine. This kind of setup\\nis commonly encountered when running nginx as a reverse proxy.\\n\\n![REST API - Developer Mode](rest-api-mode-developer.excalidraw.svg)\\n\\nVAST only listens on localhost addresses, accepts plain HTTP but still\\nchecks authentication tokens.\\n\\nPass `--mode=upstream` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=upstream\\n```\\n\\n### Mutual TLS Mode\\n\\nThe mutual TLS mode is suitable when VAST sits upstream of a separate TLS\\nterminator that may be running on a different machine. In this scenario,\\nthe connection between the terminator and VAST must again be encrypted\\nto avoid leaking the authentication token to the network.\\n\\nRegular TLS requires only the server to present a certificate to prove his\\nidentity. In mutual TLS mode, the client additionally needs to provide a\\nvalid *client certificate* to the server. This ensures that the TLS terminator\\ncannot be impersonated or bypassed.\\n\\nTypically self-signed certificates are used for that purpose, since both ends of\\nthe connection are configured together and not exposed to the public internet.\\n\\n![REST API - mTLS Mode](rest-api-mode-mtls.excalidraw.svg)\\n\\nPass `--mode=mtls` to start the REST API in mutual TLS mode:\\n\\n```bash\\nvast web server --mode=mtls\\n```\\n\\n## Usage Examples\\n\\nNow that you know how we put the REST API together, let\'s look at some\\nend-to-end examples.\\n\\n### See what\'s inside VAST\\n\\nOne straightforward example is checking the number of records in VAST:\\n\\n```bash\\ncurl \\"https://vast.example.org:42001/api/v0/status?verbosity=detailed\\" \\\\\\n  | jq .index.statistics\\n```\\n\\n```json\\n{\\n  \\"events\\": {\\n    \\"total\\": 8462\\n  },\\n  \\"layouts\\": {\\n    \\"zeek.conn\\": {\\n      \\"count\\": 8462,\\n      \\"percentage\\": 100\\n    }\\n  }\\n}\\n```\\n\\n:::caution Status changes in v3.0\\nIn the upcoming v3.0 release, the statistics under the key `.index.statistics`\\nwill move to `.catalog`. This change is already merged into the master branch.\\nConsult the status key reference for details.\\n:::\\n\\n### Perform a HTTP health check\\n\\nThe `/status` endpoint can also be used as a HTTP health check in\\n`docker-compose`:\\n\\n```yaml\\nversion: \'3.4\'\\nservices:\\n  web:\\n    image: tenzir/vast\\n    environment:\\n      - \\"VAST_START__COMMANDS=web server --mode=dev\\"\\n    ports:\\n      - \\"42001:42001\\"\\n    healthcheck:\\n      test: curl --fail http://localhost:42001/status || exit 1\\n      interval: 60s\\n      retries: 5\\n      start_period: 20s\\n      timeout: 10s\\n```\\n\\n### Run a query\\n\\nThe other initial endpoints can be used to get data out of VAST. For example, to\\nget up to two `zeek.conn` events which connect to the subnet `192.168.0.0/16`, using\\nthe VAST query expression `net.src.ip in 192.168.0.0/16`:\\n\\n```bash\\ncurl \\"http://127.0.0.1:42001/api/v0/export?limit=2&expression=net.src.ip%20in%20192.168.0.0%2f16\\"\\n```\\n\\n```json\\n{\\n  \\"version\\": \\"v2.4.0-457-gb35c25d88a\\",\\n  \\"num_events\\": 2,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\nNote that when using `curl`, all request parameters need to be properly\\nurlencoded. This can be cumbersome for the `expression` and `pipeline`\\nparameters, so we also provide an `/export` POST endpoint that accepts\\nparameters in the JSON body. The next example shows how to use POST requests\\nfrom curl. It also uses the `/query` endpoint instead of `/export` to get\\nresults iteratively instead of a one-shot result. The cost for this is having to\\nmake two API calls instead of one:\\n\\n```bash\\ncurl -XPOST -H\\"Content-Type: application/json\\" -d\'{\\"expression\\": \\"udp\\"}\' http://127.0.0.1:42001/api/v0/query/new\\n```\\n\\n```json\\n{\\"id\\": \\"31cd0f6c-915f-448e-b64a-b5ab7aae2474\\"}\\n```\\n\\n```bash\\ncurl http://127.0.0.1:42001/api/v0/query/31cd0f6c-915f-448e-b64a-b5ab7aae2474/next?n=2 | jq\\n```\\n\\n```json\\n{\\n  \\"position\\": 0,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\n:::note Still Experimental\\nPlease note that we consider the API version `v0` experimental, and we make no\\nstability guarantees at the moment.\\n:::\\n\\nAs always, if you have any question on usage, swing by our [community\\nchat](/discord). Missing routes? Let us know so that we know\\nwhat to prioritize. Now happy curling! :curling_stone:"},{"id":"/parquet-and-feather-data-engineering-woes","metadata":{"permalink":"/archive/parquet-and-feather-data-engineering-woes","source":"@site/archive/parquet-and-feather-data-engineering-woes/index.md","title":"Parquet & Feather: Data Engineering Woes","description":"Apache Arrow and [Apache","date":"2023-01-10T00:00:00.000Z","formattedDate":"January 10, 2023","tags":[{"label":"arrow","permalink":"/archive/tags/arrow"},{"label":"parquet","permalink":"/archive/tags/parquet"},{"label":"feather","permalink":"/archive/tags/feather"}],"readingTime":7.09,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Data Engineering Woes","authors":"dispanser","date":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"The New REST API","permalink":"/archive/the-new-rest-api"},"nextItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/archive/parquet-and-feather-writing-security-telemetry"}},"content":"[Apache Arrow](https://arrow.apache.org/) and [Apache\\nParquet](https://parquet.apache.org) have become the de-facto columnar formats\\nfor in-memory and on-disk representations when it comes to structured data.\\nBoth are strong together, as they provide data interoperability and foster a\\ndiverse ecosystem of data tools. But how well do they actually work together\\nfrom an engineering perspective?\\n\\n\x3c!--truncate--\x3e\\n\\nIn our previous posts, we introduced the formats and did a quantitative\\ncomparison of Parquet and Feather-on the write path. In this post, we look at\\nthe developer experience.\\n\\n:::info Parquet & Feather: 3/3\\nThis blog post is the last part of a 3-piece series on Parquet and Feather.\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. This blog post\\n\\n[parquet-and-feather-1]: /archive/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /archive/parquet-and-feather-writing-security-telemetry/\\n:::\\n\\nWhile our Feather implementation proved to be straight-forward, the Parquet\\nstore implementation turned out to be more difficult. Recall that VAST has its\\nown type system relying on [Arrow extension\\ntypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types) to\\nexpress domain-specific concepts like IP addresses, subnets, or enumerations. We\\nhit a few places where the Arrow C++ implementation does not support all VAST\\ntypes directly. It\'s trickier than we thought, as we see next.\\n\\n## Row Groups\\n\\nIn Apache Parquet, a [row group](https://parquet.apache.org/docs/concepts/) is a\\nsubset of a Parquet file that\'s itself written in a columnar fashion. Smaller\\nrow groups allow for higher granularity in reading parts of an individual file,\\nat the expense of a potentially increased file size due to less optimal\\nencoding. In VAST, we send around batches of data\\nthat are considerably smaller than what a recommended Parquet file size would\\nlook like. A typical Parquet file size recommendation is 1GB, which translates\\nto 5\u201310GB of data in memory when reading the entire file. To produce files sized\\nin this order of magnitude, we planned to use individual row groups, each of\\nwhich aligned with the size of our Arrow record batches that comprise\\n2<sup>16</sup> events occupying a few MBs.\\n\\nHowever, attempting to read a Parquet file that was split into multiple row\\ngroups doesn\'t work for some of our schemas, yielding:\\n\\n```\\nNotImplemented: Nested data conversions not implemented for chunked array outputs\\n```\\n\\nThis appears to be related to\\n[ARROW-5030](https://issues.apache.org/jira/browse/ARROW-5030). Our current\\nworkaround is to write a single row group, and split up the resulting Arrow\\nrecord batches into the desired size after reading. However, this increases\\nlatency to first result, an important metric for some interactive use cases we\\nenvision for VAST.\\n\\n## Arrow \u2192 Parquet \u2192 Arrow Roundtrip Schema Mismatch\\n\\nParquet is a separate project which precedes Arrow, and has its own data types,\\nwhich don\'t exactly align with what Arrow provides. While it\'s possible to\\ninstruct Arrow to also serialize [its own\\nschema](https://arrow.apache.org/docs/cpp/api/formats.html#_CPPv4N7parquet21ArrowWriterProperties7BuilderE)\\ninto the Parquet file metadata, this doesn\'t seem to play well in concert with\\nextension types. As a result, a record batch written to and then read from a\\nParquet file no longer adheres to the same schema!\\n\\nThis bit us in the following scenarios.\\n\\n### VAST Enumerations\\n\\nVAST comes with an enumeration type that represents a fixed mapping of strings\\nto numeric values, where the mapping is part of the type metadata. We represent\\nenums as extension types wrapping an Arrow dictionary of strings backed by\\nunsigned 8-bit integers. On read, Arrow turns these 8-bit index values into\\n32-bit values, which is not compatible with our extension type definition, so\\nthe extension type wrapping is lost. The diagram below illustrates this issue.\\n\\n![Arrow Schema Conversion](arrow-schema-conversion.excalidraw.svg)\\n\\n### Extension Types inside Maps\\n\\nBoth our address type and subnet type extensions are lost if they occur in\\nnested records. For example, a map from a VAST address to a VAST enumeration of\\nthe following Arrow type is not preserved:\\n\\n```\\nmap<extension<vast.address>, extension<vast.enumeration>>\\n```\\n\\nAfter reading it from a Parquet file, the resulting Arrow type is:\\n\\n```\\nmap<fixed_size_binary[16], string>.\\n```\\n\\nThe key, an address type, has been replaced by its physical representation,\\nwhich is 16 bytes (allowing room for an IPv6 address). Interestingly, the\\nenumeration is replaced by a string instead of a dictionary as observed in the\\nprevious paragraph. So the same type behaves differently depending on where in\\nthe schema it occurs.\\n\\nWe created an issue in the Apache JIRA to track this:\\n[ARROW-17839](https://issues.apache.org/jira/browse/ARROW-17839).\\n\\nTo fix these 3 issues, we\'re post-processing the data after reading it from\\nParquet. The workaround is a multi-step process:\\n\\n1. Side-load the Arrow schema from the Parquet metadata. This yields the actual\\n   schema, because it\'s in no way related to Parquet other than using its\\n   metadata capabilities to store it.\\n\\n1. Load the actual Arrow table. This table has its own schema, which is not the\\n   same schema as the one derived from the Parquet metadata directly.\\n\\n1. Finally, recursively walk the two schema trees with the associated data\\n   columns, and whenever there\'s a mismatch between the two, fix the data arrays\\n   by casting or transforming it, yielding a table that is aligned with the\\n   expected schema.\\n\\n   - In the first case (`dictionary` vs `vast.enumeration`) we cast the `int32`\\n     Arrow array of values into a `uint8` Arrow array, and manually create the\\n     wrapping extension type and extension array. This is relatively cheap, as\\n     casting is cheap and the wrapping is done at the array level, not the value\\n     level.\\n\\n   - In the second case (physical `binary[16]` instead of `vast.address`) we\\n     just wrap it in the appropriate extension type. Again, this is a cheap\\n     operation.\\n\\n   - The most expensive fix-up we perform is when the underlying type has been\\n     changed from an enumeration to a string: we have to create the entire array\\n     from scratch after building a lookup table that translates the string values\\n     into their corresponding numerical representation.\\n\\n## Apache Spark Support\\n\\nSo now VAST writes its data into a standardized, open format\u2014we integrate\\nseamlessly with the entire big data ecosystem, for free, right? I can read my\\nVAST database with Apache Spark and analyze security telemetry data on a\\n200-node cluster?\\n\\nNope. It\u2019s not *that* standardized. Yet. Not every tool or library supports\\nevery data type. In fact, as discussed above, writing a Parquet file and reading\\nit back *even with the same tool* doesn\'t always produce the data you started\\nwith.\\n\\nWe attempting to load a Parquet file with a single row, and a single field of\\ntype VAST\'s `count` (a 64-bit unsigned integer) into Apache Spark v3.2, we are\\ngreeted with:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false))\\n  at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1284)\\n```\\n\\nApache Spark v3.2 refuses to read the `import_time` field (a metadata column\\nadded by VAST itself). It turns out that Spark v3.2 has a\\n[regression](https://issues.apache.org/jira/browse/SPARK-40819). Let\'s try with\\nversion v3.1 instead, which shouldn\u2019t have this problem:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Parquet type not supported: INT64 (UINT_64)\\n```\\n\\nWe got past the timestamp issue, but it still doesn\'t work: Spark only supports\\nsigned integer types, and refuses to load our Parquet file with an unsigned 64\\nbit integer value. The [related Spark JIRA\\nissue](https://issues.apache.org/jira/browse/SPARK-10113) is marked as resolved,\\nbut unfortunately the resolution is \\"a better error message.\\" However, [this\\nstack overflow post](https://stackoverflow.com/q/64383029) has the solution: if\\nwe define an explicit schema, Spark happily converts our column into a signed\\ntype.\\n\\n```scala\\nval schema = StructType(\\n  Array(\\n    StructField(\\"event\\",\\n      StructType(\\n        Array(\\n          StructField(\\"c\\", LongType))))))\\n```\\n\\nFinally, it works!\\n\\n```\\nscala> spark.read.schema(schema).parquet(<file>).show()\\n+-----+\\n|event|\\n+-----+\\n| {13}|\\n+-----+\\n```\\n\\nWe were able to read VAST data in Spark, but it\'s not an easy and out-of-the-box\\nexperience we were hoping for. It turns out that different tools don\'t always\\nsupport all the data types, and additional effort is required to integrate with\\nthe big players in the Parquet ecosystem.\\n\\n## Conclusion\\n\\nWe love Apache Arrow\u2014it\'s a cornerstone of our system, and we\'d be in much\\nworse shape without it. We use it everywhere from the storage layer (using\\nFeather and Parquet) to the data plane (where we are passing around Arrow record\\nbatches).\\n\\nHowever, as VAST uses a few less common Arrow features we sometimes stumble over\\nsome of the rougher edges. We\'re looking forward to fixing some of these things\\nupstream, but sometimes you just need a quick solution to help our users.\\n\\nThe real reason why we wrote this blog post is to show how quickly the data\\nengineering can escalate. This is the long tail that nobody wants to talk about\\nwhen telling you to build your own security data lake. And it quickly adds up!\\nIt\'s also heavy-duty data wrangling, and not ideally something you want your\\nsecurity team working on when they would be more useful hunting threats. Even\\nmore reasons to use a purpose-built security data technology like VAST."},{"id":"/parquet-and-feather-writing-security-telemetry","metadata":{"permalink":"/archive/parquet-and-feather-writing-security-telemetry","source":"@site/archive/parquet-and-feather-writing-security-telemetry/index.md","title":"Parquet & Feather: Writing Security Telemetry","description":"How does Apache Parquet compare to Feather for storing","date":"2022-10-24T00:00:00.000Z","formattedDate":"October 24, 2022","tags":[{"label":"benchmark","permalink":"/archive/tags/benchmark"},{"label":"arrow","permalink":"/archive/tags/arrow"},{"label":"parquet","permalink":"/archive/tags/parquet"},{"label":"feather","permalink":"/archive/tags/feather"},{"label":"quarto","permalink":"/archive/tags/quarto"},{"label":"r","permalink":"/archive/tags/r"}],"readingTime":26.585,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Parquet & Feather: Writing Security Telemetry","authors":["dispanser","mavam"],"date":"2022-10-24T00:00:00.000Z","last_updated":"2023-02-08T00:00:00.000Z","tags":["benchmark","arrow","parquet","feather","quarto","r"]},"prevItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/archive/parquet-and-feather-data-engineering-woes"},"nextItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/archive/parquet-and-feather-enabling-open-investigations"}},"content":"How does Apache [Parquet](https://parquet.apache.org/) compare to [Feather](https://arrow.apache.org/docs/python/feather.html) for storing\\nstructured security data? In this blog post, we answer this question.\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 2/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1. [Enabling Open Investigations](/archive/parquet-and-feather-enabling-open-investigations/)\\n2. This blog post\\n3. [Data Engineering Woes](/archive/parquet-and-feather-data-engineering-woes/)\\n\\n:::\\n\\nIn the [previous blog](/archive/parquet-and-feather-enabling-open-investigations/), we explained why Parquet and\\nFeather are great building blocks for modern investigations. In this blog, we take\\na look at how they actually perform on the write path in two dimensions:\\n\\n- **Size**: how much space does typical security telemetry occupy?\\n- **Speed**: how fast can we write out to a store?\\n\\nParquet and Feather have different goals. While Parquet is an on-disk format\\nthat optimizes for size, Feather is a thin layer around the native Arrow\\nin-memory representation. This puts them at different points in the spectrum of\\nthroughput and latency.\\n\\nTo better understand this spectrum, we instrumented the write path of VAST,\\nwhich consists roughly of the following steps:\\n\\n1. Parse the input\\n2. Convert it into Arrow record batches\\n3. Ship Arrow record batches to a VAST server\\n4. Write Arrow record batches out into a Parquet or Feather store\\n5. Create an index from Arrow record batches\\n\\nSince steps (1\u20133) and (5) are the same for both stores, we ignore them in the\\nfollowing analysis and solely zoom in on (4).\\n\\n## Dataset\\n\\nFor our evaluation, we use a dataset that models a \u201cnormal day in a corporate\\nnetwork\u201d fused with data from for real-world attacks. While this approach might\\nnot be ideal for detection engineering, it provides enough diversity to analyze\\nstorage and processing behavior.\\n\\nSpecifically, we rely on a 3.77 GB PCAP trace of the [M57 case study](https://www.sciencedirect.com/science/article/pii/S1742287612000370). We\\nalso injected real-world attacks from\\n[malware-traffic-analysis.net](https://www.malware-traffic-analysis.net/index.html) into the PCAP trace. To\\nmake the timestamps look somewhat realistic, we shifted the timestamps of the\\nPCAPs to pretend that the corresponding activity happens on the same day. For\\nthis we used [`editcap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolseditcap.html) and then merged the resulting PCAPs into one\\nbig file using [`mergecap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolsmergecap.html).\\n\\nWe then ran [Zeek](https://zeek.org) and [Suricata](https://suricata.io) over\\nthe trace to produce structured logs. For full reproducibility, we host this\\ncustom data set in a [Google Drive folder](https://drive.google.com/drive/folders/1mPJYVGKTk86P2JU3KD-WFz8tUkTLK095?usp=sharing).\\n\\nVAST can ingest PCAP, Zeek, and Suricata natively. All three data sources are\\nhighly valuable for detection and investigation, which is why we use them in\\nthis analysis. They represent a good mix of nested and structured data (Zeek &\\nSuricata) vs.\xa0simple-but-bulky data (PCAP). To give you a flavor, here\u2019s an\\nexample Zeek log:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   http\\n    #open   2022-04-20-09-56-45\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   trans_depth method  host    uri referrer    version user_agent  origin  request_body_len    response_body_len   status_code status_msg  info_code   info_msg    tags    username    password    proxied orig_fuids  orig_filenames  orig_mime_types resp_fuids  resp_filenames  resp_mime_types\\n    #types  time    string  addr    port    addr    port    count   string  string  string  string  string  string  string  count   count   count   string  count   string  set[enum]   string  string  set[string] vector[string]  vector[string]  vector[string]  vector[string]  vector[string]  vector[string]\\n    1637155963.249475   CrkwBA3xeEV9dzj1n   128.14.134.170  57468   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36     -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FhEFqzHx1hVpkhWci   -   text/html\\n    1637157241.722674   Csf8Re1mi6gYI3JC6f  87.251.64.137   64078   198.71.247.91   80  1   -   -   -   -   1.1 -   -   0   18  400 Bad Request -   -   (empty) -   -   -   -   -   -   FpKcQG2BmJjEU9FXwh  -   text/html\\n    1637157318.182504   C1q1Lz1gxAAyf4Wrzk  139.162.242.152 57268   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0  -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FyTOLL1rVGzjXoNAb   -   text/html\\n    1637157331.507633   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  1   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   Fnmp6k1xVFoqqIO5Ub  -   text/html\\n    1637157331.750342   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  2   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F1uLr1giTpXx81dP4   -   text/html\\n    1637157331.915255   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  3   GET lifeisnetwork.com   /wp-includes/wlwmanifest.xml    -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F9dg5w2y748yNX9ZCc  -   text/html\\n    1637157331.987527   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  4   GET lifeisnetwork.com   /xmlrpc.php?rsd -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   FxzLxklm7xyuzTF8h   -   text/html\\n\\nHere\u2019s a snippet of a Suricata log:\\n\\n``` json\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.262184+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":7,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"128.14.134.170\\",\\"src_port\\":57468,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.237882+0100\\",\\"flow_id\\":675134617085815,\\"event_type\\":\\"flow\\",\\"src_ip\\":\\"54.176.143.72\\",\\"dest_ip\\":\\"198.71.247.91\\",\\"proto\\":\\"ICMP\\",\\"icmp_type\\":8,\\"icmp_code\\":0,\\"response_icmp_type\\":0,\\"response_icmp_code\\":0,\\"flow\\":{\\"pkts_toserver\\":1,\\"pkts_toclient\\":1,\\"bytes_toserver\\":50,\\"bytes_toclient\\":50,\\"start\\":\\"2021-11-17T14:43:34.649079+0100\\",\\"end\\":\\"2021-11-17T14:43:34.649210+0100\\",\\"age\\":0,\\"state\\":\\"established\\",\\"reason\\":\\"timeout\\",\\"alerted\\":false},\\"community_id\\":\\"1:WHH+8OuOygRPi50vrH45p9WwgA4=\\"}\\n{\\"timestamp\\":\\"2021-11-17T14:32:48.254950+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":10,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"128.14.134.170\\",\\"dest_port\\":57468,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.327585+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":206,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"139.162.242.152\\",\\"src_port\\":57268,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:gEyyy4v7MJSsjLvl+3D17G/rOIY=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.329669+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":208,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"139.162.242.152\\",\\"dest_port\\":57268,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.569634+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":224,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.750383+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":226,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.812254+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":228,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":1,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.915298+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":230,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":1}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.977269+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":232,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":2,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.987556+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":234,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/wp-includes/wlwmanifest.xml\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":2}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.049539+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":236,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":3,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.057985+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":238,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/xmlrpc.php\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":3}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.119589+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":239,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":4,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.127935+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":241,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":4}}\\n```\\n\\nNote that Zeek\u2019s tab-separated value (TSV) format is already a structured table,\\nwhereas Suricata data needs to be demultiplexed first through the `event_type`\\nfield.\\n\\nThe PCAP packet type is currently hard-coded in VAST\u2019s PCAP plugin and looks\\nlike this:\\n\\n``` go\\ntype pcap.packet = record {\\n  time: timestamp,\\n  src: addr,\\n  dst: addr,\\n  sport: port,\\n  dport: port,\\n  vlan: record {\\n    outer: count,\\n    inner: count,\\n  },\\n  community_id: string #index=hash,\\n  payload: string #skip,\\n}\\n```\\n\\nNow that we\u2019ve looked at the structure of the dataset, let\u2019s take a look at our\\nmeasurement methodology.\\n\\n### Measurement\\n\\nOur objective is understanding the storage and runtime characteristics of\\nParquet and Feather on the provided input data. To this end, we instrumented\\nVAST to produce us with a measurement trace file that we then analyze with R for\\ngaining insights. The [corresponding patch](feather-parquet-zstd-experiments.diff) is not meant for further\\nproduction, so we kept it separate. But we did find an opportunity to improve\\nVAST and [made the Zstd compression level configurable](https://github.com/tenzir/vast/pull/2623). Our [benchmark\\nscript](benchmark.fish) is available for full reproducibility.\\n\\nOur instrumentation produced a [CSV file](data.csv) with the following features:\\n\\n- **Store**: the type of store plugin used in the measurement, i.e., `parquet`\\n  or `feather`.\\n- **Construction time**: the time it takes to convert Arrow record batches into\\n  Parquet or Feather. We fenced the corresponding code blocks and computed the\\n  difference in nanoseconds.\\n- **Input size**: the number of bytes that the to-be-converted record batches\\n  consume.\\n- **Output size**: the number of bytes that the store file takes up.\\n- **Number of events**: the total number of events in all input record batches\\n- **Number of record batches**: the number Arrow record batches per store\\n- **Schema**: the name of the schema; there exists one store file per schema\\n- **Zstd compression level**: the applied Zstd compression level\\n\\nEvery row corresponds to a single store file where we varied some of these\\nparameters. We used [hyperfine](https://github.com/sharkdp/hyperfine) as\\nbenchmark driver tool, configured with 8 runs. Let\u2019s take a look at the data.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(scales)\\nlibrary(stringr)\\nlibrary(tidyr)\\n\\n# For faceting, to show clearer boundaries.\\ntheme_bw_trans <- function(...) {\\n  theme_bw(...) +\\n  theme(panel.background = element_rect(fill = \\"transparent\\"),\\n        plot.background = element_rect(fill = \\"transparent\\"),\\n        legend.key = element_rect(fill = \\"transparent\\"),\\n        legend.background = element_rect(fill = \\"transparent\\"))\\n}\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read.csv(\\"data.csv\\") |>\\n  rename(store = store_type) |>\\n  mutate(duration = dnanoseconds(duration))\\n\\noriginal <- read.csv(\\"sizes.csv\\") |>\\n  mutate(store = \\"original\\", store_class = \\"original\\") |>\\n  select(store, store_class, schema, bytes)\\n\\n# Global view on number of events per schema.\\nschemas <- data |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\n# Normalize store sizes by number of events/store.\\nnormalized <- data |>\\n  mutate(duration_normalized = duration / num_events,\\n         bytes_memory_normalized = bytes_memory / num_events,\\n         bytes_storage_normalized = bytes_in_storage / num_events,\\n         bytes_ratio = bytes_in_storage / bytes_memory)\\n\\n# Compute average over measurements.\\naggregated <- normalized |>\\n  group_by(store, schema, zstd.level) |>\\n  summarize(duration = mean(duration_normalized),\\n            memory = mean(bytes_memory_normalized),\\n            storage = mean(bytes_storage_normalized))\\n\\n# Treat in-memory measurements as just another storage type.\\nmemory <- aggregated |>\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  mutate(store = \\"memory\\", store_class = \\"memory\\") |>\\n  select(store, store_class, schema, bytes = memory)\\n\\n# Unite with rest of data.\\nunified <-\\n  aggregated |>\\n  select(-memory) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  rename(bytes = storage, store_class = store) |>\\n  unite(\\"store\\", store_class, zstd.level, sep = \\"+\\", remove = FALSE)\\n\\nschemas_gt10k <- schemas |> filter(n > 10e3) |> pull(schema)\\nschemas_gt100k <- schemas |> filter(n > 100e3) |> pull(schema)\\n\\n# Only schemas with > 100k events.\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k)\\n\\n# Helper function to format numbers with SI unit suffixes.\\nfmt_short <- function(x) {\\n  scales::label_number(scale_cut = cut_short_scale(), accuracy = 0.1)(x)\\n}\\n```\\n\\n</details>\\n\\n### Schemas\\n\\nWe have a total of 42 unique schemas:\\n\\n     [1] \\"zeek.dce_rpc\\"       \\"zeek.dhcp\\"          \\"zeek.x509\\"         \\n     [4] \\"zeek.dpd\\"           \\"zeek.ftp\\"           \\"zeek.files\\"        \\n     [7] \\"zeek.ntlm\\"          \\"zeek.kerberos\\"      \\"zeek.ocsp\\"         \\n    [10] \\"zeek.ntp\\"           \\"zeek.dns\\"           \\"zeek.packet_filter\\"\\n    [13] \\"zeek.pe\\"            \\"zeek.radius\\"        \\"zeek.http\\"         \\n    [16] \\"zeek.reporter\\"      \\"zeek.weird\\"         \\"zeek.smb_files\\"    \\n    [19] \\"zeek.sip\\"           \\"zeek.smb_mapping\\"   \\"zeek.smtp\\"         \\n    [22] \\"zeek.conn\\"          \\"zeek.snmp\\"          \\"zeek.tunnel\\"       \\n    [25] \\"zeek.ssl\\"           \\"suricata.krb5\\"      \\"suricata.ikev2\\"    \\n    [28] \\"suricata.http\\"      \\"suricata.smb\\"       \\"suricata.ftp\\"      \\n    [31] \\"suricata.dns\\"       \\"suricata.fileinfo\\"  \\"suricata.tftp\\"     \\n    [34] \\"suricata.snmp\\"      \\"suricata.sip\\"       \\"suricata.anomaly\\"  \\n    [37] \\"suricata.smtp\\"      \\"suricata.dhcp\\"      \\"suricata.tls\\"      \\n    [40] \\"suricata.dcerpc\\"    \\"suricata.flow\\"      \\"pcap.packet\\"       \\n\\nThe schemas belong to three data *modules*: Zeek, Suricata, and PCAP. A module\\nis the prefix of a concrete type, e.g., for the schema `zeek.conn` the module is\\n`zeek` and the type is `conn`. This is only a distinction in terminology,\\ninternally VAST stores the full-qualified type as schema name.\\n\\nHow many events do we have per schema?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas <- normalized |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    scale_y_log10(labels = scales::label_comma()) +\\n    labs(x = \\"Schema\\", y = \\"Number of Events\\", fill = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/number-of-events-by-schema-1.svg\';\\n\\n<Svg1 />\\n\\nThe above plot (log-scaled y-axis) shows how many events we have per type.\\nBetween 1 and 100M events, we almost see everything.\\n\\nWhat\u2019s the typical event size?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = bytes_memory / n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    labs(x = \\"Schema\\", y = \\"Bytes (in-memory)\\", color = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/event-size-by-schema-1.svg\';\\n\\n<Svg2 />\\n\\nThe above plot keeps the x-axis from the previous plot, but exchanges the y-axis\\nto show normalized event size, in memory after parsing. Most events\\ntake up a few 100 bytes, with packet data consuming a bit more, and one 5x\\noutlier: `suricata.ftp`.\\n\\nSuch distributions are normal, even with these outliers. Some telemetry events\\nsimply have more string data that\u2019s a function of user input. For `suricata.ftp`\\nspecifically, it can grow linearly with the data transmitted. Here\u2019s a stripped\\ndown example of an event that is greater than 5 kB in its raw JSON:\\n\\n``` json\\n{\\n  \\"timestamp\\": \\"2021-11-19T05:08:50.885981+0100\\",\\n  \\"flow_id\\": 1339403323589433,\\n  \\"pcap_cnt\\": 5428538,\\n  \\"event_type\\": \\"ftp\\",\\n  \\"src_ip\\": \\"10.5.5.101\\",\\n  \\"src_port\\": 50479,\\n  \\"dest_ip\\": \\"62.24.128.228\\",\\n  \\"dest_port\\": 110,\\n  \\"proto\\": \\"TCP\\",\\n  \\"tx_id\\": 12,\\n  \\"community_id\\": \\"1:kUFeGEpYT1JO1VCwF8wZWUWn0J0=\\",\\n  \\"ftp\\": {\\n    \\"completion_code\\": [\\n      \\"155\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\"188\\",\\n      \\"188\\",\\n      \\"188\\"\\n    ],\\n    \\"reply\\": [\\n      \\" 41609\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\" 125448\\",\\n      \\" 126158\\",\\n      \\" 29639\\"\\n    ],\\n    \\"reply_received\\": \\"yes\\"\\n  }\\n}\\n```\\n\\nThis matches our mental model. A few hundred bytes per event with some outliers.\\n\\n### Batching\\n\\nOn the inside, a store is a concatenation of homogeneous Arrow record batches,\\nall having the same schema.\\n\\nThe Feather format is essentially the IPC wire format of record batches. Schemas\\nand dictionaries are only included when they change. For our stores, this means\\njust once in the beginning. In order to access a given row in a Feather file,\\nyou need to start at the beginning, iterate batch by batch until you arrive at\\nthe desired batch, and then materialize it before you can access the desired\\nrow via random access.\\n\\nParquet has *row groups* that are much like a record batch, except that they are\\ncreated at write time, so Parquet determines their size rather than the incoming\\ndata. Parquet offers random access over both the row groups and within an\\nindividual batch that is materialized from a row group. The on-disk layout of\\nParquet is still row-group by row-group, and in that column by column, so\\nthere\u2019s no big difference between Parquet and Feather in that regard. Parquet\\nencodes columns using different encoding techniques than Arrow\u2019s IPC format.\\n\\nMost stores only consist of a few record batches. PCAP is the only difference.\\nSmall stores are suboptimal because the catalog keeps in-memory state that is a\\nlinear function of the number of stores. (We are aware of this concern and are\\nexploring improvements, but this topic is out of scope for this post.) The issue\\nhere is catalog fragmentation.\\n\\nAs of [v2.3](/releases/vast-v2.3), VAST has automatic rebuilding in place, which\\nmerges underfull partitions to reduce pressure on the catalog. This doesn\u2019t fix\\nthe problem of linear state, but gives us much sufficient reach for real-world\\ndeployments.\\n\\n## Size\\n\\nTo better understand the difference between Parquet and Feather, we now take a\\nlook at them right next to each other. In addition to Feather and Parquet, we\\nuse two other types of \u201cstores\u201d for the analysis to facilitate comparison:\\n\\n1. **Original**: the size of the input prior it entered VAST, e.g., the raw JSON or\\n    a PCAP file.\\n\\n2. **Memory**: the size of the data in memory, measured as the sum of Arrow\\n    buffers that make up the table slice.\\n\\nLet\u2019s kick of the analysis by getting a better understanding at the size\\ndistribution.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  bind_rows(original, memory) |>\\n  ggplot(aes(x = reorder(store, -bytes, FUN = \\"median\\"),\\n             y = bytes, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", color = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/plot-schema-distribution-boxplot-1.svg\';\\n\\n<Svg3 />\\n\\nEvery boxplot corresponds to one store, with `original` and `memory` being also\\ntreated like stores. The suffix `-Z` indicates Zstd level `Z`, with `NA` meaning\\n\u201ccompression turned off\u201d entirely. Parquet stores on the right (in purple) have\\nthe smallest size, followed by Feather (red), and then their corresponding\\nin-memory (green) and original (turquoise) representation. The negative Zstd\\nlevel -5 makes Parquet actually worse than Feather.\\n\\n:::tip Analysis\\nWhat stands out is that disabling compression for Feather inflates the data\\nlarger than the original. This is not the case for Parquet. Why? Because Parquet\\nhas an orthogonal layer of compression using dictionaries. This absorbs\\ninefficiencies in heavy-tailed distributions, which are pretty standard in\\nmachine-generated data.\\n:::\\n\\nThe y-axis of above plot is log-scaled, which makes it hard for relative\\ncomparison. Let\u2019s focus on the medians (the bars in the box) only and bring the\\ny-axis to a linear scale:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nmedians <- unified |>\\n  bind_rows(original, memory) |>\\n  group_by(store, store_class) |>\\n  summarize(bytes = median(bytes))\\n\\nmedians |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n  geom_bar(stat = \\"identity\\") +\\n  scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", fill = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg4 from \'./index_files/figure-gfm/plot-schema-distribution-medians-1.svg\';\\n\\n<Svg4 />\\n\\nTo better understand the compression in numbers, we\u2019ll anchor the original size\\nat 100% and now show the *relative* gains of Parquet and Feather:\\n\\n| Store      | Class   | Bytes/Event | Size (%) | Compression Ratio |\\n|:-----------|:--------|------------:|---------:|------------------:|\\n| parquet+19 | parquet |        53.5 |     22.7 |               4.4 |\\n| parquet+9  | parquet |        54.4 |     23.1 |               4.3 |\\n| parquet+1  | parquet |        55.8 |     23.7 |               4.2 |\\n| feather+19 | feather |        57.8 |     24.6 |               4.1 |\\n| feather+9  | feather |        66.9 |     28.4 |               3.5 |\\n| feather+1  | feather |        68.9 |     29.3 |               3.4 |\\n| parquet+-5 | parquet |        72.9 |     31.0 |               3.2 |\\n| parquet+NA | parquet |        90.8 |     38.6 |               2.6 |\\n| feather+-5 | feather |        95.8 |     40.7 |               2.5 |\\n| feather+NA | feather |       255.1 |    108.3 |               0.9 |\\n\\n:::tip Analysis\\nParquet dominates Feather with respect to space savings, but not by much for\\nhigh Zstd levels. Zstd levels \\\\> 1 do not provide substantial space savings on\\naverage, where observe a compression ratio of **\\\\~4x** over the base data. Parquet\\nstill provides a **2.6** compression ratio in the absence of compression because\\nit applies dictionary encoding.\\n\\nFeather offers competitive compression with **\\\\~3x** ratio for equal Zstd levels.\\nHowever, without compression Feather expands beyond the original dataset size at\\na compression ratio of **\\\\~0.9**.\\n:::\\n\\nThe above analysis covered averages across schemas. If we juxtapose Parquet and\\nFeather per schema, we see the difference between the two formats more clearly:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\nparquet_vs_feather_size <- unified |>\\n  select(-store, -duration) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = bytes,\\n              id_cols = c(schema, zstd.level))\\n\\nplot_parquet_vs_feather <- function(data) {\\n  data |>\\n    mutate(zstd.level = str_replace_na(zstd.level)) |>\\n    separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n    ggplot(aes(x = parquet, y = feather,\\n               shape = zstd.level, color = zstd.level)) +\\n      geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n      geom_point(alpha = 0.6, size = 3) +\\n      geom_text_repel(aes(label = schema),\\n                color = \\"grey\\",\\n                size = 1, # font size\\n                box.padding = 0.2,\\n                min.segment.length = 0, # draw all line segments\\n                max.overlaps = Inf,\\n                segment.size = 0.2,\\n                segment.color = \\"grey\\",\\n                segment.alpha = 0.3) +\\n      scale_size(range = c(0, 10)) +\\n      labs(x = \\"Bytes (Parquet)\\", y = \\"Bytes (Feather)\\",\\n           shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n}\\n\\nparquet_vs_feather_size |>\\n  filter(schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\"))\\n```\\n\\n</details>\\n\\nimport Svg5 from \'./index_files/figure-gfm/plot-parquet-vs-feather-1.svg\';\\n\\n<Svg5 />\\n\\nIn the above log-log scatterplot, the straight line is the identity function.\\nEach point represents the median store size for a given schema. If a point is on\\nthe line, it means there is no difference between Feather and Parquet. We only\\nlook at schemas with more than 100k events to ensure that the constant factor\\ndoes not perturb the analysis. (Otherwise we end up with points *below* the\\nidentity line, which are completely dwarfed by the bulk in practice.) The color\\nand shape shows the different Zstd levels, with `NA` meaning no compression.\\nPoints clouds closer to the origin mean that the corresponding store class takes\\nup less space.\\n\\n:::tip Analysis\\nWe observe that **disabling compression hits Feather the hardest**.\\nUnexpectedly, a negative Zstd level of -5 does not compress well. The remaining\\nZstd levels are difficult to take apart visually, but it appears that the point\\nclouds form a parallel line, indicating stable compression gains. Notably,\\n**compressing PCAP packets is nearly identical with Feather and Parquet**,\\npresumably because of the low entropy and packet meta data where general-purpose\\ncompressors like Zstd shine.\\n:::\\n\\nZooming in to the bottom left area with average event size of less than 100B,\\nand removing the log scaling, we see the following:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    coord_fixed()\\n```\\n\\n</details>\\n\\nimport Svg6 from \'./index_files/figure-gfm/plot-parquet-vs-feather-100-1.svg\';\\n\\n<Svg6 />\\n\\nThe respective point clouds form a parallel to the identity function, i.e., the\\ncompression ratio in this region pretty constant across schemas. There\u2019s also no\\nnoticeable difference between Zstd level 1, 9, and 19.\\n\\nIf we take pick a single point, e.g., `zeek.conn` with\\n4.7M events,\\nwe can confirm that the relative performance matches the results of our analysis\\nabove:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema == \\"zeek.conn\\") |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    labs(x = \\"Store\\", y = \\"Bytes/Event\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n    facet_wrap(~ schema, scales = \\"free\\")\\n```\\n\\n</details>\\n\\nimport Svg7 from \'./index_files/figure-gfm/plot-zeek-suricata-1.svg\';\\n\\n<Svg7 />\\n\\nFinally, we look at the fraction of space Parquet takes compared to Feather on a\\nper schema basis, restricted to schemas with more than 10k events:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tibble)\\n\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt10k) |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  ggplot(aes(x = reorder(schema, -parquet / feather),\\n             y = parquet / feather,\\n             fill = zstd.level)) +\\n    geom_hline(yintercept = 1) +\\n    geom_bar(stat = \\"identity\\", position = \\"dodge\\") +\\n    labs(x = \\"Schema\\", y = \\"Parquet / Feather (%)\\", fill = \\"Zstd Level\\") +\\n    scale_y_continuous(breaks = 6:1 * 20 / 100, labels = scales::label_percent()) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg8 from \'./index_files/figure-gfm/plot-parquet-divided-by-feather-1.svg\';\\n\\n<Svg8 />\\n\\nThe horizontal line is similar to the identity line in the scatterplot,\\nindicating that Feather and Parquet compress equally well. The bars represent\\nthat ratio of Parquet divided by Feather. The shorter the bars, the smaller the\\nsize, so the higher the gain over Feather.\\n\\n:::tip Analysis\\nWe see that Zstd level 19 brings Parquet and Feather close together. Even at\\nZstd level 1, the median ratio of Parquet stores is **78%**, and the 3rd\\nquartile **82%**. This shows that **Feather is remarkably competitive for typical\\nsecurity analytics workloads**.\\n:::\\n\\n## Speed\\n\\nNow that we have looked at the spatial properties of Parquet and Feather, we\\ntake a look at the runtime. With *speed*, we mean the time it takes to transform\\nArrow Record Batches into Parquet and Feather format. This analysis only\\nconsiders only CPU time; VAST writes the respective store in memory first and\\nthen flushes it one sequential write. Our mental model is that Feather is faster\\nthan Parquet. Is that the case when enabling compression for both?\\n\\nTo avoid distortion of small events, we also restrict the analysis to schemas\\nwith more than 100k events.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  ggplot(aes(x = reorder(store, -duration, FUN = \\"median\\"),\\n             y = duration, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n  labs(x = \\"Store\\", y = \\"Speed (us)\\", color = \\"Store\\")\\n```\\n\\n</details>\\n\\nimport Svg9 from \'./index_files/figure-gfm/duration-distribution-1.svg\';\\n\\n<Svg9 />\\n\\nThe above boxplots show the time it takes to write a store for a given store and\\ncompression level combination. The log-scaled y-axis shows the normalized to number\\nof microseconds per event, across the distribution of all schemas. The sort order\\nis the median processing time, similar to the size discussion above.\\n\\n:::tip Analysis\\nAs expected, we roughly observe an ordering according to Zstd level: more\\ncompression means a longer runtime.\\n\\nUnexpectedly, for the same Zstd level, **Parquet store creation was always\\nfaster**. Our unconfirmed hunch is that Feather compression operates on more and\\nsmaller column buffers, whereas Parquet compression only runs over the\\nconcatenated Arrow buffers, yielding bigger strides.\\n:::\\n\\nWe don\u2019t have an explanation for why disabling compression for Parquet is\\n*slower* compared Zstd levels -5 and 1. In theory, strictly less cycles are\\nspent by disabling the compression code path. Perhaps compression results in\\ndifferent memory layout that is more cache-efficient. Unfortunately, we did not\\nhave the time to dig deeper into the analysis to figure out why disabling\\nParquet compression is slower. Please don\u2019t hesitate to reach out, e.g., via our\\n[community chat](/discord).\\n\\nLet\u2019s compare Parquet and Feather by compression level, per schema:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_duration <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  select(-store, -bytes) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = duration,\\n              id_cols = c(schema, zstd.level))\\n\\nparquet_vs_feather_duration |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = parquet, y = feather,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n    geom_point(alpha = 0.7, size = 3) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_size(range = c(0, 10)) +\\n    scale_x_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Speed (Parquet)\\", y = \\"Speed (Feather)\\",\\n         shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg10 from \'./index_files/figure-gfm/pairwise-runtime-comparion-1.svg\';\\n\\n<Svg10 />\\n\\nThe above scatterplot has an identity line. Points on this line indicates that\\nthere is no speed difference between Parquet and Feather. Feather is faster for\\npoints below the line, and Parquet is faster for points above the line.\\n\\n:::tip Analysis\\nIn addition to the above boxplot, this scatterplot makes it clearer to see the\\nimpact of the schemas.\\n\\nInterestingly, **there is no significant difference in Zstd levels -5 and 1,\\nwhile levels 9 and 19 stand apart further**. Disabling compression for Feather\\nhas a stronger effect on speed than for Parquet.\\n\\nOverall, we were surprised that **Feather and Parquet are not far apart in terms\\nof write performance once compression is enabled**. Only when compression is\\ndisabled, Parquet is substantially slower in our measurements.\\n:::\\n\\n## Space-Time Trade-off\\n\\nFinally, we combine the size and speed analysis into a single benchmark. Our\\ngoal is to find an optimal parameterization, i.e., one that strictly dominates\\nothers. To this end, we now plot size against speed:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  group_by(schema, store_class, zstd.level) |>\\n  summarize(bytes = median(bytes), duration = median(duration))\\n\\ncleaned |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3, alpha = 0.7) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg11 from \'./index_files/figure-gfm/points-by-level-1.svg\';\\n\\n<Svg11 />\\n\\nEvery point in the above log-log scatterplot represents a store with a fixed\\nschema. Since we have multiple stores for a given schema, we took the median\\nsize and median speed. We then varied the run matrix by Zstd level (color) and\\nstore type (triangle/point shape). Points closer to the origin are \u201cbetter\u201d in\\nboth dimensions. So we\u2019re looking for the left-most and bottom-most ones.\\nDisabling compression puts points into the bottom-right area, and maximum\\ncompression into the top-left area.\\n\\nThe point closest to the origin has the schema `zeek.dce_rpc` for Zstd level 1,\\nboth for Feather and Parquet. Is there anything special about this log file?\\nHere\u2019s a sample:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   dce_rpc\\n    #open   2022-04-20-09-56-46\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   rtt named_pipe  endpoint    operation\\n    #types  time    string  addr    port    addr    port    interval    string  string  string\\n    1637222709.134638   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000254    135 epmapper    ept_map\\n    1637222709.140898   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000239    49671   drsuapi DRSBind\\n    1637222709.141520   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000311    49671   drsuapi DRSCrackNames\\n    1637222709.142068   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000137    49671   drsuapi DRSUnbind\\n    1637222709.143104   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000228    135 epmapper    ept_map\\n    1637222709.143642   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000147    49671   drsuapi DRSBind\\n    1637222709.144040   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000296    49671   drsuapi DRSCrackNames\\n\\nIt appears to be rather normal: 10 columns, several different data types, unique\\nIDs, and some short strings. By looking at the data alone, there is no obvious\\nhint that explains the performance.\\n\\nWith dozens to hundreds of different schemas per data source (sometimes even\\nmore), there it will be difficult to single out individual schemas. But a point\\ncloud is unwieldy for relative comparison. To better represent the variance of\\nschemas for a given configuration, we can strip the \u201cinner\u201d points and only look\\nat their convex hull:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\n# Native convex hull does the job, no need to leverage ggforce.\\nconvex_hull <- cleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration))\\n\\nconvex_hull |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(fill = zstd.level, color = zstd.level),\\n                 alpha = 0.1,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg12 from \'./index_files/figure-gfm/convex-hull-1.svg\';\\n\\n<Svg12 />\\n\\nIntuitively, the area of a given polygon captures its variance. A smaller area\\nis \u201cgood\u201d in that it offers more predictable behavior. The high amount of\\noverlap makes it still difficult to perform clearer comparisons. If we facet by\\nstore type, it becomes easier to compare the areas:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  # Native convex hull does the job, no need to leverage ggforce.\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = store_class)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = store_class, fill = store_class),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(n.breaks = 4, labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Store\\") +\\n    facet_grid(cols = vars(zstd.level)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg13 from \'./index_files/figure-gfm/convex-hull-facet-by-level-1.svg\';\\n\\n<Svg13 />\\n\\nArranging the facets above row-wise makes it easier to compare the y-axis, i.e.,\\nspeed, where lower polygons are better. Arranging them column-wise makes it easier\\nto compare the x-axis, i.e., size, where the left-most polygons are better:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = zstd.level, fill = zstd.level),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Zstd Level\\", color = \\"Zstd Level\\") +\\n    facet_grid(rows = vars(store_class)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg14 from \'./index_files/figure-gfm/convex-hull-facet-by-store-1.svg\';\\n\\n<Svg14 />\\n\\n:::tip Analysis\\nAcross both dimensions, **Zstd level 1 shows the best average space-time\\ntrade-off for both Parquet and Feather**. In the above plots, we also observe our\\nfindings from the speed analysis: Parquet still dominates when compression is\\nenabled.\\n:::\\n\\n## Conclusion\\n\\nIn summary, we set out to better understand how Parquet and Feather behave on\\nthe write path of VAST, when acquiring security telemetry from high-volume data\\nsources. Our findings show that columnar Zstd compression offers great space\\nsavings for both Parquet and Feather. For certain schemas, Feather and Parquet\\nexhibit only a marginal differences. To our surprise, writing Parquet files is\\nstill faster than Feather for our workloads.\\n\\nThe pressing next question is obviously: what about the read path, i.e., query\\nlatency? This is a topic for future, stay tuned."},{"id":"/parquet-and-feather-enabling-open-investigations","metadata":{"permalink":"/archive/parquet-and-feather-enabling-open-investigations","source":"@site/archive/parquet-and-feather-enabling-open-investigations/index.md","title":"Parquet & Feather: Enabling Open Investigations","description":"Apache Parquet is the common denominator for structured data at rest.","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"arrow","permalink":"/archive/tags/arrow"},{"label":"parquet","permalink":"/archive/tags/parquet"},{"label":"feather","permalink":"/archive/tags/feather"}],"readingTime":5.16,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"},{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Enabling Open Investigations","authors":["mavam","dispanser"],"date":"2022-10-07T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/archive/parquet-and-feather-writing-security-telemetry"},"nextItem":{"title":"A Git Retrospective","permalink":"/archive/a-git-retrospective"}},"content":"[Apache Parquet][parquet] is the common denominator for structured data at rest.\\nThe data science ecosystem has long appreciated this. But infosec? Why should\\nyou care about Parquet when building a threat detection and investigation\\nplatform? In this blog post series we share our opinionated view on this\\nquestion. In the next three blog posts, we\\n\\n1. describe how VAST uses Parquet and its little brother [Feather][feather]\\n2. benchmark the two formats against each other for typical workloads\\n3. share our experience with all the engineering gotchas we encountered along\\n   the way\\n\\n[parquet]: https://parquet.apache.org/\\n[feather]: https://arrow.apache.org/docs/python/feather.html\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 1/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1. This blog post\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-2]: /archive/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /archive/parquet-and-feather-data-engineering-woes/\\n:::\\n\\n## Why Parquet and Feather?\\n\\nParquet is the de-facto standard for storing structured data in a format\\nconducive for analytics. Nearly all analytics engines support reading Parquet\\nfiles to load a dataset in memory for subsequent analysis.\\n\\nThe data science community has long built on this foundation, but the majority\\nof infosec tooling does not build on an open foundation. Too many\\nproducts hide their data behind silos, either wrapped behind a SaaS with a thin\\nAPI, or in a custom format that requires cumbersome ETL pipelines. Nearly all\\nadvanced use cases require full access to the data. Especially when\\nthe goal is developing realtime threat detection and response systems.\\n\\nSecurity is a data problem. But how should we represent that data? This is where\\nParquet enters the picture. As a vendor-agnostic storage format for structured\\nand nested data, it decouples storage from analytics. This is where SIEM\\nmonoliths fail: they offer a single black box that tightly couples data\\nacquisition and processing capabilities. Providing a thin \\"open\\" API is not really\\nopen, as it prevents high-bandwidth data access that is needed for advanced\\nanalytics workloads.\\n\\nOpen storage prevents vendor-lock-in. When any tool can work with the data, you\\nbuild a sustainable foundation for implementing future use cases. For example,\\nwith Parquet\'s column encryption, you can offload fine-grained compliance use\\ncases to a dedicated application. Want to try out a new analytics engine? Just\\npoint it to the Parquet files.\\n\\n## Parquet\'s Little Brother\\n\\n[Feather][feather] is Parquet\'s little brother. It emerged while building a\\nproof of concept for \\"fast, language-agnostic data frame storage for Python\\n(pandas) and R.\\" The format is a thin layer on top of [Arrow\\nIPC](https://arrow.apache.org/docs/python/ipc.html#ipc), making it conducive for\\nmemory mapping and zero-copy usage. On the spectrum of speed and\\nspace-efficiency, think of it this way:\\n\\n![Parquet vs. Feather](parquet-vs-feather.excalidraw.svg)\\n\\nBefore Feather existed, VAST had its own storage format that was 95% like\\nFeather, minus a thin framing. (We called it the *segment store*.)\\n\\nWait, but Feather is an in-memory format and Parquet an on-disk format. You\\ncannot compare them! Fair point, but don\'t forget transparent Zstd compression.\\nFor some schemas, we barely notice a difference (e.g., PCAP), whereas for\\nothers, Parquet stores boil down to a fraction of their Feather equivalent.\\n\\nThe [next blog post][parquet-and-feather-2] goes into these details. For now, we\\nwant to stress that Feather is in fact a reasonable format for data at rest,\\neven when looking at space utilization alone.\\n\\n## Parquet and Feather in VAST\\n\\nVAST can store event data as Parquet or Feather. The unit of storage scaling is\\na *partition*. In Arrow terms, a partition is a persisted form of an [Arrow\\nTable][arrow-table], i.e., a concatenation of [Record\\nBatches][arrow-record-batch]. A partition has thus a fixed schema. VAST\'s store\\nplugin determines how a partition writes its buffered record\\nbatches to disk. The diagram below illustrates the architecture:\\n\\n![Parquet Analytics](parquet-analytics.excalidraw.svg)\\n\\n[arrow-table]: https://arrow.apache.org/docs/python/data.html#tables\\n[arrow-record-batch]: https://arrow.apache.org/docs/python/data.html#record-batches\\n\\nThis architecture makes it easy to point an analytics application directly to\\nthe store files, without the need for ETLing it into a dedicated warehouse, such\\nas Spark or Hadoop.\\n\\nThe event data thrown at VAST has quite some variety of schemas. During\\ningestion, VAST first demultiplexes the heterogeneous stream of events into\\nmultiple homogeneous streams, each of which has a unique schema. VAST buffers\\nevents until the partition hits a pre-configured event limit (e.g., 1M) or until\\na timeout occurs (e.g., 60m). Thereafter, VAST writes the partition in one shot\\nand persists it.\\n\\nThe buffering provides optimal freshness of the data, as it enables queries run\\non not-yet-persisted data. But it also sets an upper bound on the partition\\nsize, given that it must fit in memory in its entirety. In the future, we plan\\nto make this freshness trade-off explicit, making it possible to write out\\nlarger-than-memory stores incrementally.\\n\\n## Imbuing Domain Semantics\\n\\nIn a [past blog][blog-arrow] we described how VAST uses Arrow\'s extensible\\ntype system to add richer semantics to the data. This is how the value of VAST\\ntranscends through the analytics stack. For example, VAST has native IP address\\ntypes that you can show up in Python as [ipaddress][ipaddress] instance. This\\navoids friction in the data exchange process. Nobody wants to spend time\\nconverting bytes or strings into the semantic objects that are ultimately need\\nfor the analysis.\\n\\n[blog-arrow]: /archive/apache-arrow-as-platform-for-security-data-engineering\\n[ipaddress]: https://docs.python.org/3/library/ipaddress.html\\n\\nHere\'s how VAST\'s type system looks like:\\n\\n![Type System - VAST](type-system-vast.excalidraw.svg)\\n\\nThere exist two major classes of types: *basic*, stateless types with a static\\nstructure and a-priori known representation, and *complex*, stateful types that\\ncarry additional runtime information. We map this type system without\\ninformation loss to Arrow:\\n\\n![Type System - Arrow](type-system-arrow.excalidraw.svg)\\n\\nVAST converts enum, address, and subnet types to\\n[extension-types][arrow-extension-types]. All types are self-describing and part\\nof the record batch meta data. Conversion is bi-directional. Both Parquet and\\nFeather support fully nested structures in this type system. In theory. Our\\nthird blog post in this series describes the hurdles we had to overcome to make\\nit work in practice.\\n\\n[arrow-extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n\\nIn the next blog post, we perform a quantitative analysis of the two formats: how\\nwell do they compress the original data? How much space do they take up in\\nmemory? How much CPU time do I pay for how much space savings? In the meantime,\\nif you want to learn more about Parquet, take a look at the [blog post\\nseries][arrow-parquet-blog] from the Arrow team.\\n\\n[arrow-parquet-blog]: https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"},{"id":"/a-git-retrospective","metadata":{"permalink":"/archive/a-git-retrospective","source":"@site/archive/a-git-retrospective/index.md","title":"A Git Retrospective","description":"The VAST project is roughly a decade old. But what happened over the last 10","date":"2022-09-15T00:00:00.000Z","formattedDate":"September 15, 2022","tags":[{"label":"git","permalink":"/archive/tags/git"},{"label":"r","permalink":"/archive/tags/r"},{"label":"quarto","permalink":"/archive/tags/quarto"},{"label":"notebooks","permalink":"/archive/tags/notebooks"},{"label":"engineering","permalink":"/archive/tags/engineering"},{"label":"open-source","permalink":"/archive/tags/open-source"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A Git Retrospective","authors":"mavam","date":"2022-09-15T00:00:00.000Z","tags":["git","r","quarto","notebooks","engineering","open-source"]},"prevItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/archive/parquet-and-feather-enabling-open-investigations"},"nextItem":{"title":"Richer Typing in Sigma","permalink":"/archive/richer-typing-in-sigma"}},"content":"The VAST project is roughly a decade old. But what happened over the last 10\\nyears? This blog post looks back over time through the lens of the git *merge*\\ncommits.\\n\\nWhy merge commits? Because they represent a unit of completed contribution.\\nFeature work takes place in dedicated branches, with the merge to the main\\nbranch sealing the deal. Some feature branches have just one commit, whereas\\nothers dozens. The distribution is not uniform. As of `6f9c84198` on Sep 2,\\n2022, there are a total of 13,066 commits, with 2,334 being merges (17.9%).\\nWe\u2019ll take a deeper look at the merge commits.\\n\\n\x3c!--truncate--\x3e\\n\\n``` bash\\ncd /tmp\\ngit clone https://github.com/tenzir/vast.git\\ncd vast\\ngit checkout 6f9c841980b2333028b1ac19e2a21e99d96cbd36\\ngit log --merges --pretty=format:\\"%ad|%d\\" --date=iso-strict |\\n  sed -E \'s/(.+)\\\\|.*tag: ([^,)]+).*/\\\\1 \\\\2/\' |\\n  sed -E \'s/(.*)\\\\|.*/\\\\1 NA/\' \\\\\\n  > /tmp/vast-merge-commits.txt\\n```\\n\\nFor the statistics, we\u2019ll switch to R. In all subsequent figures, a single point\\ncorresponds to a merge commit. The reduced opacity alleviates the effects of\\noverplotting.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(readr)\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read_table(\\"/tmp/vast-merge-commits.txt\\",\\n  col_names = c(\\"time\\", \\"tag\\"),\\n  col_types = \\"Tc\\"\\n) |>\\n  arrange(time) |>\\n  mutate(count = row_number())\\n\\nfirst_contribution <- \\\\(x) data |>\\n  filter(time >= x) |>\\n  pull(count) |>\\n  first()\\n\\nevents <- tribble(\\n  ~time, ~event,\\n  ymd(\\"2016-03-17\\"), \\"NSDI \'16\\\\npublication\\",\\n  ymd(\\"2017-08-31\\"), \\"Tenzir\\\\nincorporated\\",\\n  ymd(\\"2018-07-01\\"), \\"Tobias\\",\\n  ymd(\\"2019-09-15\\"), \\"Dominik\\",\\n  ymd(\\"2020-01-01\\"), \\"Benno\\",\\n  ymd(\\"2021-12-01\\"), \\"Thomas\\",\\n  ymd(\\"2022-07-01\\"), \\"Patryk\\",\\n) |>\\n  mutate(time = as.POSIXct(time), count = Vectorize(first_contribution)(time))\\n\\ndata |>\\n  ggplot(aes(x = time, y = count)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = events,\\n    aes(xend = time, yend = count + 200),\\n    color = \\"red\\"\\n  ) +\\n  geom_label(\\n    data = events,\\n    aes(y = count + 200, label = event),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  scale_x_datetime(date_breaks = \\"1 year\\", date_labels = \\"%Y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/full-time-spectrum-1.svg\';\\n\\n<Svg1 />\\n\\nPrior to Tenzir taking ownership of the project and developing VAST, it was a\\ndissertation project evolving along during PhD work at the University of\\nCalifornia, Berkeley. We can see that the first pre-submission crunch started a\\nfew months before the [NSDI \u201916\\npaper](https://matthias.vallentin.net/papers/nsdi16.pdf).\\n\\nTenzir was born in fall 2017. Real-world contributions arrived as of 2018 when\\nthe small team set sails. Throughput increased as core contributors joined the\\nteam. Fast-forward to 2020 when we started doing public releases. The figure\\nbelow shows how this process matured.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\ndata |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.1) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"red\\",\\n    segment.alpha = 0.2,\\n    box.padding = 0.2\\n  ) +\\n  scale_x_datetime(\\n    date_breaks = \\"1 year\\",\\n    limits = c(as.POSIXct(ymd(\\"2020-01-01\\")), max(data$time)),\\n    date_labels = \\"%Y\\"\\n  ) +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/since-2020-1.svg\';\\n\\n<Svg2 />\\n\\nAs visible from the tag labels, we were at [CalVer](https://calver.org) for a\\nwhile, but ultimately switched to [SemVer](https://semver.org). Because we had\\nalready commercial users at the time, this helped us better communicate breaking\\nvs.\xa0non-breaking changes.\\n\\nLet\u2019s zoom in on all releases since v1.0. At this time, we had a solid\\nengineering and release process in place.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tidyr)\\nv1_0_0_rc1_time <- data |>\\n  filter(tag == \\"v1.0.0-rc1\\") |>\\n  pull(time)\\n\\nsince_v1_0_0_rc1 <- data |> filter(time >= v1_0_0_rc1_time)\\n\\nrc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(grepl(\\"rc\\", tag))\\n\\nnon_rc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(!grepl(\\"rc\\", tag))\\n\\nsince_v1_0_0_rc1 |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = non_rc,\\n    aes(xend = time, yend = min(count)), color = \\"red\\"\\n  ) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"grey\\",\\n    box.padding = 0.7\\n  ) +\\n  geom_point(\\n    data = rc, aes(x = time, y = count),\\n    color = \\"blue\\",\\n    size = 2\\n  ) +\\n  geom_point(\\n    data = non_rc, aes(x = time, y = count),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  geom_label(data = non_rc, aes(y = min(count)), size = 2, color = \\"red\\") +\\n  scale_x_datetime(date_breaks = \\"1 month\\", date_labels = \\"%b %y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/since-v1.0-1.svg\';\\n\\n<Svg3 />\\n\\nThe v2.0 release was a hard one for us, given the long distance to v1.1. We\\nmerged too much and testing took forever. Burnt by the time sunk in testing and\\nfixups, we decided to switch to an LPU model (\u201cleast publishable unit\u201d) to\\nreduce release cadence. We didn\u2019t manage to implement this model until after\\nv2.1 though, where the release cadence finally gets smaller. A monthly release\\nfeels about the right for our team size.\\n\\nThe key challenge is minimizing the feature freeze phase. The first release\\ncandidate (RC) kicks this phase off, and the final release lifts the\\nrestriction. In this period, features are not allowed to be merged.[^1] This is\\na delicate time window: too long and the fixups in the RC phase cause the\\npostponed pull requests to diverge, too short and we compromise on testing\\nrigor, causing a release that doesn\u2019t meet our Q&A requirements.\\n\\nThis is where we stand as of today. We\u2019re happy how far along we came, but\\nmany challenges still lay ahead of us. Increased automation and deeper testing\\nis the overarching theme, e.g., code coverage, fuzzing, GitOps. We\u2019re constantly\\nstriving to improve or processes. With a small team of passionate, senior\\nengineers, this is a lot of fun!\\n\\n[^1]: We enforced this with a `blocked` label. CI [doesn\u2019t allow\\n    merging](https://github.com/tenzir/vast/blob/6f9c841980b2333028b1ac19e2a21e99d96cbd36/.github/workflows/blocked.yaml) when this label is on a pull request."},{"id":"/richer-typing-in-sigma","metadata":{"permalink":"/archive/richer-typing-in-sigma","source":"@site/archive/richer-typing-in-sigma/index.md","title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","date":"2022-08-12T00:00:00.000Z","formattedDate":"August 12, 2022","tags":[{"label":"sigma","permalink":"/archive/tags/sigma"},{"label":"regex","permalink":"/archive/tags/regex"},{"label":"query-frontend","permalink":"/archive/tags/query-frontend"}],"readingTime":4.685,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","authors":"mavam","date":"2022-08-12T00:00:00.000Z","last_updated":"2023-02-12T00:00:00.000Z","tags":["sigma","regex","query-frontend"]},"prevItem":{"title":"A Git Retrospective","permalink":"/archive/a-git-retrospective"},"nextItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/archive/apache-arrow-as-platform-for-security-data-engineering"}},"content":"VAST\'s Sigma frontend now supports more modifiers. In the Sigma language,\\nmodifiers transform predicates in various ways, e.g., to apply a function over a\\nvalue or to change the operator of a predicate. Modifiers are the customization\\npoint to enhance expressiveness of query operations.\\n\\nThe new [pySigma][pysigma] effort, which will eventually replace the\\nnow-considered-legacy [sigma][sigma] project, comes with new modifiers as well.\\nMost notably, `lt`, `lte`, `gt`, `gte` provide comparisons over value domains\\nwith a total ordering, e.g., numbers: `x >= 42`. In addition, the `cidr`\\nmodifier interprets a value as subnet, e.g., `10.0.0.0/8`. Richer typing!\\n\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[pysigma]: https://github.com/SigmaHQ/pySigma\\n\\n\x3c!--truncate--\x3e\\n\\nHow does the frontend work? Think of it as a parser that processes the YAML and\\ntranslates it into an expression tree, where the leaves are predicates with\\ntyped operands according to VAST\'s data model. Here\'s how it works:\\n\\n![Sigma Query Frontend](sigma-query-frontend.excalidraw.svg)\\n\\nLet\'s take a closer look at some Sigma rule modifiers:\\n\\n```yaml\\nselection:\\n  x|re: \'f(o+|u)\'\\n  x|lt: 42\\n  x|cidr: 192.168.0.0/23\\n  x|base64offset|contains: \'http://\'\\n```\\n\\nThe `|` symbol applies a modifier to a field. Let\'s walk through the above\\nexample:\\n\\n1. The `re` modifier changes the predicate operand from `x == \\"f(o+|u)\\"` to\\n   `x == /f(o+|u)/`, i.e., the type of the right-hand side changes from `string`\\n   to `pattern`.\\n\\n2. The `lt` modifier changes the predicate operator from `==` to `<`, i.e.,\\n   `x == 42` becomes `x < 42`.\\n\\n3. The `cidr` modifier changes the predicate operand to type subnet. In VAST,\\n   parsing the operand type into a subnet happens automatically, so the Sigma\\n   frontend only changes the operator to `in`. That is, `x == \\"192.168.0.0/23\\"`\\n   becomes `x in 192.168.0.0/23`. Since VAST supports top-k prefix search on\\n   subnets natively, nothing else needs to be changed.\\n\\n   Other backends expand this to:\\n\\n   ```c\\n   x == \\"192.168.0.*\\" || x == \\"192.168.1.*\\"\\n   ```\\n\\n   This expansion logic on strings doesn\'t scale very well: for a `/22`, you\\n   would have to double the number of predicates, and for a `/21` quadruple\\n   them. This is where rich and deep typing in the language pays off.\\n\\n4. `x`: there are two modifiers that operate in a chained fashion,\\n   transforming the predicate in two steps:\\n\\n   1. Initial: `x == \\"http://\\"`\\n   2. `base64offset`: `x == \\"aHR0cDovL\\" || x == \\"h0dHA6Ly\\" || x == \\"odHRwOi8v\\"`\\n   3. `contains`: `x in \\"aHR0cDovL\\" || x in \\"h0dHA6Ly\\" || x in \\"odHRwOi8v\\"`\\n\\n   First, `base64offset` always expands a value into a disjunction of 3\\n   predicates, each of which performs an equality comparison to a\\n   Base64-transformed value.[^1]\\n\\n   Thereafter, the `contains` modifier translates the respective predicate\\n   operator from `==` to `in`. Other Sigma backends that don\'t support substring\\n   search natively transform the value instead by wrapping it into `*`\\n   wildcards, e.g., translate `\\"foo\\"` into `\\"*foo*\\"`.\\n\\n[^1]: What happens under the hood is a padding a string with spaces. [Anton\\nKutepov\'s article][sigma-article] illustrates how this works.\\n\\n[sigma-article]: https://tech-en.netlify.app/articles/en513032/index.html\\n\\nOur ultimate goal is to support a fully function executional platform for Sigma\\nrules. The table below shows the current implementation status of modifiers,\\nwhere \u2705 means implemented, \ud83d\udea7 not yet implemented but possible, and \u274c not yet\\nsupported by VAST\'s execution engine:\\n\\n|Modifier|Use|sigmac|VAST|\\n|--------|---|:----:|:--:|\\n|`contains`|perform a substring search with the value|\u2705|\u2705|\\n|`startswith`|match the value as a prefix|\u2705|\u2705|\\n|`endswith`|match the value as a suffix|\u2705|\u2705|\\n|`base64`|encode the value with Base64|\u2705|\u2705\\n|`base64offset`|encode value as all three possible Base64 variants|\u2705|\u2705\\n|`utf16le`/`wide`|transform the value to UTF16 little endian|\u2705|\ud83d\udea7\\n|`utf16be`|transform the value to UTF16 big endian|\u2705|\ud83d\udea7\\n|`utf16`|transform the value to UTF16|\u2705|\ud83d\udea7\\n|`re`|interpret the value as regular expression|\u2705|\ud83d\udea7\\n|`cidr`|interpret the value as a IP CIDR|\u274c|\u2705\\n|`all`|changes the expression logic from OR to AND|\u2705|\u2705\\n|`lt`|compare less than (`<`) the value|\u274c|\u2705\\n|`lte`|compare less than or equal to (`<=`) the value|\u274c|\u2705\\n|`gt`|compare greater than (`>`) the value|\u274c|\u2705\\n|`gte`|compare greater than or equal to (`>=`) the value|\u274c|\u2705\\n|`expand`|expand value to placeholder strings, e.g., `%something%`|\u274c|\u274c\\n\\nAside from completing the implementation of the missing modifiers, there are\\nthree missing pieces for Sigma rule execution to become viable in VAST:\\n\\n1. **Regular expressions**: VAST currently has no efficient mechanism to execute\\n   regular expressions. A regex lookup requires a full scan of the data.\\n   Moreover, the regular expression execution speed is abysimal. But we are\\n   aware of it and are working on this soon. The good thing is that the\\n   complexity of regular expression execution over batches of data is\\n   manageable, given that we would call into the corresponding [Arrow Compute\\n   function][arrow-containment-tests] for the heavy lifting. The number one\\n   challenge will be reduing the data to scan, because the Bloom-filter-like\\n   sketch data structures in the catalog cannot handle pattern types. If the\\n   sketches cannot identify a candidate set, all data needs to be scanned,\\n\\n   To alleviate the effects of full scans, it\'s possible to winnow down the\\n   candidate set of partitions by executing rules periodically. When making the\\n   windows asymptotically small, this yields effectively streaming execution,\\n   which VAST already supports in the form of \\"live queries\\".\\n\\n2. **Case-insensitive strings**: All strings in Sigma rules are case-insensitive\\n   by default, but VAST\'s string search is case-sensitive. As a workaround, we\\n   could translate Sigma strings into regular expressions, e.g., `\\"Foo\\"` into\\n   `/Foo/i`. Unfortunately there is a big performance gap between string\\n   equality search and regular expression search. We will need to find a better\\n   solution for production-grade rule execution.\\n\\n3. **Field mappings**: while Sigma rules execute already syntactically, VAST\\n   currently doesn\'t touch the field names in the rules and interprets them as\\n   field extractors. In other words, VAST doesn\'t support\\n   the Sigma taxonomy yet. Until we provide the mappings, you can already write\\n   generic Sigma rules using concepts.\\n\\n[arrow-containment-tests]: https://arrow.apache.org/docs/cpp/compute.html#containment-tests\\n\\nPlease don\'t hesitate to swing by our [community chat](/discord)\\nand talk with us if you are passionate about Sigma and other topics around open\\ndetection and response."},{"id":"/apache-arrow-as-platform-for-security-data-engineering","metadata":{"permalink":"/archive/apache-arrow-as-platform-for-security-data-engineering","source":"@site/archive/apache-arrow-as-platform-for-security-data-engineering/index.md","title":"Apache Arrow as Platform for Security Data Engineering","description":"How VAST leverages Apache Arrow for Security Data Engineering","date":"2022-06-17T00:00:00.000Z","formattedDate":"June 17, 2022","tags":[{"label":"architecture","permalink":"/archive/tags/architecture"},{"label":"arrow","permalink":"/archive/tags/arrow"},{"label":"performance","permalink":"/archive/tags/performance"},{"label":"query","permalink":"/archive/tags/query"}],"readingTime":5.97,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"description":"How VAST leverages Apache Arrow for Security Data Engineering","authors":"mavam","date":"2022-06-17T00:00:00.000Z","tags":["architecture","arrow","performance","query"]},"prevItem":{"title":"Richer Typing in Sigma","permalink":"/archive/richer-typing-in-sigma"}},"content":"VAST bets on [Apache Arrow][arrow] as the open interface to structured data. By\\n\\"bet,\\" we mean that VAST does not work without Arrow. And we are not alone.\\nInflux\'s [IOx][iox], DataDog\'s [Husky][husky], Anyscale\'s [Ray][ray],\\n[TensorBase][tensorbase], and [others][arrow-projects] committed themselves to\\nmaking Arrow a corner stone of their system architecture. For us, Arrow was not\\nalways a required dependency. We shifted to a tighter integration over the years\\nas the Arrow ecosystem matured. In this blog post we explain our journey of\\nbecoming an Arrow-native engine.\\n\\n[arrow]: https://arrow.apache.org\\n[iox]: https://github.com/influxdata/influxdb_iox\\n[husky]: https://www.datadoghq.com/blog/engineering/introducing-husky/\\n[ray]: https://github.com/ray-project/ray\\n[tensorbase]: https://github.com/tensorbase/tensorbase\\n[arrow-projects]: https://arrow.apache.org/powered_by/\\n\\n\x3c!--truncate--\x3e\\n\\nToday, the need to bring advanced security analytics and data engineering\\ntogether is stronger than ever, but there is a huge gap between the two fields.\\nWe see Arrow as the vehicle to close this gap, allowing us developers to\\npractice *security data engineering* to make security analytics easy for users.\\nThat is, the experience should allow experts to interact with the data in the\\nsecurity domain, end-to-end without context switching. To achieve this, we began\\nour journey with VAST by developing a data model for structured security\\ntelemetry. Having worked for a decade with the [Zeek][zeek] (fka. Bro) network\\nsecurity monitor, we understood the value of having first-class support for\\ndomain-specific entities (e.g., native representation of IPv4 and IPv6\\naddresses) and type-specific operations (e.g., the ability to perform top-k\\nprefix search to answer subnet membership queries). In addition, the ability to\\nembed domain semantics with user-defined types (e.g., IP addresses, subnets, and\\nURLs) was central to expressing complex relationships to develop effective\\nanalytical models. It was clear that we needed the domain model deep in the core\\nof the system to successfully support security analytics.\\n\\nAfter having identified the data model requirements, the question of\\nrepresentation came next. At first, we unified the internal representation with\\na row-oriented representation using [MsgPack][msgpack], which comes with a\\nmechanism for adding custom types. The assumption was that a row-based data\\nrepresentation more closely matches typical event data (e.g., JSONL) and\\ntherefore allows for much higher processing rates. Moreover, early use cases of\\nVAST were limited to interactive, multi-dimensional search to extract a subset\\nof *entire* records, spread over a longitudinal archive of data. The\\nrow-oriented encoding worked well for this.\\n\\nBut as security operations were maturing, requirements extended to analytical\\nprocessing of structured data, making a columnar format increasingly beneficial.\\nAfter having witnessed first-hand the early commitment of [Ray][ray] to Arrow,\\nwe started using Arrow as optional dependency as additional column-oriented\\nencoding. We abstracted a batch of data encoding-independent behind a \\"table\\nslice\\":\\n\\n![MsgPack & Arrow](msgpack-arrow.excalidraw.svg)\\n\\nHiding the concrete encoding behind a cell-based access interface worked for\\nlow-volume use cases, but backfired as we scaled up and slowed us down\\nsubstantially in development. We needed to make a choice. This is where timing\\nwas right: our perception of the rapidly evolving Arrow ecosystem changed.\\nArrow-based runtimes were mushrooming all over the place. Nowadays it requires\\nonly a few lines of code to integrate Arrow data into the central logic of\\napplications. We realized that the primary value proposition of Arrow is to\\n*make data interoperability easy*.\\n\\nBut data interoperability is only a sufficient condition for enabling\\nsustainable security analytics. The differentiating value of a *security* data\\nplatform is support for the *security* domain. This is where Arrow\'s extension\\ntypes come into play. They add *semantics* to otherwise\\ngeneric types, e.g., by telling the user \\"this is a transport-layer port\\" and\\nnot just a 16-bit unsigned integer, or \\"this is a connection 4-tuple to\\nrepresent a network flow\\" instead of \\"this is a record with 4 fields of type\\nstring and unsigned integer\\". Extension types are composable and allow for\\ncreating a rich typing layer with meaningful domain objects on top of a\\nstandardized data representation. Since they are embedded in the data, they do\\nnot have to be made available out-of-band when crossing the boundaries of\\ndifferent tools. Now we have self-describing security data.\\n\\nInteroperability plus support for a domain-specific data model makes Arrow a\\nsolid *data plane*. It turns out that Arrow is much more than a standardized\\ndata representation. Arrow also comes with bag of tools for working with the\\nstandardized data. In the diagram below, we show the various Arrow pieces that\\npower the architecture of VAST:\\n\\n![Arrow Data Plane](arrow-data-plane.excalidraw.svg)\\n\\nIn the center we have the Arrow data plane that powers other parts of the\\nsystem. Green elements highlight Arrow building blocks that we use today, and\\norange pieces elements we plan to use in the future. There are several aspects\\nworth pointing out:\\n\\n1. **Unified Data Plane**: When users ingest data into VAST, the\\n   parsing process converts the native data into Arrow. Similarly, a\\n   conversation boundary exists when data leaves the system, e.g., when a user\\n   wants a query result shown in JSON, CSV, or some custom format. Source and\\n   sink data formats are exchangeable plugins.\\n\\n2. **Read/Write Path Separation**: one design goal of VAST is a strict\\n   separation of read and write path, in order to scale them independently. The\\n   write path follows a horizontally scalable architecture where builders (one per\\n   schema) turn the in-memory record batches into a persistent representation.\\n   VAST currently has support for Parquet and Feather.\\n\\n3. **Pluggable Query Engine**: VAST has live/continuous queries that simply run\\n   over the stream of incoming data, and historical queries that operate on\\n   persistent data. The harboring execution engine is something we are about to\\n   make pluggable. The reason is that VAST runs in extremely different\\n   environments, from cluster to edge. Query engines are usually optimized for a\\n   specific use case, so why not use the best engine for the job at hand? Arrow\\n   makes this possible. [DuckDB][duckdb] and [DataFusion][datafusion] are great\\n   example of embeddable query engines.\\n\\n4. **Unified Control Plane**: to realize a pluggable query engine, we also need\\n   a standardized control plane. This is where [Substrait][substrait] and\\n   Flight come into play. Flight for communication and Substrait as\\n   canonical query representation. We already experimented with Substrait,\\n   converting VAST queries into a logical query plan. In fact, VAST has a \\"query\\n   language\\" plugin to make it possible to translate security content. (For\\n   example, our Sigma plugin translates [Sigma rules][sigma]\\n   into VAST queries.) In short: Substrait is to the control plane what Arrow is\\n   to the data plane. Both are needed to modularize the concept of a query\\n   engine.\\n\\nMaking our own query engine more suitable for analytical workloads has\\nreceived less attention in the past, as we prioritized high-performance data\\nacquisition, low-latency search, in-stream matching using Compute,\\nand expressiveness of the underlying domain data model. We did so because VAST\\nmust run robustly in production on numerous appliances all over the world in a\\nsecurity service provider setting, with confined processing and storage where\\nefficiency is key.\\n\\nMoving forward, we are excited to bring more analytical horse power to the\\nsystem, while opening up the arena for third-party engines. With the bag of\\ntools from the Arrow ecosystem, plus all other embeddable Arrow engines that are\\nemerging, we have a modular architecture to can cover a very wide spectrum of\\nuse cases.\\n\\n[substrait]: https://substrait.io/\\n[datafusion]: https://arrow.apache.org/datafusion/\\n[msgpack]: https://msgpack.org/index.html\\n[duckdb]: https://duckdb.org/\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[zeek]: https://zeek.org"}]}')}}]);