"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/parquet-and-feather-enabling-open-investigations","metadata":{"permalink":"/blog/parquet-and-feather-enabling-open-investigations","source":"@site/blog/parquet-and-feather-enabling-open-investigations/index.md","title":"Parquet & Feather: Enabling Open Investigations","description":"Apache Parquet is the common denominator for structured data at rest.","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":5.02,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"},{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Enabling Open Investigations","authors":["mavam","dispanser"],"date":"2022-10-07T00:00:00.000Z","tags":["arrow","parquet","feather"]},"nextItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"}},"content":"[Apache Parquet][parquet] is the common denominator for structured data at rest.\\nThe data science ecosystem has long appreciated this. But infosec? Why should\\nyou care about Parquet when building a threat detection and investigation\\nplatform? In this blog post series we share our opinionated view on this\\nquestion. In the next three blog posts, we\\n\\n1. describe how VAST uses Parquet and its little brother [Feather][feather]\\n2. benchmark the two formats against each other for typical workloads\\n3. share our experience with all the engineering gotchas we encountered along\\n   the way\\n\\n[parquet]: https://parquet.apache.org/\\n[feather]: https://arrow.apache.org/docs/python/feather.html\\n\\n\x3c!--truncate--\x3e\\n\\n## Why Parquet and Feather?\\n\\nParquet is the de-facto standard for storing structured data in a format\\nconducive for analytics. Nearly all analytics engines support reading Parquet\\nfiles to load a dataset in memory for subsequent analysis.\\n\\nThe data science community has long built on this foundation, but the majority\\nof infosec tooling [does not build on an open\\nfoundation](/docs/about/vision#the-soc-architecture-maze). Too many\\nproducts hide their data behind silos, either wrapped behind a SaaS with a thin\\nAPI, or in a custom format that requires cumbersome ETL pipelines. Nearly all\\nadvanced use cases require full access to the data. Especially when\\nthe goal is developing realtime threat detection and response systems.\\n\\nSecurity is a data problem. But how should we represent that data? This is where\\nParquet enters the picture. As a vendor-agnostic storage format for structured\\nand nested data, it decouples storage from analytics. This is where SIEM\\nmonoliths fail: they offer a single black box that tightly couples data\\nacquision and processing capabilities. Providing a thin \\"open\\" API is not really\\nopen, as it prevents high-bandwidth data access that is needed for advanced\\nanalytics workloads.\\n\\nOpen storage prevents vendor-lock-in. When any tool can work with the data, you\\nbuild a sustainable foundation for implementing future use cases. For example,\\nwith Parquet\'s column encryption, you can offload fine-grained compliance use\\ncases to a dedicated application. Want to try out a new analytics engine? Just\\npoint it to the Parquet files.\\n\\n## Parquet\'s Little Brother\\n\\n[Feather][feather] is Parquet\'s little brother. It emerged while building a\\nproof of concept for \\"fast, language-agnostic data frame storage for Python\\n(pandas) and R.\\" The format is a thin layer on top of [Arrow\\nIPC](https://arrow.apache.org/docs/python/ipc.html#ipc), making it conducive for\\nmemory mapping and zero-copy usage. On the spectrum of speed and\\nspace-efficiency, think of it this way:\\n\\n![Parquet vs. Feather](parquet-vs-feather.light.png#gh-light-mode-only)\\n![Parquet vs. Feather](parquet-vs-feather.dark.png#gh-dark-mode-only)\\n\\nBefore Feather existed, VAST had its own storage format that was 95% like\\nFeather, minus a thin framing. (We called it the *segment store*.)\\n\\nWait, but Feather is an in-memory format and Parquet an on-disk format. You\\ncannot compare them! Fair point, but don\'t forget transparent Zstd compression.\\nFor some schemas, we barely notice a difference (e.g., PCAP), whereas for others\\nschemas, Parquet stores have less than 10% the size of Feather despite.\\n\\nThe next blog post goes into these details. For now, we want to stress that\\nFeather is in fact a reasonable format for data at rest, even when looking at\\nspace utilization alone.\\n\\n## Parquet and Feather in VAST\\n\\nVAST can store event data as Parquet or Feather. The unit of storage scaling is\\na *partition*. In Arrow terms, a partition is a persisted form of an [Arrow\\nTable][arrow-table], i.e., a concatenation of [Record\\nBatches][arrow-record-batch]. A partition has thus a fixed schema. VAST\'s [store\\nplugin][store-plugin] determines how a partition writes its buffered record\\nbatches to disk. The diagram below illustrates the architecture:\\n\\n![Parquet Analytics](parquet-analytics.light.png#gh-light-mode-only)\\n![Parquet Analytics](parquet-analytics.dark.png#gh-dark-mode-only)\\n\\n[arrow-table]: https://arrow.apache.org/docs/python/data.html#tables\\n[arrow-record-batch]: https://arrow.apache.org/docs/python/data.html#record-batches\\n[store-plugin]: /docs/understand/architecture/plugins#store\\n\\nThis architecture makes it easy to point an analytics application directly to\\nthe store files, without the need for ETLing it into a dedicated warehouse, such\\nas Spark or Hadoop.\\n\\nThe event data thrown at VAST has quite some variety of schemas. During\\ningestion, VAST first demultiplexes the heterogeneous stream of events into\\nmultiple homogenous streams, each of which has a unique schema. VAST buffers\\nevents until the partition hits a pre-configured event limit (e.g., 1M) or until\\na timeout occurs (e.g., 60m). Thereafter, VAST writes the partition in one shot\\nand persists it.\\n\\nThe buffering provides optimal freshness of the data, as it enables queries run\\non not-yet-persisted data. But it also sets an upper bound on the partition\\nsize, given that it must fit in memory in its entirety. In the future, we plan\\nto make this freshness trade-off explicit, making it possible to write out\\nlarger-than-memory stores incrementally.\\n\\n## Imbueing Domain Semantics\\n\\nIn a [past blog][blog-arrow] we described how VAST uses Arrow\'s extensible\\ntype system to add richer semantics to the data. This is how the value of VAST\\ntranscends through the analytics stack. For example, VAST has native IP address\\ntypes that you can show up in Python as [ipaddress][ipaddress] instance. This\\navoids friction in the data exchange process. Nobody wants to spend time\\nconverting bytes or strings into the semantic objects that are ultimately need\\nfor the analysis.\\n\\n[blog-arrow]: /blog/apache-arrow-as-platform-for-security-data-engineering\\n[ipaddress]: https://docs.python.org/3/library/ipaddress.html\\n\\nHere\'s how [VAST\'s type system](/docs/understand/data-model/type-system) looks\\nlike:\\n\\n![Type System](/img/type-system-vast.light.png#gh-light-mode-only)\\n![Type System](/img/type-system-vast.dark.png#gh-dark-mode-only)\\n\\nThere exist two major classes of types: *basic*, stateless types with a static\\nstructure and a-priori known representation, and *complex*, stateful types that\\ncarry additional runtime information. We map this type system without\\ninformation loss to Arrow:\\n\\n![Type System](/img/type-system-arrow.light.png#gh-light-mode-only)\\n![Type System](/img/type-system-arrow.dark.png#gh-dark-mode-only)\\n\\nVAST converts enum, adress, and subnet types to\\n[extension-types][arrow-extension-types]. All types are self-describing and part\\nof the record batch meta data. Conversion is bi-directional. Both Parquet and\\nFeather support fully nested structures in this type system. In theory. Our\\nthird blog post in this series desribes the hurdles we had to overcome to make\\nit work in practice.\\n\\n[arrow-extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n\\nIn the next blog post, we perform a quantitive analysis of the two formats: how\\nwell do they compress the original data? How much space do they take up in\\nmemory? How much CPU time do I pay for how much space savings? In the meantime,\\nif you want to learn more about Parquet, take a look at the [blog post\\nseries][arrow-parquet-blog] from the Arrow team.\\n\\n[arrow-parquet-blog]: https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"},{"id":"/a-git-retrospective","metadata":{"permalink":"/blog/a-git-retrospective","source":"@site/blog/a-git-retrospective/index.md","title":"A Git Retrospective","description":"The VAST project is roughly a decade old. But what happened over the last 10","date":"2022-09-15T00:00:00.000Z","formattedDate":"September 15, 2022","tags":[{"label":"git","permalink":"/blog/tags/git"},{"label":"r","permalink":"/blog/tags/r"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"notebooks","permalink":"/blog/tags/notebooks"},{"label":"engineering","permalink":"/blog/tags/engineering"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A Git Retrospective","authors":"mavam","date":"2022-09-15T00:00:00.000Z","tags":["git","r","quarto","notebooks","engineering","open-source"]},"prevItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"},"nextItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"}},"content":"The VAST project is roughly a decade old. But what happened over the last 10\\nyears? This blog post looks back over time through the lens of the git *merge*\\ncommits.\\n\\nWhy merge commits? Because they represent a unit of completed contribution.\\nFeature work takes place in dedicated branches, with the merge to the main\\nbranch sealing the deal. Some feature branches have just one commit, whereas\\nothers dozens. The distribution is not uniform. As of `6f9c84198` on Sep 2,\\n2022, there are a total of 13,066 commits, with 2,334 being merges (17.9%).\\nWe\u2019ll take a deeper look at the merge commits.\\n\\n\x3c!--truncate--\x3e\\n\\n``` bash\\ncd /tmp\\ngit clone https://github.com/tenzir/vast.git\\ncd vast\\ngit checkout 6f9c841980b2333028b1ac19e2a21e99d96cbd36\\ngit log --merges --pretty=format:\\"%ad|%d\\" --date=iso-strict |\\n  sed -E \'s/(.+)\\\\|.*tag: ([^,)]+).*/\\\\1 \\\\2/\' |\\n  sed -E \'s/(.*)\\\\|.*/\\\\1 NA/\' \\\\\\n  > /tmp/vast-merge-commits.txt\\n```\\n\\nFor the statistics, we\u2019ll switch to R. In all subsequent figures, a single point\\ncorresponds to a merge commit. The reduced opacity alleviates the effects of\\noverplotting.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(readr)\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read_table(\\"/tmp/vast-merge-commits.txt\\",\\n  col_names = c(\\"time\\", \\"tag\\"),\\n  col_types = \\"Tc\\"\\n) |>\\n  arrange(time) |>\\n  mutate(count = row_number())\\n\\nfirst_contribution <- \\\\(x) data |>\\n  filter(time >= x) |>\\n  pull(count) |>\\n  first()\\n\\nevents <- tribble(\\n  ~time, ~event,\\n  ymd(\\"2016-03-17\\"), \\"NSDI \'16\\\\npublication\\",\\n  ymd(\\"2017-08-31\\"), \\"Tenzir\\\\nincorporated\\",\\n  ymd(\\"2018-07-01\\"), \\"Tobias\\",\\n  ymd(\\"2019-09-15\\"), \\"Dominik\\",\\n  ymd(\\"2020-01-01\\"), \\"Benno\\",\\n  ymd(\\"2021-12-01\\"), \\"Thomas\\",\\n  ymd(\\"2022-07-01\\"), \\"Patryk\\",\\n) |>\\n  mutate(time = as.POSIXct(time), count = Vectorize(first_contribution)(time))\\n\\ndata |>\\n  ggplot(aes(x = time, y = count)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = events,\\n    aes(xend = time, yend = count + 200),\\n    color = \\"red\\"\\n  ) +\\n  geom_label(\\n    data = events,\\n    aes(y = count + 200, label = event),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  scale_x_datetime(date_breaks = \\"1 year\\", date_labels = \\"%Y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/full-time-spectrum-1.svg\';\\n\\n<Svg1 />\\n\\nPrior to Tenzir taking ownership of the project and developing VAST, it was a\\ndissertation project evolving along during PhD work at the University of\\nCalifornia, Berkeley. We can see that the first pre-submission crunch started a\\nfew months before the [NSDI \u201916\\npaper](https://matthias.vallentin.net/papers/nsdi16.pdf).\\n\\nTenzir was born in fall 2017. Real-world contributions arrived as of 2018 when\\nthe small team set sails. Throughput increased as core contributors joined the\\nteam. Fast-forward to 2020 when we started doing public releases. The figure\\nbelow shows how this process matured.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\ndata |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.1) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"red\\",\\n    segment.alpha = 0.2,\\n    box.padding = 0.2\\n  ) +\\n  scale_x_datetime(\\n    date_breaks = \\"1 year\\",\\n    limits = c(as.POSIXct(ymd(\\"2020-01-01\\")), max(data$time)),\\n    date_labels = \\"%Y\\"\\n  ) +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/since-2020-1.svg\';\\n\\n<Svg2 />\\n\\nAs visible from the tag labels, we were at [CalVer](https://calver.org) for a\\nwhile, but ultimately switched to [SemVer](https://semver.org). Because we had\\nalready commercial users at the time, this helped us better communicate breaking\\nvs.\xa0non-breaking changes.\\n\\nLet\u2019s zoom in on all releases since v1.0. At this time, we had a solid\\nengineering and release process in place.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tidyr)\\nv1_0_0_rc1_time <- data |>\\n  filter(tag == \\"v1.0.0-rc1\\") |>\\n  pull(time)\\n\\nsince_v1_0_0_rc1 <- data |> filter(time >= v1_0_0_rc1_time)\\n\\nrc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(grepl(\\"rc\\", tag))\\n\\nnon_rc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(!grepl(\\"rc\\", tag))\\n\\nsince_v1_0_0_rc1 |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = non_rc,\\n    aes(xend = time, yend = min(count)), color = \\"red\\"\\n  ) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"grey\\",\\n    box.padding = 0.7\\n  ) +\\n  geom_point(\\n    data = rc, aes(x = time, y = count),\\n    color = \\"blue\\",\\n    size = 2\\n  ) +\\n  geom_point(\\n    data = non_rc, aes(x = time, y = count),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  geom_label(data = non_rc, aes(y = min(count)), size = 2, color = \\"red\\") +\\n  scale_x_datetime(date_breaks = \\"1 month\\", date_labels = \\"%b %y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/since-v1.0-1.svg\';\\n\\n<Svg3 />\\n\\nThe v2.0 release was a hard one for us, given the long distance to v1.1. We\\nmerged too much and testing took forever. Burnt by the time sunk in testing and\\nfixups, we decided to switch to an LPU model (\u201cleast publishable unit\u201d) to\\nreduce release cadence. We didn\u2019t manage to implement this model until after\\nv2.1 though, where the release cadence finally gets smaller. A monthly release\\nfeels about the right for our team size.\\n\\nThe key challenge is minimizing the feature freeze phase. The first release\\ncandidate (RC) kicks this phase off, and the final release lifts the\\nrestriction. In this period, features are not allowed to be merged.[^1] This is\\na delicate time window: too long and the fixups in the RC phase cause the\\npostponed pull requests to diverge, too short and we compromise on testing\\nrigor, causing a release that doesn\u2019t meet our Q&A requirements.\\n\\nThis is where we stand as of today. We\u2019re happy how far along we came, but\\nmany challenges still lay ahead of us. Increased automation and deeper testing\\nis the overarching theme, e.g., code coverage, fuzzing, GitOps. We\u2019re constantly\\nstriving to improve or processes. With a small team of passionate, senior\\nengineers, this is a lot of fun!\\n\\n[^1]: We enforced this with a `blocked` label. CI [doesn\u2019t allow\\n    merging](https://github.com/tenzir/vast/blob/6f9c841980b2333028b1ac19e2a21e99d96cbd36/.github/workflows/blocked.yaml) when this label is on a pull request."},{"id":"/public-roadmap-and-open-rfcs","metadata":{"permalink":"/blog/public-roadmap-and-open-rfcs","source":"@site/blog/public-roadmap-and-open-rfcs/index.md","title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","date":"2022-09-07T00:00:00.000Z","formattedDate":"September 7, 2022","tags":[{"label":"roadmap","permalink":"/blog/tags/roadmap"},{"label":"github","permalink":"/blog/tags/github"},{"label":"rfc","permalink":"/blog/tags/rfc"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":3.745,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","authors":"mavam","date":"2022-09-07T00:00:00.000Z","tags":["roadmap","github","rfc","open-source"]},"prevItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"},"nextItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"}},"content":"We are happy to announce that we have published [our engineering\\nroadmap][roadmap] along with an [RFC process][rfc] to actively participate in\\nshaping upcoming topics. This blog post explains why and how we did it.\\n\\n[roadmap]: https://vast.io/roadmap\\n[rfc]: /docs/contribute/rfc\\n\\n\x3c!--truncate--\x3e\\n\\nAs a community-first open-source project, we constantly strive for increasing\\ntransparency. Our long-term goal is establishing a fully open governance model.\\nThis will allow for a clear delineation of the open-source project and unbiased\\ncommercial offerings that we, the engineering team behind VAST at\\n[Tenzir](https://tenzir.com), provide on top. Until we have bootstrapped\\nourselves and an active community, we aim for the right balance between open\\nand closed.\\n\\nOne step in the direction of open is publishing our [roadmap][roadmap] and\\nenabling the community to participate in the planning through an [Request For\\nComments (RFC)][rfc] process.\\n\\n## Public Roadmap\\n\\nIn the process of opening the roadmap, we had to answer several questions:\\n\\n1. **Audience**: should the content be for users only? What about developers?\\n   Should we only mention features or also refactorings?\\n\\n2. **Interaction**: should this just be a read-only page or something the\\n   community can directly interact with?\\n\\n3. **Tooling**: what is the right tool to encode the roadmap?\\n\\nLet\'s go through them one by one.\\n\\nRegarding audience, we want to avoid an overly narrow target group, as we are in\\nphase of growth where breadth instead of depth is more important. Moreover, we\\ngain more transparency if we can unveil all ongoing thrusts. Therefore, we want\\nto cover the full spectrum of personas, but make it possible for each individual\\ntype of persona to get a relevant view.\\n\\nRegarding interaction, we are actively looking for engagement. Throwing a\\nread-only version over the fence to the community is certainly informational,\\nbut we are looking for creating dialogue. Therefore, we want to allow everyone\\nto discuss the various roadmap items in the open.\\n\\nRegarding tooling, we are in need for something that integrates well with the\\nexisting environment. Our GitHub presence includes code, documentation, website\\ncontent, and third-party integrations. We also promote use of GitHub Discussions\\nto engage with us. This makes GitHub the focal point to engage with the content.\\nTherefore, we decided to encode the roadmap as GitHub issues; for clarity in a\\ndedicated repository at <https://github.com/tenzir/public-roadmap>.\\n\\nWe decided against dual-purposing the issue tracker of our main repository\\n<https://github.com/tenzir/vast> because it would add roadmap items as many\\nopen, long-running issues that scatter the attention and potentially confuse the\\ncommunity. That said, the primary value of the issue tracker is the layer on top\\nof issues: [GitHub Projects][github-projects], which allows for organizing\\nissues across multiple dimensions in a visually appealing way.\\n\\n[github-projects]: https://docs.github.com/en/issues/planning-and-tracking-with-projects\\n\\nThe quarterly board view make it easy to understand ongoing thrusts:\\n\\n[![Github Roadmap - Board](roadmap-board.jpg)][roadmap]\\n\\nThe milestones view provides a different perspective that focuses more on the\\nbigger-picture theme:\\n\\n[![Github Roadmap - Milestones](roadmap-milestones.jpg)][roadmap]\\n\\n## Open RFCs\\n\\nThe roadmap provides a lens into the short-term future. We don\'t want it to be\\njust read-only. Fundamentally, we want to build something that our users love.\\nWe also want to tap into the full potential of our enthusiasts by making it\\npossible to engage in technical depth with upcoming changes. Therefore, we are\\nestablishing a formal [Request for Comments (RFC) process][rfc].\\n\\nTo get an idea, how an RFC looks like, here\'s the [RFC template][rfc-template]:\\n\\n[rfc-template]: https://github.com/tenzir/vast/blob/master/rfc/000-template/README.md\\n\\nimport CodeBlock from \'@theme/CodeBlock\';\\nimport Template from \'!!raw-loader!@site/../rfc/000-template/README.md\';\\n\\n<CodeBlock language=\\"markdown\\">{Template}</CodeBlock>\\n\\n[RFC-001: Composable Pipelines](https://github.com/tenzir/vast/pull/2511) is an\\nexample instantiation of this template.\\n\\nThe RFC reviews take place 100% in the open. As of today, reviewers constitute\\nmembers from Tenzir\'s engineering team. Given our current resource constraints\\nand project state, we can only support a corporate-backed governance model. That\\nsaid, opening ourselves up is laying the foundation of trust and committment\\nthat we want to go beyond a walled garden. We understand that this is a long\\njourney and are excited about what\'s ahead of us.\\n\\nWhen an RFC gets accepted, it means that we put it on the roadmap, adjacent to\\nexisting items that compete for prioritization. In other words, even though we\\naccepted an RFC, there will be an indeterminate period of time until we can\\ndevote resources. We will always encourage community-led efforts and are\\nenthusiastic about supporting external projects that we can support within our\\ncapacities.\\n\\nThese are our \\"growing pains\\" that we can hopefully overcome together while\\nbuilding a thriving community. We still have our [community\\nSlack](http://slack.tenzir.com) where we are looking forward to interact with\\neveryone with questions or feedback. See you there!"},{"id":"/vast-v2.3","metadata":{"permalink":"/blog/vast-v2.3","source":"@site/blog/vast-v2.3/index.md","title":"VAST v2.3","description":"Automatic Rebuilds","date":"2022-09-01T00:00:00.000Z","formattedDate":"September 1, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.905,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.3","description":"Automatic Rebuilds","authors":"dominiklohmann","date":"2022-09-01T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"},"nextItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"}},"content":"[VAST v2.3][github-vast-release] is now available, which introduces an automatic\\ndata defragmentation capability.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Automatic Rebuilds\\n\\nVAST server processes now continuously rebuild partitions in the background. The\\nfollowing diagram visualizes what happens under the hood:\\n\\n![Rebuild](/img/rebuild-light.png#gh-light-mode-only)\\n![Rebuild](/img/rebuild-dark.png#gh-dark-mode-only)\\n\\nRebuilding kicks in when a partition has the following properties:\\n\\n1. **Outdated**: if a partitions does not have the latest partition version, it\\n   may not enjoy the latest features and optimizations. It makes it also faster\\n   to adopt VAST versions that include breaking changes in the storage layout.\\n   Therefore, VAST rebuilds outdated partitions to bring them into the most\\n   recent state.\\n\\n2. **Undersized**: numerous small partitions can cause fragmentation in the\\n   catalog, causing higher memory consumption, larger database footprint, and\\n   slower queries. Rebuilding merges undersized partitions, thereby\\n   defragmenting the system. This reduces the resource footprint and makes\\n   queries faster.\\n\\nTo enable automatic rebuilding, set the new `vast.automatic-rebuild` option.\\n\\n```yaml\\nvast:\\n  # Control automatic rebuilding of partitions in the background for\\n  # optimization purposes. The given number controls how many rebuilds to run\\n  # concurrently, and thus directly controls the performance vs. memory and CPU\\n  # usage trade-off. Set to 0 to disable. Defaults to 1.\\n  automatic-rebuild: 1\\n```\\n\\nNow that we have an LSM-style merge operation of partitions, we reduced\\nthe partition cutoff timeout to 5 minutes from 1 hour by default (controlled\\nthrough the option `vast.active-partition-timeout`). This reduces the risk of\\ndata loss in case of a crash. This comes in handy in particular for low-volume\\ndata sources that never exhaust their capacity.\\n\\n## Optional Partition Indexes\\n\\nHistorically, VAST evolved from a special-purpose bitmap indexing system into a\\ngeneral-purpose telemetry engine for security data. Today, VAST has a two-tiered\\nindexing architecture with sparse sketch structures at the top, followed by a\\nsecond layer of dense indexes. As of this release, it is possible to disable\\nthis second layer.\\n\\nThe space savings can be substantial based on the size of your index. For\\nexample, if the first layer of indexing always yields highly selective results,\\nthen it the dense indexes do not provide a lot of value. One scenario would be\\nretro-matching: if you only do IoC-style point queries, they will be most likely\\ncovered well by the sketches. If you do not have selective queries, the dense\\nindex is not helping much anyway, since you need access the base data anyway. A\\nreally good use case for the indexes when your have a scatterd data access\\npatterns, i.e., highly selective results *within* a partition, but a result that\\nspans many disparate partitions.\\n\\nIn a simplified model, VAST performs three steps when executing a query:\\n\\n1. Send the query to the catalog, which maintains VAST\'s partitions, and ask it\\n   for a list of candidate partitions. The catalog maintains the first tier of\\n   sparse indexes, currently one per partition.\\n\\n2. Send the query to all candidate partitions in parallel, each of which\\n   contains dense indexes for all fields in the partition\'s schema. The index\\n   lookup yields a set of candidate records IDs within the partition.\\n\\n3. Send the query to all candidate partition\'s stores, provided the index lookup\\n   yielded record IDs. Then evaluating the query against the candidate events\\n   and return the result.\\n\\nHere\'s how you can configure a partition index to be disabled:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n        # Don\'t create partition indexes the suricata.http.http.url field.\\n      - targets:\\n          - suricata.http.http.url\\n        partition-index: false\\n        # Don\'t create partition indexes for fields of type addr.\\n      - targets:\\n          - :addr\\n        partition-index: false\\n```\\n\\n## Improved Responsiveness Under High Load\\n\\nTwo small changes improve VAST\'s behavior under exceptionally high load.\\n\\nFirst, the new `vast.connection-timeout` option allows for modifying the default\\nclient-to-server connection timeout of 10 seconds. Previously, if a VAST server\\nwas too busy to respond to a new client within 10 seconds, the client simply\\nexited with an unintelligable `request_timeout` error message. Here\'s how you\\ncan set a custom timeout:\\n\\n```yaml\\nvast:\\n  # The timeout for connecting to a VAST server. Set to 0 seconds to wait\\n  # indefinitely.\\n  connection-timeout: 10s\\n```\\n\\nThe option is additionally available under the environment variable\\n`VAST_CONNECTION_TIMEOUT` and the `--connection-timeout` command-line option.\\n\\nSecond, we improved the operability of VAST servers under high load from\\nautomated low-priority queries. We noticed that when spawning thousands of\\nautomated retro-match queries that compaction would stall and make little\\nvisible progress, risking the disk running full or no longer being compliant\\nwith GDPR-related policies enforced by compaction.\\n\\nTo ensure that compaction\'s internal and regular user-issued queries work as\\nexpected even in this scenario, VAST now considers queries issued with\\n`--low-priority`, with even less priority compared to regular queries (down from\\n33.3% to 4%) and internal high-priority queries used for rebuilding and\\ncompaction (down from 12.5% to 1%)."},{"id":"/richer-typing-in-sigma","metadata":{"permalink":"/blog/richer-typing-in-sigma","source":"@site/blog/richer-typing-in-sigma/index.md","title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","date":"2022-08-12T00:00:00.000Z","formattedDate":"August 12, 2022","tags":[{"label":"sigma","permalink":"/blog/tags/sigma"},{"label":"regex","permalink":"/blog/tags/regex"},{"label":"query-frontend","permalink":"/blog/tags/query-frontend"}],"readingTime":4.72,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","authors":"mavam","date":"2022-08-12T00:00:00.000Z","tags":["sigma","regex","query-frontend"]},"prevItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"},"nextItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"}},"content":"VAST\'s [Sigma frontend](/docs/understand/query-language/frontends/sigma)\\nnow supports more modifiers. In the Sigma language, modifiers transform\\npredicates in various ways, e.g., to apply a function over a value or to change\\nthe operator of a predicate. Modifiers are the customization point to enhance\\nexpressiveness of query operations.\\n\\nThe new [pySigma][pysigma] effort, which will eventually replace the\\nnow-considered-legacy [sigma][sigma] project, comes with new modifiers as well.\\nMost notably, `lt`, `lte`, `gt`, `gte` provide comparisons over value domains\\nwith a total ordering, e.g., numbers: `x >= 42`. In addition, the `cidr`\\nmodifier interprets a value as subnet, e.g., `10.0.0.0/8`. Richer typing!\\n\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[pysigma]: https://github.com/SigmaHQ/pySigma\\n\\n\x3c!--truncate--\x3e\\n\\nHow does the frontend work? Think of it as a parser that processes the YAML and\\ntranslates it into an expression tree, where the leaves are predicates with\\ntyped operands according to VAST\'s data model. Here\'s how it works:\\n\\n![Sigma Query Frontend](/img/sigma-query-frontend-light.png#gh-light-mode-only)\\n![Sigma Query Frontend](/img/sigma-query-frontend-dark.png#gh-dark-mode-only)\\n\\nLet\'s take a closer look at some Sigma rule modifiers:\\n\\n```yaml\\nselection:\\n  x|re: \'f(o+|u)\'\\n  x|lt: 42\\n  x|cidr: 192.168.0.0/23\\n  x|base64offset|contains: \'http://\'\\n```\\n\\nThe `|` symbol applies a modifier to a field. Let\'s walk through the above\\nexample:\\n\\n1. The `re` modifier changes the predicate operand from `x == \\"f(o+|u)\\"` to\\n   `x == /f(o+|u)/`, i.e., the type of the right-hand side changes from `string`\\n   to `pattern`.\\n\\n2. The `lt` modifier changes the predicate operator from `==` to `<`, i.e.,\\n   `x == 42` becomes `x < 42`.\\n\\n3. The `cidr` modifier changes the predicate operand to type subnet. In VAST,\\n   parsing the operand type into a subnet happens automatically, so the Sigma\\n   frontend only changes the operator to `in`. That is, `x == \\"192.168.0.0/23\\"`\\n   becomes `x in 192.168.0.0/23`. Since VAST supports top-k prefix search on\\n   subnets natively, nothing else needs to be changed.\\n\\n   Other backends expand this to:\\n\\n   ```c\\n   x == \\"192.168.0.*\\" || x == \\"192.168.1.*\\"\\n   ```\\n\\n   This expansion logic on strings doesn\'t scale very well: for a `/22`, you\\n   would have to double the number of predicates, and for a `/21` quadruple\\n   them. This is where rich and deep typing in the language pays off.\\n\\n4. `x`: there are two modifiers that operate in a chained fashion,\\n   transforming the predicate in two steps:\\n\\n   1. Initial: `x == \\"http://\\"`\\n   2. `base64offset`: `x == \\"aHR0cDovL\\" || x == \\"h0dHA6Ly\\" || x == \\"odHRwOi8v\\"`\\n   3. `contains`: `x in \\"aHR0cDovL\\" || x in \\"h0dHA6Ly\\" || x in \\"odHRwOi8v\\"`\\n\\n   First, `base64offset` always expands a value into a disjunction of 3\\n   predicates, each of which performs an equality comparison to a\\n   Base64-transformed value.[^1]\\n\\n   Thereafter, the `contains` modifier translates the respective predicate\\n   operator from `==` to `in`. Other Sigma backends that don\'t support substring\\n   search natively transform the value instead by wrapping it into `*`\\n   wildcards, e.g., translate `\\"foo\\"` into `\\"*foo*\\"`.\\n\\n[^1]: What happens under the hood is a padding a string with spaces. [Anton\\nKutepov\'s article][sigma-article] illustrates how this works.\\n\\n[sigma-article]: https://tech-en.netlify.app/articles/en513032/index.html\\n\\nOur ultimate goal is to support a fully function executional platform for Sigma\\nrules. The table below shows the current implementation status of modifiers,\\nwhere \u2705 means implemented, \ud83d\udea7 not yet implemented but possible, and \u274c not yet\\nsupported by VAST\'s execution engine:\\n\\n|Modifier|Use|sigmac|VAST|\\n|--------|---|:----:|:--:|\\n|`contains`|perform a substring search with the value|\u2705|\u2705|\\n|`startswith`|match the value as a prefix|\u2705|\u2705|\\n|`endswith`|match the value as a suffix|\u2705|\u2705|\\n|`base64`|encode the value with Base64|\u2705|\u2705\\n|`base64offset`|encode value as all three possible Base64 variants|\u2705|\u2705\\n|`utf16le`/`wide`|transform the value to UTF16 little endian|\u2705|\ud83d\udea7\\n|`utf16be`|transform the value to UTF16 big endian|\u2705|\ud83d\udea7\\n|`utf16`|transform the value to UTF16|\u2705|\ud83d\udea7\\n|`re`|interpret the value as regular expression|\u2705|\ud83d\udea7\\n|`cidr`|interpret the value as a IP CIDR|\u274c|\u2705\\n|`all`|changes the expression logic from OR to AND|\u2705|\u2705\\n|`lt`|compare less than (`<`) the value|\u274c|\u2705\\n|`lte`|compare less than or equal to (`<=`) the value|\u274c|\u2705\\n|`gt`|compare greater than (`>`) the value|\u274c|\u2705\\n|`gte`|compare greater than or equal to (`>=`) the value|\u274c|\u2705\\n|`expand`|expand value to placeholder strings, e.g., `%something%`|\u274c|\u274c\\n\\nAside from completing the implementation of the missing modifiers, there are\\nthree missing pieces for Sigma rule execution to become viable in VAST:\\n\\n1. **Regular expressions**: VAST currently has no efficient mechanism to execute\\n   regular expressions. A regex lookup requires a full scan of the data.\\n   Moreover, the regular expression execution speed is abysimal. But we are\\n   aware of it and are working on this soon. The good thing is that the\\n   complexity of regular expression execution over batches of data is\\n   manageable, given that we would call into the corresponding [Arrow Compute\\n   function][arrow-containment-tests] for the heavy lifting. The number one\\n   challenge will be reduing the data to scan, because the Bloom-filter-like\\n   sketch data structures in the catalog cannot handle pattern types. If the\\n   sketches cannot identify a candidate set, all data needs to be scanned,\\n\\n   To alleviate the effects of full scans, it\'s possible to winnow down the\\n   candidate set of partitions by executing rules periodically. When making the\\n   windows asymptotically small, this yields effectively streaming execution,\\n   which VAST already supports in the form of \\"live queries\\".\\n\\n2. **Case-insensitive strings**: All strings in Sigma rules are case-insensitive\\n   by default, but VAST\'s string search is case-sensitive. As a workaround, we\\n   could translate Sigma strings into regular expressions, e.g., `\\"Foo\\"` into\\n   `/Foo/i`. Unfortunately there is a big performance gap between string\\n   equality search and regular expression search. We will need to find a better\\n   solution for production-grade rule execution.\\n\\n3. **Field mappings**: while Sigma rules execute already syntactically, VAST\\n   currently doesn\'t touch the field names in the rules and interprets them as\\n   [field extractors][field-extractors]. In other words, VAST doesn\'t support\\n   the Sigma taxonomy yet. Until we provide the mappings, you can already write\\n   generic Sigma rules using [concepts][concepts].\\n\\n[arrow-containment-tests]: https://arrow.apache.org/docs/cpp/compute.html#containment-tests\\n[field-extractors]: https://vast.io/docs/understand/query-language/expressions#field-extractor\\n[concepts]: https://vast.io/docs/understand/data-model/taxonomies#concepts\\n\\nPlease don\'t hesitate to swing by our [Community Slack](http://slack.tenzir.com)\\nand talk with us if you are passionate about Sigma and other topics around open\\ndetection and response."},{"id":"/vast-v2.2","metadata":{"permalink":"/blog/vast-v2.2","source":"@site/blog/vast-v2.2/index.md","title":"VAST v2.2","description":"Pipelines","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"summarize","permalink":"/blog/tags/summarize"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":2.145,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.2","description":"Pipelines","authors":"lava","date":"2022-08-05T00:00:00.000Z","tags":["release","summarize","pipelines"]},"prevItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"},"nextItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"}},"content":"We released [VAST v2.2][github-vast-release] \ud83d\ude4c! Transforms now have a new name:\\n[pipelines](/blog/vast-v2.2#transforms-are-now-pipelines). The [summarize\\noperator](/blog/vast-v2.2#summarization-improvements) also underwent a facelift,\\nmaking aggregation functions pluggable and allowing for assigning names to\\noutput fields.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.2.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Transforms are now Pipelines\\n\\nAfter carefully reconsidering our naming decisions related to query execution\\nand data transformation, we came up with a naming convention that does a better\\njob in capturing the underlying concepts.\\n\\nMost notably, we renamed *transforms* to *pipelines*. A transform *step* is now a\\npipeline *operator*. This nomenclature is much more familiar to users coming\\nfrom dataflow and collection-based query engines. The implementation underneath\\nhasn\'t changed. As in the [Volcano model][volcano], data still flows through\\noperators, each of which consumes input from upstream operators and produces\\noutput for downstream operators. What we term a pipeline is the sequence of such\\nchained operators.\\n\\n[volcano]: https://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf\\n\\nWhile pipelines are not yet available at the query layer, they soon will be.\\nUntil then, you can deploy pipelines at load-time to [transform data in motion\\nor data at rest](/docs/use/transform).\\n\\nFrom a user perspective, the configuration keys associated with transforms have\\nchanged. Here\'s the updated example from our previous [VAST v1.0 release\\nblog](/blog/vast-v1.0).\\n\\n```yaml\\nvast:\\n  # Specify and name our pipelines, each of which are a list of configured\\n  # pipeline operators. Pipeline operators are plugins, enabling users to \\n  # write complex transformations in native code using C++ and Apache Arrow.\\n  pipelines:\\n     # Prevent events with certain strings to be exported, e.g., \\n     # \\"tenzir\\" or \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each pipeline at server- or client-side, on\\n  # `import` or `export`, and restrict them to a list of event types.\\n  pipeline-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - pipeline: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Summarization Improvements\\n\\nIn line with the above nomenclature changes, we\'ve improved the behavior of the\\n[`summarize`][summarize] operator. It is now possible to specify an explicit\\nname for the output fields. This is helpful when the downstream processing needs\\na predictable schema. Previously, VAST took simply the name of the input field.\\nThe syntax was as follows:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    min:\\n      - ts # implied name for aggregate field\\n```\\n\\nWe now switched the syntax such that the new field name is at the beginning:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    ts_min: # explicit name for aggregate field\\n      min: ts\\n```\\n\\nIn SQL, this would be the `AS` token: `SELECT min(ts) AS min_ts`.\\n\\n[summarize]: /docs/understand/query-language/operators/summarize"},{"id":"/vast-v2.1","metadata":{"permalink":"/blog/vast-v2.1","source":"@site/blog/vast-v2.1/index.md","title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","date":"2022-07-07T00:00:00.000Z","formattedDate":"July 7, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","authors":"dominiklohmann","date":"2022-07-07T00:00:00.000Z","tags":["release","performance"]},"prevItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"},"nextItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"}},"content":"[VAST v2.1][github-vast-release] is out! This release comes with a particular\\nfocus on performance and reducing the size of VAST databases. It brings a new\\nutility for optimizing databases in production, allowing existing deployments to\\ntake full advantage of the improvements after upgrading.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## New Project Site\\n\\nVAST has new project site: [vast.io](https://vast.io). We ported all\\ndocumentation from `https://docs.tenzir.com`, added a lot of new content, and\\nrestructured the reading experience along the user journey.\\n\\nYou can find the Threat Bus documentation in [Use VAST \u2192 Integrate \u2192 Threat\\nBus](/docs/use/integrate/threatbus). Threat Bus is now officially in\\nmaintainance mode: we are only supporting existing features with bugfixes. That\\nsaid, Threat Bus will resurface in a new shape with its existing functionality\\nintegrated into VAST itself. Stay tuned.\\n\\n## Performance Improvements\\n\\nVAST now compresses data with [Zstd](http://www.zstd.net). The default\\nconfiguration achieves over 2x space savings. When transferring data between\\nclient and server processes, compression reduces the amount of transferred data\\nby up to 5x.\\n\\nAdditionally, VAST now compresses on-disk indexes with Zstd, resulting in a\\n50-80% size reduction depending on the type of indexes used.\\n\\nThis allowed us to increase the default partition size from 1,048,576 to\\n4,194,304 events[^1], and the default number of events in a single batch from 1,024\\nto 65,536, resulting in a massive performance increase at the cost of a ~20%\\nlarger memory footprint at peak loads. Use the option `vast.max-partition-size`\\nto tune this space-time tradeoff.\\n\\nTo benchmark this, we used [`speeve`][speeve] to generate 20 EVE JSON files\\ncontaining 8,388,608 events each[^2]. We spawned a VAST server process and ran\\n20 VAST client processes in parallel, with one process per file.\\n\\nWe observed a reduction of **up to 73%** of disk space utilization:\\n\\n![Database Size](storage-light.png#gh-light-mode-only)\\n![Database Size](storage-dark.png#gh-dark-mode-only)\\n\\nIn addition, we were able to scale the ingest rate by almost **6x** due to the\\nhigher batch size and the reduced memory usage per batch:\\n\\n![Ingest Rate](rate-light.png#gh-light-mode-only)\\n![Ingest Rate](rate-dark.png#gh-dark-mode-only)\\n\\nThe table below summaries the benchmarks:\\n\\n||VAST v2.0|VAST v2.1|Change|\\n|-:|:-|:-|:-|\\n|Ingest Duration|1,650 s|242 s|-85.3%|\\n|Ingest Rate|101,680 events/s|693,273 events/s|+581.8%|\\n|Index Size|14,791 MiB|5,721 MiB|-61.3%|\\n|Store Size|37,656 MiB|8,491 MiB|-77.5%|\\n|Database Size|52,446 MiB|14,212 MiB|-72.9%|\\n\\n:::note Compressed Filesystems\\nThe above benchmarks ran on filesystems without compression. We expect the gain\\nfrom compression to be smaller when using compressed filesystems like\\n[`btrfs`][btrfs].\\n:::\\n\\n[speeve]: https://github.com/satta/speeve\\n[btrfs]: https://btrfs.wiki.kernel.org/index.php/Main_Page\\n\\n[^1]: VAST v2.0 failed to write its partitions to disk with the defaults for\\n  v2.1 because the on-disk size exceeded the maximum possible size of a\\n  FlatBuffers table, which VAST internally uses to have an open standard for its\\n  persistent state.\\n[^2]: This resulted in 167,772,160 events, with a total of 200\'917\'930 unique\\n  values with a schema distribution of 80.74% `suricata.flow`, 7.85%\\n  `suricata.dns`, 5.35% `suricata.http`, 4.57% `suricata.fileinfo`, 1.04%\\n  `suricata.tls`, 0.41% `suricata.ftp`, and 0.04% `suricata.smtp`.\\n\\n## Rebuild VAST Databases\\n\\nThe new changes to VAST\'s internal data format only apply to newly ingested\\ndata. To retrofit changes, we introduce a new `rebuild` command with this\\nrelease. A rebuild effectively re-ingests events from existing partitions and\\natomically replaces them with partitions of the new format.\\n\\nThis makes it possible to upgrade persistent state to a newer version, or\\nrecreate persistent state after changing configuration parameters, e.g.,\\nswitching from the Feather to the Parquet store backend (that will land in\\nv2.2). Rebuilding partitions also recreates their sparse indexes that\\naccellerate query execution. The process takes place asynchronously in the\\nbackground.\\n\\nWe recommend running `vast rebuild` to upgrade your VAST v1.x partitions to VAST\\nv2.x partitions to take advantage of the new compression and an improved\\ninternal representation.\\n\\nThis is how you run it:\\n\\n```bash\\nvast rebuild [--all] [--undersized] [--parallel=<number>] [<expression>]\\n```\\n\\nA rebuild is not only useful when upgrading outdated partitions, but also when\\nchanging parameters of up-to-date partitions. Use the `--all` flag to extend a\\nrebuild operation to _all_ partitions. (Internally, VAST versions the partition\\nstate via FlatBuffers. An outdated partition is one whose version number is not\\nthe newest.)\\n\\nThe `--undersized` flag causes VAST to only rebuild partitions that are under\\nthe configured partition size limit `vast.max-partition-size`.\\n\\nThe `--parallel` options is a performance tuning knob. The parallelism level\\ncontrols how many sets of partitions to rebuild in parallel. This value defaults\\nto 1 to limit the CPU and memory requirements of the rebuilding process, which\\ngrow linearly with the selected parallelism level.\\n\\nAn optional expression allows for restricting the set of partitions to rebuild.\\nVAST performs a catalog lookup with the expression to identify the set of\\ncandidate partitions. This process may yield false positives, as with regular\\nqueries, which may cause unaffected partitions to undergo a rebuild. For\\nexample, to rebuild outdated partitions containing `suricata.flow` events\\nolder than 2 weeks, run the following command:\\n\\n```bash\\nvast rebuild \'#type == \\"suricata.flow\\" && #import_time < 2 weeks ago\'\\n```"},{"id":"/apache-arrow-as-platform-for-security-data-engineering","metadata":{"permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering","source":"@site/blog/apache-arrow-as-platform-for-security-data-engineering/index.md","title":"Apache Arrow as Platform for Security Data Engineering","description":"How VAST leverages Apache Arrow for Security Data Engineering","date":"2022-06-17T00:00:00.000Z","formattedDate":"June 17, 2022","tags":[{"label":"architecture","permalink":"/blog/tags/architecture"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":6.05,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"description":"How VAST leverages Apache Arrow for Security Data Engineering","authors":"mavam","date":"2022-06-17T00:00:00.000Z","tags":["architecture","arrow","performance","query"]},"prevItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"},"nextItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"}},"content":"VAST bets on [Apache Arrow][arrow] as the open interface to structured data. By\\n\\"bet,\\" we mean that VAST does not work without Arrow. And we are not alone.\\nInflux\'s [IOx][iox], DataDog\'s [Husky][husky], Anyscale\'s [Ray][ray],\\n[TensorBase][tensorbase], and [others][arrow-projects] committed themselves to\\nmaking Arrow a corner stone of their system architecture. For us, Arrow was not\\nalways a required dependency. We shifted to a tighter integration over the years\\nas the Arrow ecosystem matured. In this blog post we explain our journey of\\nbecoming an Arrow-native engine.\\n\\n[arrow]: https://arrow.apache.org\\n[iox]: https://github.com/influxdata/influxdb_iox\\n[husky]: https://www.datadoghq.com/blog/engineering/introducing-husky/\\n[ray]: https://github.com/ray-project/ray\\n[tensorbase]: https://github.com/tensorbase/tensorbase\\n[arrow-projects]: https://arrow.apache.org/powered_by/\\n\\n\x3c!--truncate--\x3e\\n\\nToday, the need to bring advanced security analytics and data engineering\\ntogether is stronger than ever, but there is a huge gap between the two fields.\\nWe see Arrow as the vehicle to close this gap, allowing us developers to\\npractice *security data engineering* to make security analytics easy for users.\\nThat is, the experience should allow experts to interact with the data in the\\nsecurity domain, end-to-end without context switching. To achieve this, we began\\nour journey with VAST by developing a data model for structured security\\ntelemetry. Having worked for a decade with the [Zeek][zeek] (fka. Bro) network\\nsecurity monitor, we understood the value of having first-class support for\\ndomain-specific entities (e.g., native representation of IPv4 and IPv6\\naddresses) and type-specific operations (e.g., the ability to perform top-k\\nprefix search to answer subnet membership queries). In addition, the ability to\\nembed domain semantics with user-defined types (e.g., IP addresses, subnets, and\\nURLs) was central to expressing complex relationships to develop effective\\nanalytical models. It was clear that we needed the domain model deep in the core\\nof the system to successfully support security analytics.\\n\\nAfter having identified the data model requirements, the question of\\nrepresentation came next. At first, we unified the internal representation with\\na row-oriented representation using [MsgPack][msgpack], which comes with a\\nmechanism for adding custom types. The assumption was that a row-based data\\nrepresentation more closely matches typical event data (e.g., JSONL) and\\ntherefore allows for much higher processing rates. Moreover, early use cases of\\nVAST were limited to interactive, multi-dimensional search to extract a subset\\nof *entire* records, spread over a longitudinal archive of data. The\\nrow-oriented encoding worked well for this.\\n\\nBut as security operations were maturing, requirements extended to analytical\\nprocessing of structured data, making a columnar format increasingly beneficial.\\nAfter having witnessed first-hand the early commitment of [Ray][ray] to Arrow,\\nwe started using Arrow as optional dependency as additional column-oriented\\nencoding. We abstracted a batch of data encoding-independent behind a \\"table\\nslice\\":\\n\\n![MsgPack & Arrow](msgpack-arrow.light.png#gh-light-mode-only)\\n![MsgPack & Arrow](msgpack-arrow.dark.png#gh-dark-mode-only)\\n\\nHiding the concrete encoding behind a cell-based access interface worked for\\nlow-volume use cases, but backfired as we scaled up and slowed us down\\nsubstantially in development. We needed to make a choice. This is where timing\\nwas right: our perception of the rapidly evolving Arrow ecosystem changed.\\nArrow-based runtimes were mushrooming all over the place. Nowadays it requires\\nonly a few lines of code to integrate Arrow data into the central logic of\\napplications. We realized that the primary value proposition of Arrow is to\\n*make data interoperability easy*.\\n\\nBut data interoperability is only a sufficient condition for enabling\\nsustainable security analytics. The differentiating value of a *security* data\\nplatform is support for the *security* domain. This is where Arrow\'s [extension\\ntypes][extension-types] come into play. They add *semantics* to otherwise\\ngeneric types, e.g., by telling the user \\"this is a transport-layer port\\" and\\nnot just a 16-bit unsigned integer, or \\"this is a connection 4-tuple to\\nrepresent a network flow\\" instead of \\"this is a record with 4 fields of type\\nstring and unsigned integer\\". Extension types are composable and allow for\\ncreating a rich typing layer with meaningful domain objects on top of a\\nstandardized data representation. Since they are embedded in the data, they do\\nnot have to be made available out-of-band when crossing the boundaries of\\ndifferent tools. Now we have self-describing security data.\\n\\nInteroperability plus support for a domain-specific data model makes Arrow a\\nsolid *data plane*. It turns out that Arrow is much more than a standardized\\ndata representation. Arrow also comes with bag of tools for working with the\\nstandardized data. In the diagram below, we show the various Arrow pieces that\\npower the architecture of VAST:\\n\\n![Arrow Data Plane](arrow-data-plane.light.png#gh-light-mode-only)\\n![Arrow Data Plane](arrow-data-plane.dark.png#gh-dark-mode-only)\\n\\nIn the center we have the Arrow data plane that powers other parts of the\\nsystem. Green elements highlight Arrow building blocks that we use today, and\\norange pieces elements we plan to use in the future. There are several aspects\\nworth pointing out:\\n\\n1. **Unified Data Plane**: When users ingest data into VAST, the\\n   parsing process converts the native data into Arrow. Similarly, a\\n   conversation boundary exists when data leaves the system, e.g., when a user\\n   wants a query result shown in JSON, CSV, or some custom format. Source and\\n   sink data formats are [exchangeable\\n   plugins](/docs/understand/architecture/plugins).\\n\\n2. **Read/Write Path Separation**: one design goal of VAST is a strict\\n   separation of read and write path, in order to scale them independently. The\\n   write path follows a horizontally scalable architecture where builders (one per\\n   schema) turn the in-memory record batches into a persistent representation.\\n   VAST currently has support for Parquet and Feather.\\n\\n3. **Pluggable Query Engine**: VAST has live/continuous queries that simply run\\n   over the stream of incoming data, and historical queries that operate on\\n   persistent data. The harboring execution engine is something we are about to\\n   make pluggable. The reason is that VAST runs in extremely different\\n   environments, from cluster to edge. Query engines are usually optimized for a\\n   specific use case, so why not use the best engine for the job at hand? Arrow\\n   makes this possible. [DuckDB][duckdb] and [DataFusion][datafusion] are great\\n   example of embeddable query engines.\\n\\n4. **Unified Control Plane**: to realize a pluggable query engine, we also need\\n   a standardized control plane. This is where [Substrait][substrait] and\\n   [Flight][flight] come into play. Flight for communication and Substrait as\\n   canonical query representation. We already experimented with Substrait,\\n   converting VAST queries into a logical query plan. In fact, VAST has a \\"query\\n   language\\" plugin to make it possible to translate security content. (For\\n   example, our [Sigma plugin][sigma-plugin] translates [Sigma rules][sigma]\\n   into VAST queries.) In short: Substrait is to the control plane what Arrow is\\n   to the data plane. Both are needed to modularize the concept of a query\\n   engine.\\n\\nMaking our own query engine more suitable for analytical workloads has\\nreceived less attention in the past, as we prioritized high-performance data\\nacquisition, low-latency search, in-stream matching using [Compute][compute],\\nand expressiveness of the underlying domain data model. We did so because VAST\\nmust run robustly in production on numerous appliances all over the world in a\\nsecurity service provider setting, with confined processing and storage where\\nefficiency is key.\\n\\nMoving forward, we are excited to bring more analytical horse power to the\\nsystem, while opening up the arena for third-party engines. With the bag of\\ntools from the Arrow ecosystem, plus all other embeddable Arrow engines that are\\nemerging, we have a modular architecture to can cover a very wide spectrum of\\nuse cases.\\n\\n[compute]: https://arrow.apache.org/docs/cpp/compute.html\\n[extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n[flight]: https://arrow.apache.org/docs/format/Flight.html\\n[substrait]: https://substrait.io/\\n[datafusion]: https://arrow.apache.org/datafusion/\\n[datafusion-c]: https://github.com/datafusion-contrib/datafusion-c\\n[msgpack]: https://msgpack.org/index.html\\n[duckdb]: https://duckdb.org/\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[sigma-plugin]: /docs/understand/query-language/frontends/sigma\\n[zeek]: https://zeek.org"},{"id":"/vast-v2.0","metadata":{"permalink":"/blog/vast-v2.0","source":"@site/blog/vast-v2.0/index.md","title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","date":"2022-05-16T00:00:00.000Z","formattedDate":"May 16, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"pcap","permalink":"/blog/tags/pcap"}],"readingTime":6.335,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","authors":"dominiklohmann","date":"2022-05-16T00:00:00.000Z","tags":["release","compaction","performance","pcap"]},"prevItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"},"nextItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"}},"content":"Dear community, we are excited to announce [VAST v2.0][github-vast-release],\\nbringing faster execution of bulk-submitted queries, improved tunability of\\nindex structures, and new configurability through environment variables.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.0.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Scheduling\\n\\nVAST is now more intelligent in how it schedules queries.\\n\\nWhen a query arrives at the VAST server, VAST first goes to the catalog which\\nreturns a set of on-disk candidate partitions that the query may be applicable\\nto. Previous versions of VAST simply iterated through the available queries as\\nthey came in, loading partition by partition to extract events. Due to memory\\nconstraints, VAST is only able to keep some partitions in memory, which causes\\nfrequent loading and unloading of the same partitions for queries that access\\nthe same data. Now, VAST loads partitions depending on how many queries they are\\nrelevant for and evaluates all ongoing queries for one partition at a time.\\n\\nAdditionally, VAST now partitions the data for each schema separately, moving\\naway from partitions that contain events of multiple schemas. This helps with\\ncommon access patterns and speeds up queries restricted to a single schema.\\n\\nThe numbers speak for themselves:\\n\\n![Benchmarks](scheduler-light.png#gh-light-mode-only)\\n![Benchmarks](scheduler-dark.png#gh-dark-mode-only)\\n\\n## Updates to Aging, Compaction, and the Disk Monitor\\n\\nVAST v1.0 deprecated the experimental aging feature. Given popular demand we\'ve\\ndecided to un-deprecate it and to actually implement it on top of the same\\nbuilding blocks the new compaction mechanism uses, which means that it is now\\nfully working and no longer considered experimental.\\n\\nThe compaction plugin is now able to apply general time-based compactions that\\nare not restricted to a specific set of types. This makes it possible for\\noperators to implement rules like \\"delete all data after 1 week\\", without having\\nto list all possible data types that may occur.\\n\\nSome smaller interface changes improve the observability of the compactor for\\noperators: The  `vast compaction status` command prints the current compaction\\nstatus, and the `vast compaction list` command now lists all configured\\ncompaction rules of the VAST node.\\n\\nAdditionally, we\'ve improved overall stability and fault tolerance improvements\\nsurrounding the disk monitor and compaction features.\\n\\n## Fine-tuned Catalog Configuration\\n\\n:::note Advanced Users\\nThis section is for advanced users only.\\n:::\\n\\nThe catalog manages partition metadata and is responsible for deciding whether a\\npartition qualifies for a certain query. It does so by maintaining sketch data\\nstructures (e.g., Bloom filters, summary statistics) for each partition.\\nSketches are highly space-efficient at the cost of being probabilistic and\\nyielding false positives.\\n\\nDue to this characteristic, sketches can grow sublinear: doubling the number of\\nevents in a sketch does not lead to a doubling of the memory requirement.\\nBecause the catalog must be traversed in full for a given query it needs to be\\nmaintained in active memory to provide high responsiveness.\\n\\nA false positive can have substantial impact on the query latency by\\nmaterializing irrelevant partitions, which involves unnecessary I/O. Based on\\nthe cost of I/O, this penalty may be substantial. Conversely, reducing the false\\npositive rate increases the memory consumption, leading to a higher resident set\\nsize and larger RAM requirements.\\n\\nYou can control this space-time trade-off in the configuration section\\n`vast.index` by specifying index rules. Each rule corresponds to one sketch and\\nconsists of the following components:\\n\\n`targets`: a list of extractors to describe the set of fields whose values to\\nadd to the sketch. `fp-rate`: an optional value to control the false-positive\\nrate of the sketch.\\n\\nVAST does not create field-level sketches unless a dedicated rule with a\\nmatching target configuration exists. Here\'s an example:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n      - targets:\\n          # field synopses: need to specify fully qualified field name\\n          - suricata.http.http.url\\n        fp-rate: 0.005\\n      - targets:\\n          - :addr\\n        fp-rate: 0.1\\n```\\n\\nThis configuration includes two rules (= two sketches), where the first rule\\nincludes a field extractor and the second a type extractor. The first rule\\napplies to a single field, `suricata.http.http.url`, and has a false-positive\\nrate of 0.5%. The second rule creates one sketch for all fields of type `addr`\\nthat has a false-positive rate of 10%.\\n\\n## Configuring VAST with Environment Variables\\n\\nVAST now offers an additional configuration path besides editing YAML\\nconfiguration files and providing command line arguments: *setting environment\\nvariables*. This enables a convenient configuration experience when using\\ncontainer runtimes, such as Docker, where the other two configuration paths have\\na mediocre UX at best:\\n\\nThe container entry point is limited to adding command line arguments, where not\\nall options may be set. For Docker Compose and Kubernetes, it is often not\\ntrivially possible to even add command line arguments.\\n\\nProviding a manual configuration file is a heavy-weight action, because it\\nrequires (1) generating a potentially templated configuration file, and (2)\\nmounting that file into a location where VAST would read it.\\n\\nAn environment variable has the form `KEY=VALUE`. VAST processes only\\nenvironment variables having the form `VAST_{KEY}=VALUE`. For example,\\n`VAST_ENDPOINT=1.2.3.4` translates to the command line option\\n`--endpoint=1.2.3.4` and YAML configuration `vast.endpoint: 1.2.3.4`.\\n\\nRegarding precedence, environment variables override configuration file\\nsettings, and command line arguments override environment variables. Please\\nconsult the [documentation](/docs/setup/configure#environment-variables)\\nfor a more detailed explanation of how to specify keys and values.\\n\\n## VLAN Tag Extraction and Better Packet Decapsulation\\n\\nVAST now extracts [802.1Q VLAN tags](https://en.wikipedia.org/wiki/IEEE_802.1Q)\\nfrom packets, making it possible to filter packets based on VLAN ID. The packet\\nschema includes a new nested record `vlan` with two fields: `outer` and `inner`\\nto represent the respective VLAN ID. For example, you can generate PCAP traces\\nof packets based on VLAN IDs as follows:\\n\\n```bash\\nvast export pcap \'vlan.outer > 0 || vlan.inner in [1, 2, 3]\' | tcpdump -r - -nl\\n```\\n\\nVLAN tags occur in many variations, and VAST extracts them in case of\\nsingle-tagging and  [QinQ\\ndouble-tagging](https://en.wikipedia.org/wiki/IEEE_802.1ad). Consult the [PCAP\\ndocumentation](/docs/use/ingest#pcap) for details on this feature.\\n\\nInternally, the packet decapsulation logic has been rewritten to follow a\\nlayered approach: frames, packets, and segments are the building blocks. The\\nplan is to reuse this architecture when switching to kernel-bypass packet\\nacquisition using DPDK. If you would like to see more work on the front of\\nhigh-performance packet recording, please reach out.\\n\\n## Breaking Changes\\n\\nThe `--verbosity` command-line option is now called `--console-verbosity`. The\\nshorthand options `-v`, `-vv`, `-vvv`, `-q`, `-qq`, and  `-qqq`  are unchanged.\\nThis aligns the command-line option with the configuration option\\n`vast.console-verbosity`, and disambiguates from the `vast.file-verbosity`\\noption.\\n\\nThe _Meta Index_ is now called the _Catalog_. This affects multiple status and\\nmetrics keys. We plan to extend the functionality of the Catalog in a future\\nrelease, turning it into a more powerful first instance for lookups.\\n\\nTransform steps that add or modify columns now add or modify the columns\\nin-place rather than at the end, preserving the nesting structure of the\\noriginal data.\\n\\n## Changes for Developers\\n\\nThe `vast get` command no longer exists. The command allowed for retrieving\\nevents by their internal unique ID, which we are looking to remove entirely in\\nthe future.\\n\\nChanges to the internal data representation of VAST require all transform step\\nplugins to be updated. The output format of the vast export arrow command\\nchanged for the address, subnet, pattern, and enumeration types, which are now\\nmodeled as [Arrow Extension\\nTypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types). The\\nrecord type is no longer flattened. The mapping of VAST types to Apache Arrow\\ndata types  is now considered stable.\\n\\n## Smaller Things\\n\\n- VAST client commands now start much faster and use less memory.\\n- The `vast count --estimate \'<query>\'` feature no longer unnecessarily causes\\n  stores to load from disk, resulting in major speedups for larger databases and\\n  broad queries.\\n- The [tenzir/vast](https://github.com/tenzir/vast) repository now contains\\n  experimental Terraform scripts for deploying VAST to AWS Fargate and Lambda."},{"id":"/vast-v1.1.2","metadata":{"permalink":"/blog/vast-v1.1.2","source":"@site/blog/vast-v1.1.2/index.md","title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","date":"2022-03-29T00:00:00.000Z","formattedDate":"March 29, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.33,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","authors":"lava","date":"2022-03-29T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"},"nextItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"}},"content":"Dear community, we are happy to announce the release of [VAST\\nv1.1.2](https://github.com/tenzir/vast/releases/tag/v1.1.2), the latest release\\non the VAST v1.1 series. This release contains a fix for a race condition that\\ncould lead to VAST eventually becoming unresponsive to queries in large\\ndeployments.\\n\\n\x3c!--truncate--\x3e\\n\\nFixed a race condition that would cause queries to become stuck when an exporter\\nwould time out during the meta index lookup.\\n[#2165](https://github.com/tenzir/vast/pull/2165)"},{"id":"/vast-v1.1.1","metadata":{"permalink":"/blog/vast-v1.1.1","source":"@site/blog/vast-v1.1.1/index.md","title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","date":"2022-03-25T00:00:00.000Z","formattedDate":"March 25, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.635,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-25T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"},"nextItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"Dear community, we are excited to announce [VAST\\nv1.1.1][github-vast-release-new].\\n\\nThis release contains some important bug fixes on top of everything included in\\nthe [VAST v1.1][github-vast-release-old] release.\\n\\n[github-vast-release-new]: https://github.com/tenzir/vast/releases/tag/v1.1.1\\n[github-vast-release-old]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n- The disk monitor now correctly continues deleting until below the low water\\n  mark after a partition failed to delete.\\n- We fixed a rarely occurring race condition that caused query workers to become\\n  stuck after delivering all results until the corresponding client process\\n  terminated.\\n- Queries that timed out or were externally terminated while in the query\\n  backlog that had more unhandled candidate than taste partitions no longer\\n  permanently get stuck. This critical bug caused VAST to idle permanently on\\n  the export path once all workers were stuck.\\n\\nThanks to [@norg](https://github.com/norg) for reporting the issues."},{"id":"/vast-v1.1","metadata":{"permalink":"/blog/vast-v1.1","source":"@site/blog/vast-v1.1/index.md","title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":5.985,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-03T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"},"nextItem":{"title":"VAST v1.0","permalink":"/blog/vast-v1.0"}},"content":"Dear community, we are excited to announce [VAST v1.1][github-vast-release],\\nwhich ships with exciting new features: *query language plugins* to exchange the\\nquery expression frontend, and *compaction* as a mechanism for expressing\\nfine-grained data retention policies and gradually aging out data instead of\\nsimply deleting it.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Language Plugins\\n\\nVAST features [a new query language plugin\\ntype](https://vast.io/docs/understand/architecture/plugins#query-language)\\nthat makes it possible to exchange the querying frontend, that is, replace the\\nlanguage in which the user writes queries. This makes it easier to integrate\\nVAST into specific domains without compromising the policy-neutral system core.\\n\\nThe first instance of the query language plugin is the [`sigma`\\nplugin](https://github.com/tenzir/vast/tree/master/plugins/sigma), which make it\\npossible to pass [Sigma\\nrules](https://vast.io/docs/understand/query-language/frontends/sigma) as\\ninput instead of a standard VAST query expression. Prior to this plugin, VAST\\nattempted to parse a query as Sigma rule first, and if that failed, tried to\\nparse it as a VAST expression. The behavior changed in that VAST now always\\ntries to interpret user input as VAST expression, and if that fails, goes\\nthrough all other loaded query language plugins.\\n\\nMoving forward, we will make it easier for integrators to BYO query language and\\nleverage VAST as an execution engine. We have already\\n[experimented](https://github.com/tenzir/vast/pull/2075) with\\n[Substrait](https://substrait.io), a cross-language protobuf spec for query\\nplans. The vision is that users can easily connect *any* query language that\\ncompiles into Substrait, and VAST takes the query plan as binary substrait blob.\\nSubstrait is still a very young project, but if the Arrow integration starts to\\nmature, it has the potential to enable very powerful types of queries without\\nmuch heavy lifting on our end. We already use the Arrow Compute API to implement\\ngeneric grouping and aggregation during compaction, which allows us to avoid\\nhand-roll and optimize compute kernels for standard functions.\\n\\n## Compaction Plugin\\n\\nCompaction is a feature to perform fine-grained transformation of historical\\ndata to manage a fixed storage budget. This gives operators full control over\\nshrinking data gradually\u2014both from a temporal and spatial angle:\\n\\n**Spatial**: Traditionally, reaching a storage budget triggers deletion of the\\noldest (or least-recently-used) data. This is a binary decision to throw away a\\nsubset of events. It does not differentiate the utility of data within an event.\\nWhat if you could only throw away the irrelevant parts and keep the information\\nthat might still be useful for longitudinal investigations? What if you could\\naggregate multiple events into a single one that captures valuable information?\\nImagine, for example, halving the space utilization of events with network flow\\ninformation and keeping them 6 months longer; or imagine you could roll up a set\\nof flows into a traffic matrix that only captures who communicated with whom in\\na given timeframe.\\n\\nBy incrementally elevating data into more space-efficient representations,\\ncompaction gives you a much more powerful mechanism to achieve long retention\\nperiods while working with high-volume telemetry.\\n\\n**Temporal**: data residency regulations often come with compliance policies\\nwith maximum retention periods, e.g., data containing personal data. For\\nexample, a policy may dictate a maximum retention of 1 week for events\\ncontaining URIs and 3 months for events containing IP addresses related to\\nnetwork connections. However, these retention windows could be broadened when\\npseudonomyzing or anonymizing the relevant fields.\\n\\nCompaction has a policy-based approach to specify these temporal constraints in\\na clear, declarative fashion.\\n\\nCompaction supersedes both the disk monitor and aging, being able to cover the\\nentire functionality of their behaviors in a more configurable way. The disk\\nmonitor remains unchanged and the experimental aging feature is deprecated (see\\nbelow).\\n\\n## Updates to Transform Steps\\n\\n### Aggregate Step\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Additionally, we renamed the `aggregate` operator to\\n[`summarize`][summarize]. Please keep this in mind when reading the example\\nbelow and consult the\\n[documentation](/docs/understand/query-language/pipelines) for the\\nup-to-date syntax.\\n[summarize]: /docs/understand/query-language/operators/summarize\\n:::\\n\\nThe new `aggregate` transform step plugin allows for reducing data with an\\naggregation operation over a group of columns.\\n\\nAggregation is a two-step process of first bucketing data in groups of values,\\nand then executing an aggregation function that computes a single value over the\\nbucket. The functionality is in line with what standard execution engines offer\\nvia \\"group-by\\" and \\"aggregate\\".\\n\\nBased on how the transformation is invoked in VAST, the boundary for determining\\nwhat goes into a grouping can be a table slice (e.g., during import/export) or\\nan entire partition (during compaction).\\n\\nHow this works is best shown on example data. Consider the following events\\nrepresenting flow data that contain a source IP address, a start and end\\ntimestamp, the number of bytes per flow, a boolean flag whether there is an\\nassociated alert, and a unique identifier.\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 87122, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": false, \\"unique_id\\": 1}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2022-02-22T10:36:43\\", \\"end\\": \\"2022-02-22T10:36:48\\", \\"alerted\\": false, \\"unique_id\\": 2}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 640, \\"start\\": \\"2022-02-22T10:36:46\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": true, \\"unique_id\\": 3}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 2162, \\"start\\": \\"2022-02-22T10:36:49\\", \\"end\\": \\"2022-02-22T10:36:51\\", \\"alerted\\": false, \\"unique_id\\": 4}\\n```\\n\\nWe can now configure a transformation that groups the events by their source IP\\naddress, takes the sum of the number of bytes, the minimum of the start\\ntimestamp, the maximum of the end timestamp, and the disjunction of the alerted\\nflag. Since the unique identifier cannot be aggregated in a meaningful manner,\\nit  is discarded.\\n\\n```yaml\\nvast:\\n  transforms:\\n    example-aggregation:\\n      - aggregate:\\n          group-by:\\n            - source_ip\\n          sum:\\n            - num_bytes\\n          min:\\n            - start\\n          max:\\n            - end\\n          any:\\n            - alerted\\n```\\n\\nAfter applying the transform, the resulting events will look like this:\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 89924, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-02T10:36:51\\", \\"alerted\\": true}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2020-11-06T10:36:43\\", \\"end\\": \\"2020-02-22T10:36:48\\", \\"alerted\\": false}\\n```\\n\\nUnlike the built-in transform steps, `aggregate` is a separate open-source\\nplugin that needs to be manually enabled in your `vast.yaml` configuration to be\\nusable:\\n\\n```yaml\\nvast:\\n  plugins:\\n    - aggregate\\n```\\n\\n### Rename Step\\n\\nThe new `rename` transform step is a built-in that allows for changing the name\\nof the schema of data. This is particularly useful when a transformation changes\\nthe shape of the data. E.g., an aggregated `suricata.flow` should likely be\\nrenamed because it is of a different layout.\\n\\nThis is how you configure the transform step:\\n\\n```yaml\\nrename:\\n  layout-names:\\n    - from: suricata.flow\\n      to: suricata.aggregated_flow\\n```\\n\\n### Project and Select Steps\\n\\nThe built-in `project` and `select` transform steps now drop table slices where\\nno columns and rows match the configuration respectively instead of leaving the\\ndata untouched.\\n\\n## Deprecations\\n\\nThe `msgpack` encoding no longer exists. As we integrate deeper with Apache\\nArrow, the `arrow` encoding is now the only option. Configuration options for\\n`msgpack` will be removed in an upcoming major release. On startup, VAST now\\nwarns if any of the deprecated options are in use.\\n\\nVAST\u2019s *aging* feature never made it out of the experimental stage: it only\\nerased data without updating the index correctly, leading to unnecessary lookups\\ndue to overly large candidate sets and miscounts in the statistics. Because\\ntime-based compaction is a superset of the aging functionality (that also\\nupdates the index correctly), we will remove aging in a future release. VAST now\\nwarns on startup if it\u2019s configured to run aging."},{"id":"/vast-v1.0","metadata":{"permalink":"/blog/vast-v1.0","source":"@site/blog/vast-v1.0/index.md","title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"transforms","permalink":"/blog/tags/transforms"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":3.195,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","authors":"dominiklohmann","date":"2022-01-27T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","transforms","query"]},"prevItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"We are happy to announce [VAST v1.0][github-vast-release]!\\n\\nThis release brings a new approach to software versioning for Tenzir. We laid\\nout the semantics in detail in a new [VERSIONING][github-versioning-md]\\ndocument.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.0.0\\n[github-versioning-md]: https://github.com/tenzir/vast/blob/v1.0.0/VERSIONING.md\\n\\n\x3c!--truncate--\x3e\\n\\n## Query events based on their import time\\n\\nThe new [`#import_time` extractor][docs-meta-extractor] allows for exporting\\nevents based on the time they arrived at VAST. Most of the time, this timestamp\\nis not far away from the timestamp of when the event occurred, but in certain\\ncases the two may deviate substantially, e.g., when ingesting historical events\\nfrom several years ago.\\n\\nFor example, to export all Suricata alerts that arrived at VAST on New Years Eve\\nas JSON, run this command:\\n\\n```bash\\nvast export json \'#type == \\"suricata.alert\\" && #import_time >= 2021-12-31 && #import_time < 2022-01-01\'\\n```\\n\\nThis differs from the [`:timestamp` type extractor][docs-type-extractor] that\\nqueries all events that contain a type `timestamp`, which is an alias for the\\n`time` type.  By convention, the `timestamp` type represents the event time\\nembedded in the data itself. However, the import time  is not part of the event\\ndata itself, but rather part of metadata of every batch of events that VAST\\ncreates.\\n\\n[docs-meta-extractor]: https://vast.io/docs/understand/query-language/expressions#meta-extractor\\n[docs-type-extractor]: https://vast.io/docs/understand/query-language/expressions#type-extractor\\n\\n## Omit `null` fields in the JSON export\\n\\nVAST renders all fields defined in the schema when exporting events as JSON. A\\ncommon option for many tools that handle JSON is to skip rendering `null`\\nfields, and the new `--omit-nulls` option to the JSON export does exactly that.\\n\\nTo use it on a case-by-case basis, add this flag to any JSON export.\\n\\n```bash\\nvast export json --omit-nulls \'<query>\'\\n\\n# This also works when attaching to a matcher.\\nvast matcher attach json --omit-nulls <matcher>\\n```\\n\\nTo always enable it, add this to your `vast.yaml` configuration file:\\n\\n```yaml\\nvast:\\n  import:\\n    omit-nulls: true\\n```\\n\\n## Selection and Projection Transform Steps\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Please keep this in mind when reading the example below and consult the\\n[documentation](/docs/understand/query-language/pipelines) for the\\nup-to-date syntax.\\n:::\\n\\nReshaping data during import and export is a common use case that VAST now\\nsupports. The two new built-in transform steps allow for filtering columns and\\nrows. Filtering columns (*projection*) takes a list of column names as input,\\nand filtering rows (*selection*)  works with an arbitrary query expression.\\n\\nHere\u2019s a usage example that sanitizes data leaving VAST during a query. If any\\nstring field in an event contains the value `tenzir` or `secret-username`, VAST\\nwill not include the event in the result set. The example below applies this\\nsanitization only to the events  `suricata.dns` and `suricata.http`, as defined\\nin the section `transform-triggers`.\\n\\n```yaml\\nvast:\\n  # Specify and name our transforms, each of which are a list of configured\\n  # transform steps. Transform steps are plugins, enabling users to write more\\n  # complex transformations in native code using C++ and Apache Arrow.\\n  transforms:\\n     # Prevent events with certain strings to be exported, e.g., \\"tenzir\\" or\\n     # \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each transform at server- or client-side, on\\n  # import or export, and restrict them to a list of event types.\\n  transform-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - transform: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Threat Bus 2022.01.27\\n\\nThanks to a contribution from Sascha Steinbiss\\n([@satta](https://github.com/satta)), Threat Bus only reports failure when\\ntransforming a sighting context if the return code of the transforming program\\nindicates failure.\\n\\nA small peek behind the curtain: We\u2019re building the next generation of Threat\\nBus as part of VAST. We will continue to develop and maintain Threat Bus and its\\napps for the time being.\\n\\nThreat Bus 2022.01.27 is available [\ud83d\udc49\\nhere](https://github.com/tenzir/threatbus/releases/tag/2022.01.27)."}]}')}}]);