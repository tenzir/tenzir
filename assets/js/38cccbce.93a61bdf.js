"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[63719],{20709:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"tenzir-node-v4.32","metadata":{"permalink":"/releases/tenzir-node-v4.32","source":"@site/releases/tenzir-node-v4.32/index.md","title":"Tenzir Node v4.32: Google SecOps","description":"Tenzir Node v4.32 features a new Google SecOps sink operator","date":"2025-04-04T00:00:00.000Z","formattedDate":"April 4, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":2.72,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir Node v4.32: Google SecOps","slug":"tenzir-node-v4.32","authors":["IyeOnline"],"date":"2025-04-04T00:00:00.000Z","tags":["release","node"],"comments":true},"nextItem":{"title":"Tenzir Node v4.31: OpenSearch Ingestion and if-expressions","permalink":"/releases/tenzir-node-v4.31"}},"content":"[Tenzir Node v4.32][github-release] features a new Google SecOps sink operator\\nand improvements to accessing structured types in TQL.\\n\\n![Tenzir Node v4.32](tenzir-node-v4.32.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.32.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Google SecOps Integration\\n\\n[operator-docs]: /next/tql2/operators/to_google_secops\\n\\nThe [`to_google_secops`][operator-docs] operator makes it possible to send events to Google SecOps:\\n\\n```tql\\nfrom {log: \\"31-Mar-2025 01:35:02.187 client 0.0.0.0#4238: query: tenzir.com IN A + (255.255.255.255)\\"}\\nto_google_secops \\\\\\n  customer_id=\\"00000000-0000-0000-00000000000000000\\",\\n  private_key=secret(\\"my_secops_key\\"),\\n  client_email=\\"somebody@example.com\\",\\n  log_text=log,\\n  log_type=\\"BIND_DNS\\",\\n  region=\\"europe\\"\\n```\\n\\n## Convenient Structure Access\\n\\n### Lenient Field Access using `.?`\\n\\nThe `.?` operator is a new alternative to the `.` operator that allows field\\naccess without warnings when the field does not exist or the parent record is\\n`null`.\\n\\n```tql title=\\"Different forms of record access\\"\\nfrom { }\\nelement1 = this.key                    // Raises a warning\\nelement2 = this.?key                   // Does not raise a warning\\nelement3 = this.key if this.has(\\"key\\") // Equivalent to the above\\n```\\n```tql\\n{element1: null, element2: null, element3: null}\\n```\\n\\n### `record.get()` and `list.get()`\\n\\nThe `get` method on records or lists is an alternative to index expressions that\\nallows for specifying a default value when the list index is out of bounds or\\nthe record field is missing.\\n\\n```tql title=\\"Get the first element of a list, or a fallback value\\"\\nfrom (\\n  {xs: [1, 2, 3]},\\n  {xs: []},\\n}\\nfirst = xs.get(0, -1)\\n```\\n\\n```tql\\n{first: 1}\\n{first: -1}\\n```\\n\\n```tql title=\\"Access a field of a record, or a fallback value\\"\\nfrom (\\n  {x: 1, y: 2},\\n  {x: 3},\\n}\\nx = x.get(\\"x\\", -1)\\ny = y.get(\\"y\\", -1)\\n```\\n\\n```tql\\n{x: 1, y: 2}\\n{x: 3, y: -1}\\n```\\n\\n#### Graceful Mappings\\n\\nThe new `get` function allows you to write mappings that gracefully handle\\nfailures without raising (potentially multiple) warnings.\\n\\n```tql title=\\"Map country tags to country names\\"\\nlet $country_tag_to_name = {\\n  ger: \\"Germany\\",\\n  fra: \\"France\\",\\n  ind: \\"India\\"\\n}\\nlet $fallback = \\"unknown\\"\\n\\nfrom (\\n  {country_tag: \\"ger\\"},\\n  {country_tag: \\"ind\\"},\\n  {country_tag: \\"ita\\"}\\n)\\ncountry = $country_tag_to_name.get(country_tag, $fallback)\\n```\\n```tql\\n{country_tag: \\"ger\\", country: \\"Germany\\"}\\n{country_tag: \\"ind\\", country: \\"India\\"}\\n{country_tag: \\"ita\\", country: \\"unknown\\"}\\n```\\n\\n### Indexing records with integers\\n\\nIndexing expressions on records now support numeric indices to access record\\nfields. For example, `this[0]` returns the first field of the top-level record.\\n\\n### More powerful `has`\\n\\nThe `has` method on records no longer requires the field name to be a constant.\\n\\n```tql\\nfrom {x: \\"hello\\", fieldname: \\"x\\"}\\nr = x.has(fieldname)\\n```\\n```tql\\n{x: \\"hello\\", fieldname: \\"x\\", r: true}\\n```\\n\\n## New Way to Access the Config\\n\\nThe `config` function replaces the previous `config` operator as a more flexible\\nmechanism to access variables from the configuration file.\\n\\n```tql title=\\"Get all user defined operators\\"\\nfrom config()\\nthis = tenzir.operators\\nunroll this\\n```\\n\\n:::info `config` operator has been removed\\nIf you rely on the old operator, you can use `from config()` as a replacement.\\n:::\\n\\n## Fixes, Improvements & Other Small Changes\\n\\nThis release also contains a number of small fixes and improvements, which you\\ncan find in the [changelog][changelog].\\n\\n## Let\'s Connect\\n\\nDo you want to directly engage with Tenzir? Join our [Discord server][discord],\\nwhere we discuss projects and features and host our bi-weekly office hours\\n(every second Tuesday at 5 PM CET). Regardless of whether you just want to hang\\nout or have that one very specific question you just need answered, you are always\\nwelcome!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4310"},{"id":"tenzir-node-v4.31","metadata":{"permalink":"/releases/tenzir-node-v4.31","source":"@site/releases/tenzir-node-v4.31/index.md","title":"Tenzir Node v4.31: OpenSearch Ingestion and if-expressions","description":"Tenzir Node v4.31 is now available, with several new features","date":"2025-03-31T00:00:00.000Z","formattedDate":"March 31, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":2.24,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"},{"name":"Raghav Narang","title":"Software Engineer","url":"https://github.com/raxyte","email":"raghav@tenzir.com","imageURL":"https://github.com/raxyte.png","key":"raxyte"}],"frontMatter":{"title":"Tenzir Node v4.31: OpenSearch Ingestion and if-expressions","slug":"tenzir-node-v4.31","authors":["IyeOnline","raxyte"],"date":"2025-03-31T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Node v4.32: Google SecOps","permalink":"/releases/tenzir-node-v4.32"},"nextItem":{"title":"Tenzir Node v4.30: ClickHouse Integration","permalink":"/releases/tenzir-node-v4.30"}},"content":"[Tenzir Node v4.31][github-release] is now available, with several new features\\nincluding OpenSearch emulation for broader data ingestion, flexible if/else\\nexpressions in TQL, and a new operator for writing Syslog messages.\\n\\n![Tenzir Node v4.31](tenzir-node-v4.31.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.31.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Emulating OpenSearch\\n\\nTenzir can now receive data from a variety of tools such as\\n[Beats](https://www.elastic.co/beats) using their OpenSearch/Elasticsearch\\noutputs via the new [`from_opensearch`](/next/tql2/operators/from_opensearch)\\noperator.\\n\\nFor example, to receive events from [Filebeat](https://www.elastic.co/beats/filebeat):\\n\\nSetup an Elasticsearch sink for Filebeat:\\n\\n```yml title=\\"filebeat.yml\\"\\noutput.elasticsearch:\\n  hosts: [\\"<host to forward to>\\"]\\n```\\n\\nand listen on the host with:\\n\\n```tql\\nfrom_opensearch keep_actions=true\\n```\\n\\n```tql\\n{create:{_index:\\"filebeat-8.17.3\\"}}\\n{\\"@timestamp\\":2025-03-31T13:42:28.068Z,log:{offset:1,file:{path:\\"/mounted/logfile\\"}},message:\\"hello\\",input:{type:\\"log\\"},host:{name:\\"eb21\\"},agent:{id:\\"682cfcf4-f251-4576-abcb-6c8bcadfda08\\",name:\\"eb21\\",type:\\"filebeat\\",version:\\"8.17.3\\",ephemeral_id:\\"17f74f6e-36f0-4045-93e6-c549874716df\\"},ecs:{version:\\"8.0.0\\"}}\\n{create:{_index:\\"filebeat-8.17.3\\"}}\\n{\\"@timestamp\\":2025-03-31T13:42:28.068Z,log:{offset:7,file:{path:\\"/mounted/logfile\\"}},message:\\"this\\",input:{type:\\"log\\"},host:{name:\\"eb21\\"},agent:{id:\\"682cfcf4-f251-4576-abcb-6c8bcadfda08\\",name:\\"eb21\\",type:\\"filebeat\\",version:\\"8.17.3\\",ephemeral_id:\\"17f74f6e-36f0-4045-93e6-c549874716df\\"},ecs:{version:\\"8.0.0\\"}}\\n```\\n\\n## `if \u2026 else \u2026` expressions\\n\\nThis release also brings `if` and `else` expressions. They work similar to the\\nshort-hand form in Python.\\n\\nThe rules are very simple:\\n- `value if true` => `value`\\n- `value if false` => `null`\\n- `value if null` => `null` and warning\\n- `value else fallback` => `value`\\n- `null else fallback` => `null`\\n\\nThese can be combined:\\n- `value if true else fallback` => `value`\\n- `value if false else fallback` => `fallback`\\n- `value if null else fallback` => `fallback` and warning\\n\\nFor example, `if \u2026 else \u2026` combines nicely with the new `metrics \\"pipeline\\"`:\\n\\n```tql\\nmetrics \\"pipeline\\"\\nsummarize (\\n  pipeline_id,\\n  ingress=sum(ingress.bytes / ingress.duration.count_seconds() if not\\ningress.internal else 0.0),\\n  from_node=sum(ingress.bytes / ingress.duration.count_seconds() if\\ningress.internal else 0.0),\\n  egress=sum(egress.bytes / egress.duration.count_seconds() if not\\negress.internal else 0.0),\\n  to_node=sum(egress.bytes / egress.duration.count_seconds() if egress.internal\\nelse 0.0),\\n)\\n```\\n\\nThis returns an overview of the ingress and egress of all pipelines, but\\nadditionally separates out node-internal ingress and egress. The latter was not\\neasily possible before.\\n\\n:::tip Standalone `else`\\nNote that `else` can also be used standalone, and is effectively a superior\\nversion of the `otherwise` function. That is, `foo.otherwise(bar)` is now better\\nwritten as `foo else bar`.\\n:::\\n\\n## Writing Syslog Messages\\n\\nWith this release, we also introduce the `write_syslog` operator that converts\\nincoming events into [RFC 5424](https://datatracker.ietf.org/doc/html/rfc5424)\\nSyslog Messages.\\n\\n```tql title=\\"Example pipeline\\"\\nfrom {\\n  facility: 1,\\n  severity: 1,\\n  timestamp: now(),\\n  hostname: \\"localhost\\",\\n  structured_data: {\\n    origin: {\\n      key: \\"value\\",\\n    },\\n  },\\n  message: \\"Tenzir\\",\\n}\\nwrite_syslog\\n```\\n\\n```log\\n<9>1 2025-03-31T13:28:55.971210Z localhost - - - [origin key=\\"value\\"] Tenzir\\n```\\n\\n## Fixes, Improvements & Other Small Changes\\n\\nThis release also contains a number of small fixes and improvements, which you\\ncan find in the [changelog][changelog].\\n\\n## Let\'s Connect\\n\\nDo you want to directly engage with Tenzir? Join our [Discord server][discord],\\nwhere we discuss projects and features and host our bi-weekly office hours\\n(every second Tuesday at 5 PM CET). Regardless of whether you just want to hang\\nout or have that one very specific question you just need answered, you are always\\nwelcome!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4310"},{"id":"tenzir-node-v4.30","metadata":{"permalink":"/releases/tenzir-node-v4.30","source":"@site/releases/tenzir-node-v4.30/index.md","title":"Tenzir Node v4.30: ClickHouse Integration","description":"Tenzir Node v4.30 introduces a to_clickhouse","date":"2025-03-18T00:00:00.000Z","formattedDate":"March 18, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":2.31,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir Node v4.30: ClickHouse Integration","slug":"tenzir-node-v4.30","authors":["IyeOnline"],"date":"2025-03-18T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Node v4.31: OpenSearch Ingestion and if-expressions","permalink":"/releases/tenzir-node-v4.31"},"nextItem":{"title":"Tenzir Platform v1.9: Accelerated Data Exploration","permalink":"/releases/tenzir-platform-v1.9"}},"content":"[Tenzir Node v4.30][github-release] introduces a [`to_clickhouse`][operator-docs]\\noperator and streamlines the TLS settings for all operators.\\n\\n![Tenzir Node v4.30](tenzir-node-v.4.30.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.30.0\\n\\n[operator-docs]: /next/tql2/operators/to_clickhouse\\n[clickhouse-website]: https://clickhouse.com/\\n[clickhouse-cpp-lib]: https://github.com/ClickHouse/clickhouse-cpp\\n\\n\x3c!-- truncate --\x3e\\n\\n## ClickHouse Integration\\n\\nTenzir can now seamlessly write data to a [ClickHouse][clickhouse-website]\\ntable, using the new [`to_clickhouse`][operator-docs] operator. The operator\\nuses ClickHouse\'s C++ client library [clickhouse-cpp][clickhouse-cpp-lib]\\nto efficiently insert blocks in a columnar fashion. It supports all of Tenzir\'s\\ntypes and can both create tables as well as append to existing ones:\\n\\n```tql\\nfrom { i: 42, d: 10.0, b: true, l: [42], r:{ s:\\"string\\" } }\\nto_clickhouse table=\\"example\\", primary=i\\n```\\n\\n## Streamlined TLS Settings\\n\\nMany operators feature some TLS settings, that all worked very similar, but not\\nidentical. With v4.30, the TLS settings are now are now identical between all\\noperators and all share the same semantics.\\n\\nAdditionally, there is a new configuration option `tenzir.cacert`, which allows\\nsetting of a global default for the CA certificates file. The default value will\\nbe chosen appropriately for the system the node runs on. Of course the per-operator\\narguments still take precedence.\\n\\n```yaml title=\\"Set a cacert default in the config\\"\\n# /opt/tenzir/etc/tenzir/tenzir.yaml\\ntenzir:\\n  cacert: path/to/custom.bundle\\n```\\n\\n:::warning TLS by default\\nWith these changes we also enabled TLS by default on most operators. This may\\ncause some pipelines that rely on TLS not being enabled to stop working. If you\\nwant to explicitly disable TLS, you can still do so by providing `tls=false` to\\nthese operators.\\n:::\\n\\n## Pipeline Metrics\\n\\nTenzir previously only featured very detailed operator metrics, which provided\\na whole lot of detail, but were both cumbersome to work with and fairly expensive\\nto collect.\\n\\nThis release introduces [pipeline metrics](/next/tql2/operators/metrics#tenzirmetricspipeline),\\nwhich give you information about the throughput of entire pipelines. These metrics\\nare much easier to work with, for example to create Dashboards:\\n\\n```tql title=\\"Ingress of all Pipelines\\"\\nmetrics \\"pipeline\\"\\nchart_line x=timestamp, y={\\n  \\"Ingress Gb/s\\": mean(ingress.bytes / 1G / ingress.duration.count_seconds()),\\n}, resolution=10s, group=pipeline_id, x_min = now()-1d\\n```\\n\\n:::info Deprecation Notice\\nThe old Operator Metrics are now deprecated and will be removed in some future\\nrelease.\\n:::\\n\\n## Fixes & Updates\\n\\nThis release also contains a few small fixes. Most notably we enabled native\\nGoogle Cloud Storage support to TQL2 in the form of the [`load_gcs`][load_gcs]\\nand [`save_gcs`][save_gcs] operators. These are also supported using the `gs://`\\nURI scheme in the generic `from` and `to` operators.\\n\\n[load_gcs]: /next/tql2/operators/load_gcs\\n[save_gcs]: /next/tql2/operators/save_gcs\\n\\n## Let\'s Connect!\\n\\nJoin our [Discord server][discord], to ask questions, discuss features or just\\nhang out! Here we also host our bi-weekly office hours (every second Tuesday at 5 PM CET),\\nwhere we showcase the latest features, give sneak peaks into upcoming ones,\\nanswer your questions or just discuss general tech topics!\\nWe are looking forward to getting in touch with you!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4230"},{"id":"tenzir-platform-v1.9","metadata":{"permalink":"/releases/tenzir-platform-v1.9","source":"@site/releases/tenzir-platform-v1.9/index.md","title":"Tenzir Platform v1.9: Accelerated Data Exploration","description":"We\'re happy to announce Tenzir Platform v1.9, introducing the next generation of the Tenzir Platform\'s Explorer.","date":"2025-03-14T00:00:00.000Z","formattedDate":"March 14, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":2.815,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir Platform v1.9: Accelerated Data Exploration","slug":"tenzir-platform-v1.9","authors":["dominiklohmann"],"date":"2025-03-14T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Node v4.30: ClickHouse Integration","permalink":"/releases/tenzir-node-v4.30"},"nextItem":{"title":"Tenzir Node v4.29: Nested Printing","permalink":"/releases/tenzir-node-v4.29"}},"content":"We\'re happy to announce [Tenzir Platform v1.9][github-release], introducing the next generation of the Tenzir Platform\'s Explorer.\\n\\n![Tenzir Platform v1.9](tenzir-platform-v1.9.svg)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.9.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Idea Behind the Explorer\\n\\nThe Explorer has a very simple interface: Users can write a pipeline in TQL and\\nrun it. If the pipeline has a sink, the Explorer will ask the user to deploy\\ntheir pipeline. If the pipeline does not have a sink, results are displayed in a\\ndata table right below the editor.\\n\\nThis allows for rapid prototyping of pipelines: Start with just a source, hit\\nrun to see results. Add a transformation, hit _Run_ again and see what\'s\\nchanged. Add more transformations\u2014rinse and repeat. Once you\'re satisfied with\\nyour results, add a sink, hit _Run_ one last time, and permanently deploy your\\npipeline.\\n\\n## What Changed?\\n\\nBefore this release, the results table featured a _Load More_ button that\\nallowed you to load more results as needed. The schemas pane showed a summary of\\nalready loaded events.\\n\\nNow, the schemas pane automatically shows all results that a pipeline produces,\\nand the table loads events only for the selected schema. Additionally, the table\\nnow supports pagination, allowing users to navigate through large datasets\\nefficiently.\\n\\n## Wait, How?\\n\\nWe fundamentally revisited how the Explorer fetches results from pipelines\\nwithout a user-provided sink. Previously, the Explorer would suspend the\\npipeline after fetching the first batch of results, resuming it only when the\\nuser hit the _Load More_ button. Now, the Explorer loads all results into a\\ncache that lives at the node directly, displaying a summary of the cache in the\\nschemas pane and fetching individual pages from the cache as needed.\\n\\nThis causes additional memory usage on the node, but improves the performance\\nand user experience of the Explorer significantly.\\n\\n## Limiting Cache Sizes\\n\\nIn the Explorer, users can control how many results the cache can contain at\\nmost. This option defaults to 10,000 events, but can be adjusted up to 1,000,000\\nevents.\\n\\nTenzir Node v4.28 or newer support the `tenzir.cache.capacity` option that\\nlimits the total cache usage for the node to a specified amount of memory.\\n\\nIf the capacity is exceeded, the oldest cache entries are evicted from the node.\\n\\nAdditionally, the `tenzir.cache.lifetime` option controls how long a cache\\nremains valid after the pipeline has finished writing it. Afterwards, the cache\\nis evicted from the node.\\n\\nHere\'s how you can set the options:\\n\\n```yaml\\n# /opt/tenzir/etc/tenzir/tenzir.yaml\\ntenzir:\\n  cache:\\n    # Total cache capacity in bytes for the node. Must be at least 64Mi.\\n    # Defaults to 1Gi.\\n    capacity: 1Gi\\n    # Maximum lifetime of a cache. Defaults to 10min.\\n    lifetime: 10min\\n```\\n\\nThe options can also be specified as the `TENZIR_CACHE__CAPACITY` and\\n`TENZIR_CACHE__LIFETIME` environment variables, respectively.\\n\\n## Faster Downloads\\n\\nPreviously, the _Download_ button in the Explorer caused pipelines to run again.\\nThis is no longer necessary\u2014we can now just fetch the already cached results\\nagain, format them as needed, which makes downloading practically instant.\\n\\nThis even works when the pipeline is still running, as downloading will now just\\nfetch the cached results at the time when the download was initiated.\\n\\n## Join Us for Office Hours\\n\\nJoin us for our bi-weekly office hours every other Tuesday at 5 PM CET on our\\n[Discord server][discord]. It\'s a great opportunity to share your experiences,\\nask questions, and help shape the future of Tenzir with your valuable feedback!\\n\\n[discord]: /discord"},{"id":"tenzir-node-v4.29","metadata":{"permalink":"/releases/tenzir-node-v4.29","source":"@site/releases/tenzir-node-v4.29/index.md","title":"Tenzir Node v4.29: Nested Printing","description":"Tenzir Node v4.29 introduces new functions for printing values as strings,","date":"2025-02-25T00:00:00.000Z","formattedDate":"February 25, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":1.61,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir Node v4.29: Nested Printing","slug":"tenzir-node-v4.29","authors":["IyeOnline"],"date":"2025-02-25T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.9: Accelerated Data Exploration","permalink":"/releases/tenzir-platform-v1.9"},"nextItem":{"title":"Tenzir Node v4.28: Nested Parsing","permalink":"/releases/tenzir-node-v4.28"}},"content":"[Tenzir Node v4.29][github-release] introduces new functions for printing values as strings,\\ncomplementing v4.28\'s parsing functions.\\n\\n![Tenzir Node v4.29](tenzir-node-v4.29.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.29.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Printing Values\\n\\nWith this release, we\'ve added a bunch of new functions for printing values as\\nstrings:\\n\\n- `print_json` prints any value as its JSON representation.\\n- `print_yaml` prints any value as its YAML representation.\\n- `print_csv`, `print_ssv`, and `print_tsv` to print records as comma-, space-\\n  and tab-separated strings respectively.\\n- `print_kv` prints any value as a sequence of key-value pairs.\\n\\nThe new `print_*` functions behave similar to the `write_*` operators, except that\\nthey work on values within events instead of streams of events.\\n\\nIn addition, we also added a new operator, `write_kv`. It is the dual for our\\n`read_kv` operator and enables you to write events as key-value pairs.\\n\\n:::warning Pipelines May Require Updating\\nAlong with these new functions, we decided to streamline arguments for a lot of\\nreading & writing operators as well as the parsing function.\\nTheir _separator_ arguments are now all named arguments and the suffix `*_sep`\\nhas been replaced with a proper `*_separator`. Additionally, the `unflatten`\\noption is now called `unflatten_separator`. For more details, see the\\n[changelog][changelog].\\n:::\\n\\n## Timely Functions\\n\\nWe introduced new functions ranging from `years(number)` to `nanoseconds(number)`,\\nwhich allow you to convert any `number` to a `duration`. Their counterparts, such\\nas `count_years(duration)`, convert a `duration` back into plain count of the\\nrespective unit.\\n\\n## Fixes, Improvements & Other Small Changes\\n\\nThis release also contains a number of small fixes and improvements, which you\\ncan find in the [changelog][changelog].\\n\\n## Let\'s Connect!\\n\\nDo you want to directly engage with Tenzir? Join our [Discord server][discord],\\nwhere we discuss projects and features and host our bi-weekly office hours\\n(every second Tuesday at 5 PM CET). Regardless of whether you just want to hang\\nout or have that one very specific question you just need answered, you are always\\nwelcome!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4290"},{"id":"tenzir-node-v4.28","metadata":{"permalink":"/releases/tenzir-node-v4.28","source":"@site/releases/tenzir-node-v4.28/index.md","title":"Tenzir Node v4.28: Nested Parsing","description":"Tenzir Node v4.28 makes it easier than before to parse deeply nested data","date":"2025-02-10T00:00:00.000Z","formattedDate":"February 10, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":1.29,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir Node v4.28: Nested Parsing","slug":"tenzir-node-v4.28","authors":["dominiklohmann"],"date":"2025-02-10T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Node v4.29: Nested Printing","permalink":"/releases/tenzir-node-v4.29"},"nextItem":{"title":"Tenzir Node v4.27: Amazon MSK IAM Integration","permalink":"/releases/tenzir-node-v4.27"}},"content":"[Tenzir Node v4.28][github-release] makes it easier than before to parse deeply nested data\\nstructures.\\n\\n![Tenzir Node v4.28](tenzir-node-v4.28.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.28.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Parsing Nested Data Structures\\n\\nWith this release, we\'ve added a bunch of new functions for parsing nested data\\nfrom a string:\\n\\n- `parse_kv` parses key-value pairs.\\n- `parse_grok` parses Grok patterns.\\n- `parse_csv`, `parse_ssv`, and `parse_tsv` parse comma-, space-, and\\n  tab-separated values, respectively.\\n- `parse_leef` and `parse_cef` parse LEEF and CEF data, respectively.\\n- `parse_syslog` parses Syslog data.\\n- `parse_json` parses JSON values.\\n- `parse_yaml` parses YAML values.\\n\\n:::tip Read the Guide\\nTo learn more about parsing nested data structures, check out our new [guide on\\nparsing nested data](/next/usage/parse-nested-data).\\n:::\\n\\nThe new `parse_*` functions behave similar to the `read_*` operators, except\\nthat they work on one string field at a time instead of a stream of bytes in the\\npipeline.\\n\\n## VSCode TQL Package\\n\\nIf you\'re writing TQL in Visual Studio Code, you can now enjoy syntax\\nhighlighting just like in the Tenzir Platform: The VSCode TQL package is now\\navailable on the [Visual Studio Marketplace][vscode-tql].\\n\\n[vscode-tql]: https://marketplace.visualstudio.com/items?itemName=tenzir.vscode-tql\\n\\n## Let\'s Connect!\\n\\nWant to be part of something exciting? Our vibrant community is waiting for you!\\nDrop into our bi-weekly office hours (every second Tuesday at 5 PM CET) on\\n[Discord][discord] where ideas flow freely, sneak peeks of new features abound,\\nand conversations spark between Tenzir enthusiasts and our passionate team.\\nWhether you\'ve got burning questions, fascinating use cases to share, or just\\nwant to hang out\u2014our virtual door is always open!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4280"},{"id":"tenzir-node-v4.27","metadata":{"permalink":"/releases/tenzir-node-v4.27","source":"@site/releases/tenzir-node-v4.27/index.md","title":"Tenzir Node v4.27: Amazon MSK IAM Integration","description":"Tenzir Node v4.27 enhances the charting capabilities and integrates with IAM for","date":"2025-01-30T00:00:00.000Z","formattedDate":"January 30, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":3.09,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"},{"name":"Raghav Narang","title":"Software Engineer","url":"https://github.com/raxyte","email":"raghav@tenzir.com","imageURL":"https://github.com/raxyte.png","key":"raxyte"}],"frontMatter":{"title":"Tenzir Node v4.27: Amazon MSK IAM Integration","slug":"tenzir-node-v4.27","authors":["lava","raxyte"],"date":"2025-01-30T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Node v4.28: Nested Parsing","permalink":"/releases/tenzir-node-v4.28"},"nextItem":{"title":"Tenzir Platform v1.8: Charting the Unknown","permalink":"/releases/tenzir-platform-v1.8"}},"content":"[Tenzir Node v4.27][github-release] enhances the charting capabilities and integrates with IAM for\\nauthenticating to Amazon MSK.\\n\\n![Tenzir Node v4.27](tenzir-node-v4.27.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.27.0\\n\\n\x3c!-- truncate --\x3e\\n\\n:::warning TQL1 Deprecation\\nTQL1 pipelines are deprecated starting from this release and the node will warn\\non every execution of such pipelines. TQL2 is now in a much more mature state\\nand is the recommended way forward. [Read\\nmore.](https://docs.tenzir.com/tql2-migration)\\n:::\\n\\n## AWS IAM Authentication for MSK\\n\\n[Amazon Managed Streaming for Apache Kafka (Amazon\\nMSK)](https://aws.amazon.com/msk/) is a streaming data service that manages\\nApache Kafka infrastructure and operations, making it easier for developers and\\nDevOps managers to run Apache Kafka applications and Apache Kafka Connect\\nconnectors on AWS without becoming experts in operating Apache Kafka.\\n\\nServerless MSK instances currently only support IAM Authentication, which means\\nyou could not communicate with them using Tenzir. This unfortunate situation has\\nnow changed!\\n\\nWith this release, the `load_kafka` and `save_kafka` operators can now\\nauthenticate with MSK using AWS IAM by simply specifying the `aws_iam`\\noption with a record of configuration values such as:\\n\\n```tql\\nload_kafka \\"kafkaesque-data\\", aws_iam={region: \\"eu-west-1\\"}\\n```\\n\\nThe above pipeline will try to fetch credentials from [various different\\nlocations](/next/tql2/operators/load_kafka#aws_iam--record-optional) including\\nInstance Metadata Services. This means you can attach a role with the necessary\\npermissions directly to an EC2 instance and Tenzir will automatically pick it up.\\n\\n### Assuming roles\\n\\nRoles can also be assumed by giving the `assume_role` parameter to the `aws_iam` option.\\n\\n```tql\\nsave_kafka \\"topic\\", aws_iam={region: \\"eu-west-1\\", assume_role: \\"arn:aws:iam::1234567890:role/my-msk-role\\"}\\n```\\n\\nThe above pipeline attempts to fetch temporary credentials from Amazon STS for\\nthe given ARN.\\n\\n### Example\\n\\n#### Collecting High Severity OCSF events from MSK\\n\\n```tql\\nlet $endpoints = [\\"indexer-1-url\\", \\"indexer-2-url\\"]\\n\\nload_kafka \\"ocsf-events\\", aws_iam={region: \\"us-east-2\\", assume_role: \\"arn:aws:iam::1234567890:role/my-msk-role\\"}\\nread_json\\nwhere severity_id >= 4 // High and above\\nload_balance $endpoints {\\n    to_splunk $endpoints, hec_token=secret(\\"SPLUNK_TOKEN\\")\\n}\\n```\\n\\nThe above pipeline reads OCSF events from MSK, assuming the role referenced by\\nthe provided ARN. The incoming data is then filtered for severity and sent to\\nSplunk clusters in a load balanced fashion.\\n\\n## Charts, Retention and TLS\\n\\nThis release also includes a number of other notable features for the Tenzir Node.\\n\\n### Charts\\n\\nThis release brings over the family of familiar charting operators from TQL1\\nwith some new delightful features. The new operators allow you to group by\\ndifferent fields or choose a resolution for a time-series-like data and more!\\n\\nWe explore charting in more detail in our upcoming Tenzir Platform v1.8\\nrelease blog post, so stay tuned.\\n\\n### Retention\\n\\nTwo new settings `tenzir.retention.metrics` and `tenzir.retention.diagnostics`\\ncontrol the retention time of metrics and diagnostics.\\n\\nThese options indicate for how long to store metrics and diagnostics, respectively.\\nFor example, the following configuration stores metrics for 30 days and diagnostics\\nindefinitely:\\n\\n```yaml\\n# /opt/tenzir/etc/tenzir/tenzir.yaml\\ntenzir:\\n  retention:\\n    metrics: 30d\\n```\\n\\n### TLS\\n\\nWe\'ve added new options for establishing the connection to the Tenzir Platform\\nthat make it easier to use the Tenzir Node in self-hosted environments with\\nprivate certificate authorities.\\n\\nThe `tenzir.platform.cacert` option points to a file containing one or more\\nCA certificates that are used for validating the certificate presented by\\nthe platform.\\n\\nThe `tenzir.platform.skip-peer-verification` option can be enabled in order to\\nconnect to a Tenzir Platform instance that is using self-signed certificates.\\n\\n```sh\\nTENZIR_PLATFORM__CACERT=/path/to/certificates.crt\\nTENZIR_PLATFORM__SKIP_PEER_VERIFICATION=true/false\\n```\\n\\nNote that these settings only apply to the connection made from\\nthe Tenzir Node to the Tenzir Platform on startup, and not to\\nany outgoing HTTP connections made by individual pipelines.\\n\\n## Let\'s Connect!\\n\\nWe\u2019re excited to engage with our community!\\nJoin us every second Tuesday at 5 PM CET for office hours on [Discord][discord].\\nShare your ideas, preview upcoming features, or chat with fellow Tenzir users\\nand our team. Bring your questions, use cases, or just stop by to say hello!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4270"},{"id":"tenzir-platform-v1.8","metadata":{"permalink":"/releases/tenzir-platform-v1.8","source":"@site/releases/tenzir-platform-v1.8/index.md","title":"Tenzir Platform v1.8: Charting the Unknown","description":"We\'re happy to announce Tenzir Platform v1.8, with","date":"2025-01-30T00:00:00.000Z","formattedDate":"January 30, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":2.17,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"},{"name":"Raghav Narang","title":"Software Engineer","url":"https://github.com/raxyte","email":"raghav@tenzir.com","imageURL":"https://github.com/raxyte.png","key":"raxyte"}],"frontMatter":{"title":"Tenzir Platform v1.8: Charting the Unknown","slug":"tenzir-platform-v1.8","authors":["lava","raxyte"],"date":"2025-01-30T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Node v4.27: Amazon MSK IAM Integration","permalink":"/releases/tenzir-node-v4.27"},"nextItem":{"title":"Tenzir Node v4.26: Amazon Security Lake Integration","permalink":"/releases/tenzir-node-v4.26"}},"content":"We\'re happy to announce [Tenzir Platform v1.8][github-release], with\\nnew and improved charting as well as a new single-user mode for\\nSovereign Edition users.\\n\\n![Tenzir Platform v1.8](tenzir-platform-1.8.png)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.8.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Charting\\n\\n[Tenzir Node v4.27.0](/releases/tenzir-node-v4.27) introduced four new charting\\noperators for TQL2 -- `chart_line`, `chart_area`, `chart_bar` and `chart_pie`.\\n\\nThese new operators offer easy ways to aggregate, pivot, bucket and visualize\\nyour data. Combine these with [metrics](/tql2/operators/metrics) from the\\nTenzir Node and you have a powerful way to digest information about Node health\\nand more.\\n\\n### Examples\\n\\n#### Visualizing CPU load\\n\\nThe following pipeline uses an area chart to display the maximum and mean CPU\\nLoad for every 30 minute interval over the last 5 days, utilizing information\\nfrom [CPU metrics](/tql2/operators/metrics#tenzirmetricscpu) emitted by the\\nTenzir Node:\\n\\n```tql\\nmetrics \\"cpu\\"\\nchart_area x=timestamp, y={\\"Avg. Load\\": mean(loadavg_1m), \\"Max. Load\\": max(loadavg_1m)},\\n          resolution=30min,\\n          x_min=now()-5d,\\n          y_min=4,\\n          y_max=10\\n```\\n\\nNote the usage of `y_min` and `y_max` options to keep a stable view when the\\nload might vary a lot.\\n\\n![Node Load](node-load.png)\\n\\n#### Visualizing schema counts\\n\\nThe following pipeline shows a pie chart of all events stored in the internal\\ndatabase of the Tenzir Node. Each slice is labeled with the schema name and its\\nsize is proportional to the count of events stored with that schema.\\n\\n```tql\\nmetrics \\"import\\"\\nchart_pie label=schema, value=sum(events)\\n```\\n\\n![Schema counts](schema-pie.png)\\n\\n### Dashboards\\n\\nCharts can also be added to your [Tenzir\\ndashboards](https://app.tenzir.com/dashboards) to be readily viewed at any time.\\n\\nTo do so, click the \\"Add to Dashboard\\" button in the results pane and assign\\na name to the chart. Afterwards, it will be visible in Dashboards tab\\nof the Tenzir Platform.\\n\\n![Add to dashboard](add-to-dashboard.png)\\n\\n## Single-User Mode\\n\\nWe added a new, simplified example setup for users of the Sovereign Edition\\nthat gets rid of all manual setup and just has a single default user that\\nis automatically signed in.\\n\\nTo try it out, just start a docker compose stack in the `examples/localdev`\\nfolder of your platform checkout.\\n\\n```sh\\ngit clone https://github.com/tenzir/platform\\ncd platform/examples/localdev\\ndocker compose up\\n```\\n\\n## Bug fixes\\n\\nSome other noteworthy changes in this release include:\\n\\n- For Sovereign Edition users, the `platform` and `websocket-gateway`\\n  containers now support the environment variables `TLS_CERTFILE`,\\n  `TLS_KEYFILE` and `TLS_CAFILE` that can be used to enable native TLS\\n  support.\\n- Disabled TQL autocomplete in the editor.\\n- Label Filters are now fully clickable.\\n\\n## Join Us for Office Hours\\n\\nJoin us for our bi-weekly office hours every other Tuesday at 5 PM CET on our\\n[Discord server][discord]. It\'s a great opportunity to share your experiences,\\nask questions, and help shape the future of Tenzir with your valuable feedback!\\n\\n[discord]: /discord"},{"id":"tenzir-node-v4.26","metadata":{"permalink":"/releases/tenzir-node-v4.26","source":"@site/releases/tenzir-node-v4.26/index.md","title":"Tenzir Node v4.26: Amazon Security Lake Integration","description":"Tenzir Node v4.26 enhances our native Parquet and OCSF capabilities with a new","date":"2025-01-22T00:00:00.000Z","formattedDate":"January 22, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":2.375,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir Node v4.26: Amazon Security Lake Integration","slug":"tenzir-node-v4.26","authors":["lava"],"date":"2025-01-22T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.8: Charting the Unknown","permalink":"/releases/tenzir-platform-v1.8"},"nextItem":{"title":"Tenzir Node v4.25: Sinks Galore!","permalink":"/releases/tenzir-node-v4.25"}},"content":"[Tenzir Node v4.26][github-release] enhances our native Parquet and OCSF capabilities with a new\\noperator for writing data directly to Amazon Security Lake (ASL).\\n\\n![Tenzir Node v4.26](tenzir-node-v4.26.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.26.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Amazon Security Lake\\n\\nThe new `to_asl` operator enables users to write data directly to the\\nAmazon Security Lake:\\n\\n```tql\\nlet $s3_uri = \\"s3://aws-security-data-lake-eu-west-2-lake-abcdefghijklmnopqrstuvwxyz1234/ext/tenzir_network_activity/\\"\\n\\nload_kafka \\"ocsf_events\\"\\nread_ndjson\\nwhere @name == \\"ocsf.network_activity\\"\\nto_asl $s3_uri,\\n  region=\\"eu-west-2\\",\\n  accountId=\\"123456789012\\"\\n```\\n\\nFor more information, visit the [Integrations](/next/integrations/amazon/security-lake)\\npage in our documentation.\\n\\n## Output Changes\\n\\nWith this version, we made some changes to the default output format\\nof the Tenzir Node in order to make it easier for downstream tooling\\nto work with the data provided by a Tenzir Node.\\n\\n:::warning Potentially Breaking Change\\nThis may break automations that parse the JSON output of a Tenzir Pipeline.\\n:::\\n\\n### TQL Output\\n\\nBy default, the node now outputs pipeline results as native TQL data\\nrather than JSON.\\n\\nTQL is a superset of JSON, adding support for native IP addresses,\\nsubnets, timestamps, and durations, which were previously represented\\nas strings. Additionally, TQL now includes trailing commas by default.\\n\\nTo restore the previous JSON output format, append the [`write_json`](/next/tql2/operators/write_json)\\noperator to your pipeline or configure a default sink:\\n\\n```env\\nTENZIR_EXEC__IMPLICIT_EVENTS_SINK=\'write_json | save_file \\"-\\"\'\\n```\\n\\nBefore:\\n\\n```txt\\n{\\n  \\"activity_id\\": 16,\\n  \\"activity_name\\": \\"Query\\",\\n  \\"rdata\\": \\"31.3.245.133\\",\\n  \\"time\\": \\"2020-06-05T14:39:59.305988\\",\\n  \\"duration\\": \\"40s\\",\\n  \\"dst_endpoint\\": {\\n    \\"ip\\": \\"192.168.4.1\\",\\n    \\"port\\": 53\\n  }\\n}\\n```\\n\\nNow:\\n\\n```tql\\n{\\n  activity_id: 16,\\n  activity_name: \\"Query\\",\\n  rdata: 31.3.245.133,\\n  time: 2020-06-05T14:39:59.305988Z,\\n  duration: 40s,\\n  dst_endpoint: {\\n    ip: 192.168.4.1,\\n    port: 53,\\n  },\\n}\\n```\\n\\n### Timestamp Rendering\\n\\nTimestamps now include a `Z` suffix to indicate UTC time. The fractional seconds\\ndisplay has been refined: timestamps without sub-second precision no longer\\ninclude a fractional part, while others are printed with 3, 6, or 9 decimal\\nplaces based on their resolution.\\n\\nAdditionally, durations expressed in minutes now use `min` instead of `m`, and\\nfractional durations are displayed with full precision instead of rounding to\\ntwo decimal places.\\n\\n## TQL Features\\n\\nWe are continously working on expanding the feature set available for writing\\npipelines. This release adds two new functions, one new operator, and\\nadditional options to the JSON output.\\n\\n### Functions\\n\\nThe new [`string.match_regex(regex:string)`](/next/tql2/functions/match_regex)\\nfunction checks whether a string partially matches a regular expression.\\n\\nThe new [`merge`](/next/tql2/functions/merge) function combines two records. The\\nexpression `merge(foo, bar)` is a shorthand for `{...foo, ...bar}`.\\n\\n### Operators\\n\\nYou can use the new [`write_tql`](/next/tql2/operators/write_tql) operator\\nprints events as TQL expressions.\\n\\nWe added `strip` options to [`write_json`](/next/tql2/operators/write_json)\\nand [`write_ndjson`](/next/tql2/operators/write_ndjson), allowing you to\\nstrip null fields as well as empty records or lists.\\n\\n## Let\'s Connect!\\n\\nWe\u2019re excited to engage with our community!\\nJoin us every second Tuesday at 5 PM CET for office hours on [Discord][discord].\\nShare your ideas, preview upcoming features, or chat with fellow Tenzir users\\nand our team. Bring your questions, use cases, or just stop by to say hello!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4260"},{"id":"tenzir-node-v4.25","metadata":{"permalink":"/releases/tenzir-node-v4.25","source":"@site/releases/tenzir-node-v4.25/index.md","title":"Tenzir Node v4.25: Sinks Galore!","description":"Tenzir Node v4.25 adds new sinks for Snowflake, OpenSearch, and Elasticsearch,","date":"2025-01-08T00:00:00.000Z","formattedDate":"January 8, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":3.855,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir Node v4.25: Sinks Galore!","slug":"tenzir-node-v4.25","authors":["lava"],"date":"2025-01-08T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Node v4.26: Amazon Security Lake Integration","permalink":"/releases/tenzir-node-v4.26"},"nextItem":{"title":"Tenzir Platform v1.7: Explorer Drag\'n\'Drop","permalink":"/releases/tenzir-platform-v1.7"}},"content":"[Tenzir Node v4.25][github-release] adds new sinks for Snowflake, OpenSearch, and Elasticsearch,\\nallowing seamless data integration and output. It also introduces major\\nenhancements to TQL2, including new language features and operator\\nimprovements.\\n\\n![Tenzir Node v4.25](tenzir-node-v4.25.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.25.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## TQL2\\n\\nWe continue enhancing TQL2 to make it the default language for writing\\npipelines. This release expands TQL2\'s capabilities with numerous improvements\\nand features.\\n\\nTo simplify TQL2 adoption, we introduced a TQL2-only mode. When you enable\\nthis mode for a Tenzir Node, all pipelines on that node run in TQL2, and\\nthe Explorer interface automatically selects TQL2 mode. On such nodes, you\\ncan only use TQL1 through the `legacy` operator.\\n\\nTo enable it, set `TENZIR_TQL2=true` in your environment,\\nconfigure `tenzir.tql2: true` in your settings, or start the node with\\n`tenzir-node --tql2`.\\n\\n:::warning Call to Action\\nWe\'re going to enable TQL2-only mode by default in the upcoming\\nTenzir Node v5.0 release.\\n\\nPlease try to test it out and report any issues back to us, so we can\\nensure a seamless upgrade experience.\\n:::\\n\\n### From and To Operators\\n\\nWith the [`from`](/next/tql2/operators/from) and\\n[`to`](/next/tql2/operators/to) operators, we have ported one of the final major\\nfunctionalities from TQL1 to TQL2.\\n\\nThe [`from`](/next/tql2/operators/from) operator lets you onboard data from\\nmost sources effortlessly. For example, instead of\\n\\n```tql\\nload_http \\"https://example.com/file.json.gz\\"\\ndecompress_gzip\\nread_json\\n```\\n\\nyou can now write\\n\\n```tql\\nfrom \\"https://example.com/file.json.gz\\"\\n```\\n\\nto automatically determine the load operator, compression, and format.\\nIf you want to learn more about how the automatic detection works, check\\nout the [reference documentation](/next/tql2/operators/from).\\n\\nConversely, the [`to`](/next/tql2/operators/to) operator simplifies data\\ndelivery to most destinations. For instance, instead of\\n\\n```tql\\nwrite_json\\ncompress \\"gzip\\"\\nsave_file \\"myfile.json.gz\\"\\n```\\n\\nyou can now write\\n\\n```tql\\nto \\"file://file.json.gz\\"\\n```\\n\\nto achieve the same result.\\n\\n### IPs and Timestamps\\n\\nWe improved support for working with the native IP and timestamp types in TQL2.\\n\\nThe `in` operator now checks IP or subnet data for subnet membership. For\\nexample, to check whether an IP address belongs to a subnet, use an expression\\nlike `1.2.3.4 in 1.2.0.0/16`. Similarly, to verify whether a subnet includes\\nanother subnet, write `1.2.0.0/16 in 1.0.0.0/8`. A new function,\\n[`subnet(string)`](/next/tql2/functions/subnet), lets you parse strings as\\nsubnets.\\n\\nTo streamline time-related computations, we added the\\n[`from_epoch(x:duration) -> time`](/next/tql2/functions/from_epoch) function,\\nwhich converts durations to epoch times. Additionally, you can now convert\\nstrings to durations with the\\n[`duration(string)`](/next/tql2/functions/duration) function.\\n\\n### Operator Updates\\n\\nWe introduced significant updates to some TQL2 operators.\\n\\nThe HTTP operators now include several new options. The\\n[`load_http`](/next/tql2/operators/load_http) operator supports options like\\n`data`, `json`, `form`, `skip_peer_verification`, `skip_hostname_verification`,\\n`chunked`, and `multipart`. The `skip_peer_verification` and\\n`skip_hostname_verification` options are also available for the\\n[`save_http`](/next/tql2/operators/save_http) operator.\\n\\nWe split the `compress` and `decompress` operators into\\nseparate operators for each compression algorithm:\\n\\n- [`compress_gzip`](/next/tql2/operators/compress_gzip)\\n- [`compress_bz2`](/next/tql2/operators/compress_bz2)\\n- [`compress_brotli`](/next/tql2/operators/compress_brotli)\\n- [`compress_lz4`](/next/tql2/operators/compress_lz4)\\n- [`compress_zstd`](/next/tql2/operators/compress_zstd)\\n\\nas well as their respective `decompress_*` versions.\\n\\nThis allows us to expose more algorithm-specific options within each operator.\\nFor example, the [`compress_gzip`](/next/tql2/operators/compress_gzip) and\\n[`decompress_gzip`](/next/tql2/operators/decompress_gzip) operators now\\noffer additional options, such as `compress_gzip level=10, format=\\"deflate\\"`.\\n\\n## New Sinks: Snowflake, OpenSearch, and Elasticsearch\\n\\nTenzir Node v4.25 offers several new sinks for sending your data. The\\n[`to_snowflake`](/next/tql2/operators/to_snowflake) sink enables seamless\\nwriting of data into [Snowflake](https://www.snowflake.com) databases, helping\\nusers integrate Tenzir with one of the most popular cloud data platforms.\\n\\n![Snowflake Sink](snowflake.excalidraw.svg)\\n\\nTo send data to snowflake, a pipeline like this can be used:\\n\\n```tql\\nfrom {foo: 42, bar: true}\\nto_snowflake \\\\\\n  account_identifier=\\"asldyuf-xgb47555\\",\\n  user_name=\\"tenzir_user\\",\\n  password=\\"password1234\\",\\n  database=\\"MY_DB\\",\\n  schema=\\"MY_SCHEMA\\",\\n  table=\\"TENZIR\\"\\n```\\n\\nThe new [`to_opensearch`](/next/tql2/operators/to_opensearch)\\nsink operator allow direct data output to [OpenSearch](https://opensearch.org/)\\nand OpenSearch Bulk API compatible providers such as\\n[Elasticsearch](https://www.elastic.co/elasticsearch).\\n\\n![Opensearch Sink](opensearch.excalidraw.svg)\\n\\nThe [`to_opensearch`](/next/tql2/operators/to_opensearch) also\\nintegrates with the new [`from`](/next/tql2/operators/from) and\\n[`to`](/next/tql2/operators/to) operators introduced above. So\\nit is possible to write to OpenSearch or ElasticSearch instances\\nby using the `opensearch:` or `elasticsearch:` URL schemes.\\n\\n```tql\\nfrom {event: \\"example\\"}\\nto \\"opensearch://localhost:9200\\", action=\\"create\\", index=\\"main\\"\\n```\\n\\nFor more information on using Tenzir in combination with these new sinks, check\\nout our integration pages for [OpenSearch](/next/integrations/opensearch),\\n[ElasticSearch](/next/integrations/elasticsearch) and\\n[Snowflake](/next/integrations/snowflake)\\n\\n## Bugfixes\\n\\nThis release also includes several important bug fixes. Key fixes include:\\n\\n- Resolved a parsing error with operator parenthesis continuation to ensure\\n  proper evaluation of expressions like `where (x or y) and z`.\\n- Fixed issues with handling empty records in `write_parquet`, enhancing\\n  compatibility with Parquet\u2019s limitations.\\n- Prevented skipping required positional arguments in the argument parser\\n  to ensure robust script execution.\\n\\nCheck the [changelog][changelog] for the complete list.\\n\\n## Let\'s Connect!\\n\\nWe\u2019re excited to engage with our community!\\nJoin us every second Tuesday at 5 PM CET for office hours on [Discord][discord].\\nShare your ideas, preview upcoming features, or chat with fellow Tenzir users\\nand our team. Bring your questions, use cases, or just stop by to say hello!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4250"},{"id":"tenzir-platform-v1.7","metadata":{"permalink":"/releases/tenzir-platform-v1.7","source":"@site/releases/tenzir-platform-v1.7/index.md","title":"Tenzir Platform v1.7: Explorer Drag\'n\'Drop","description":"To kick off the new year, we\'re releasing [Tenzir Platform","date":"2025-01-08T00:00:00.000Z","formattedDate":"January 8, 2025","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":1.78,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir Platform v1.7: Explorer Drag\'n\'Drop","slug":"tenzir-platform-v1.7","authors":["lava"],"date":"2025-01-08T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Node v4.25: Sinks Galore!","permalink":"/releases/tenzir-node-v4.25"},"nextItem":{"title":"Tenzir Platform v1.6: Example Pipelines","permalink":"/releases/tenzir-platform-v1.6"}},"content":"To kick off the new year, we\'re releasing [Tenzir Platform\\nv1.7][github-release], featuring support for file drag and drop and a lot of\\nstability improvements.\\n\\n![Tenzir Platform v1.7](tenzir-platform-v1.7.png)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.7.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Explorer Drag\'n\'Drop\\n\\nIt is now possible to drag and drop files to the explorer window in order to\\ncreate a pipeline reading the data from the dropped file.\\n\\nThe generated pipeline is using the [`from`\\noperator](/next/tql2/operators/from), so in order to use this feature a Tenzir\\nNode with v4.25 or later is required.\\n\\nAll file types that are [automatically\\nrecognized](/next/tql2/operators/from#file-extensions) by the `from` operator\\ncan be used as source files.\\n\\n## Webhook Analytics\\n\\nFor Sovereign Edition users, it is now possible to specify a webhook URL as a\\ndestination for analytics events.\\n\\nTo configure this URL, set the `ANALYTICS_WEBHOOK_URL` environment variable in\\nthe platform environment to the desired value. If no URL is configured, no\\nanalytics are collected.\\n\\nNote that analytics events are currently sent synchronously, so a slow analytics\\nsink has the potential to slow down regular platform operations.\\n\\n## Stability Improvements\\n\\nOur recent focus on app stability continues to yield steady improvements that\\nimprove the quality of life for all users of the Tenzir Platform.\\n\\nFor example, we now avoid unnecessary network activity by preventing the\\npipeline list requests from targeting offline nodes, we fixed the pipeline list\\nspinner showing unnecessarily when changing a pipeline and we resolved an issue\\nwith pipeline keepalive daemons persisting longer than needed.\\n\\n## Bug fixes\\n\\nThis release also contains several additional bugfixes:\\n\\n- Fix sparkbar metrics query crashing when selecting offline nodes.\\n- Fix the keyboard shortcut triggering a pipeline rerun instead of confirming\\n  the modal.\\n- Fix dashboard creation with an empty name.\\n- Ensure \\"Add to dashboard\\" adds content based on the currently selected table\\n  or chart view.\\n- Fix several issues related to detailed activity metrics.\\n\\n## Join Us for Office Hours\\n\\nJoin us for our bi-weekly office hours every other Tuesday at 5 PM CET on our\\n[Discord server][discord]. It\'s a great opportunity to share your experiences,\\nask questions, and help shape the future of Tenzir with your valuable feedback!\\n\\n[discord]: /discord"},{"id":"tenzir-platform-v1.6","metadata":{"permalink":"/releases/tenzir-platform-v1.6","source":"@site/releases/tenzir-platform-v1.6/index.md","title":"Tenzir Platform v1.6: Example Pipelines","description":"We\'re happy to announce Tenzir Platform v1.6, featuring a new UI","date":"2024-12-17T00:00:00.000Z","formattedDate":"December 17, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":1.605,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir Platform v1.6: Example Pipelines","slug":"tenzir-platform-v1.6","authors":["lava"],"date":"2024-12-17T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.7: Explorer Drag\'n\'Drop","permalink":"/releases/tenzir-platform-v1.7"},"nextItem":{"title":"Tenzir Platform v1.5: Revamped Dashboards","permalink":"/releases/tenzir-platform-v1.5"}},"content":"We\'re happy to announce [Tenzir Platform v1.6][github-release], featuring a new UI\\nfor example pipelines and support for the new TQL2 mode for nodes.\\n\\n![Tenzir Platform v1.6](tenzir-platform-v1.6.png)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.6.0\\n\\n\x3c!-- truncate --\x3e\\n\\n:::warning Minimum Node Version\\nTenzir Platform v1.6 requires Tenzir Node v4.24 or later.\\n\\nYou must upgrade older nodes before they can reconnect to the newest Tenzir Platform version.\\n:::\\n\\n## Pipeline Examples\\n\\nWe\'ve added a new Examples pane to the Explorer for getting started with Tenzir\\nquickly.\\n\\nThe pane includes example pipelines from all installed packages. Head to the\\nLibrary to install packages for your integrations and see the examples in the\\nExplorer right away.\\n\\n## TQL2-only Mode\\n\\nAs part of our ongoing [migration\\nefforts](https://docs.tenzir.com/tql2-migration) from TQL1 to TQL2, the upcoming\\nTenzir Node v4.25 release will include a TQL2-only mode that has the effect of\\ntreating all pipelines on that node as TQL2 pipelines.\\n\\nThis platform version includes support for this feature, including locking the\\nTQL version toggle in the explorer to be always set to TQL2 and switching all\\ninternal pipelines to use TQL2.\\n\\nIf you\'re using a development version of the node, run `tenzir-node --tql2` or\\nset `TENZIR_TQL2=true` in your environment to enable this mode. It will become\\nthe default in the next major release.\\n\\n## Other Improvements\\n\\nThis release also includes a number of additional bugfixes:\\n\\n- Fixed an issue that could lead to an internal server error with some identity\\n  providers when using an on-prem version of the Tenzir Platform with refresh\\n  tokens.\\n- Revised the placement of context actions within context items.\\n- A lot of general refactoring of our code base to increase type safety and\\n  stability.\\n\\n## Join Us for Office Hours\\n\\nJoin us for our bi-weekly office hours every other Tuesday at 5 PM CET on our\\n[Discord server][discord]. It\'s a great opportunity to share your experiences,\\nask questions, and help shape the future of Tenzir with your valuable feedback!\\n\\n[discord]: /discord"},{"id":"tenzir-platform-v1.5","metadata":{"permalink":"/releases/tenzir-platform-v1.5","source":"@site/releases/tenzir-platform-v1.5/index.md","title":"Tenzir Platform v1.5: Revamped Dashboards","description":"Today we\'re announcing Tenzir Platform v1.5, which brings a","date":"2024-12-06T00:00:00.000Z","formattedDate":"December 6, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":1.725,"hasTruncateMarker":true,"authors":[{"name":"Sayan Sarkar","title":"Software Engineer","url":"https://github.com/dit7ya","email":"sayan@tenzir.com","imageURL":"https://github.com/dit7ya.png","key":"dit7ya"},{"name":"Danyl Fernandes","title":"Software Engineer","url":"https://github.com/gitryder","email":"danyl@tenzir.com","imageURL":"https://github.com/gitryder.png","key":"gitryder"}],"frontMatter":{"title":"Tenzir Platform v1.5: Revamped Dashboards","slug":"tenzir-platform-v1.5","authors":["dit7ya","gitryder"],"date":"2024-12-06T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.6: Example Pipelines","permalink":"/releases/tenzir-platform-v1.6"},"nextItem":{"title":"Tenzir Node v4.24: List Manipulation","permalink":"/releases/tenzir-node-v4.24"}},"content":"Today we\'re announcing [Tenzir Platform v1.5][github-release], which brings a\\nricher dashboarding experience and adds a new contexts page.\\n\\n![Tenzir Platform v1.5](tenzir-platform-v1.5.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.5.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Multi-Node Dashboards\\n\\nDashboards received a major upgrade in this release.\\n\\nIn the past, every Tenzir Node had exactly one dashboard associated with it.\\nNow, dashboards are independent of nodes: Select your dashboard, and then select\\nwhat node to view it for.\\n\\nUnder the hood, we preserve which node you last viewed a dashboard for, and even\\nallow you to configure which node to view each individual chart or table on a\\ndashboard for. This makes building system-wide overviews, e.g., a health monitoring dashboard across nodes, finally possible.\\n\\nTo create a new dashboard, simply hit the \\"Add Dashboard\\" on the Dashboards\\npage, or create one on the fly when adding a chart to a dashboard in the\\nExplorer.\\n\\nAnd this is just the start\u2014we\u2019ve got more dashboard improvements in the\\npipeline, so stay tuned!\\n\\n## Contexts Page\\n\\nThe new Contexts page allows for managing contexts directly in the Tenzir\\nPlatform. Previously, creating a context required installing a package or\\nrunning a pipeline. Now, a visual interface guides you through the process.\\n\\nThe new interface makes context management faster, easier, and more intuitive,\\nfitting seamlessly into your daily workflow. Give it a try today!\\n\\n## Other Improvements\\n\\nAs usual, we\'ve squashed a lot of bugs on the way:\\n\\n- Fixed an issue that prevented package uninstallation.\\n- Resolved a bug that could cause infinite loading when logging into a new\\n  account after logging out.\\n- Corrected an error that broke account deletion in the app.\\n- Fixed a bug that prevented package uninstallation.\\n- Fixed another bug of the app going into infinite loading in case of logging\\n  into a new account after logging out in the browser.\\n\\n## Join Us for Office Hours\\n\\nWe\'d love to connect with you at our office hours, held every second Tuesday at\\n5 PM CET on our [Discord server][discord]. Drop by for a chat\u2014we always enjoy\\nhearing your thoughts and feedback!\\n\\n[discord]: /discord"},{"id":"tenzir-node-v4.24","metadata":{"permalink":"/releases/tenzir-node-v4.24","source":"@site/releases/tenzir-node-v4.24/index.md","title":"Tenzir Node v4.24: List Manipulation","description":"Working with lists is easier than ever with Tenzir Node v4.24","date":"2024-12-03T00:00:00.000Z","formattedDate":"December 3, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":4.005,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir Node v4.24: List Manipulation","slug":"tenzir-node-v4.24","authors":["dominiklohmann"],"date":"2024-12-03T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.5: Revamped Dashboards","permalink":"/releases/tenzir-platform-v1.5"},"nextItem":{"title":"Tenzir Platform v1.4: Platform Alerts","permalink":"/releases/tenzir-platform-v1.4"}},"content":"Working with lists is easier than ever with [Tenzir Node v4.24][github-release]\\nand its new functions for list manipulation. Also, contexts are now first-class\\ncitizens in TQL2.\\n\\n![Tenzir Node v4.24](tenzir-node-v4.24.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.24.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Working with Lists\\n\\nWe\'ve added a number of functions that make it easier than before to work with\\nlists in TQL2.\\n\\n### Simple List Manipulation\\n\\nLet\'s start simple: How do you append to a list, prepend to a list, or concatenate lists?\\n\\n```tql title=\\"Append to a list\\"\\nfrom {xs: [1, 2, 3]}\\nxs = xs.append(4)\\n```\\n\\n```tql\\n{xs: [1, 2, 3, 4]}\\n```\\n\\n```tql title=\\"Prepend to a list\\"\\nfrom {xs: [1, 2, 3]}\\nxs = xs.prepend(4)\\n```\\n\\n```tql\\n{xs: [4, 1, 2, 3]}\\n```\\n\\n```tql title=\\"Concatenate lists\\"\\nfrom {xs: [1, 2], ys: [3, 4]}\\nzs = xs.concatenate(ys)\\n```\\n\\n```tql\\n{xs: [1, 2], ys: [3, 4], zs: [1, 2, 3, 4]}\\n```\\n\\n### Handling Lists of Strings\\n\\nWorking with lists of strings is also a common task. Here\'s how you can split a\\nstring, split a string with a regex, and join a list of strings.\\n\\n```tql title=\\"Split a string\\"\\nfrom {s: \\"1,2,3\\"}\\nxs = s.split(\\",\\")\\n```\\n\\n```tql\\n{s: \\"1,2,3\\", xs: [\\"1\\", \\"2\\", \\"3\\"]}\\n```\\n\\n```tql title=\\"Split a string with a regex\\"\\nfrom {s: \\"1,2;3\\"}\\nxs = s.split_regex(\\"[,;]\\")\\n```\\n\\n```tql\\n{s: \\"1,2;3\\", xs: [\\"1\\", \\"2\\", \\"3\\"]}\\n```\\n\\n```tql title=\\"Join a list\\"\\nfrom {xs: [\\"1\\", \\"2\\", \\"3\\"]}\\ns = xs.join(\\",\\")\\n```\\n\\n```tql\\n{xs: [\\"1\\", \\"2\\", \\"3\\"], s: \\"1,2,3\\"}\\n```\\n\\n### Modify and Remove List Elements\\n\\nTwo new functions `map` and `where` allow you to modify and filter lists.\\n\\n```tql title=\\"Modify list elements\\"\\nfrom {xs: [1, 2, 3]}\\ndoubled = xs.map(x, x * 2)\\n```\\n\\n```tql\\n{xs: [1, 2, 3], doubled: [2, 4, 6]}\\n```\\n\\n```tql title=\\"Remove list elements\\"\\nfrom {xs: [1, 2, 3, 4, 5]}\\nlarge = xs.where(x, x > 3)\\n```\\n\\n```tql\\n{xs: [1, 2, 3, 4, 5], large: [4, 5]}\\n```\\n\\n### Aggregation Functions\\n\\nWith the `summarize` operator, Tenzir already supports aggregating values across\\nmultiple events. Now, all aggregation functions work on lists as well as regular\\nfunctions:\\n\\n```tql title=\\"Sum of a list\\"\\nfrom {xs: [1, 2, 3]}\\ntotal = xs.sum()\\n```\\n\\n```tql\\n{xs: [1, 2, 3], total: 6}\\n```\\n\\n```tql title=\\"Minimum of a list\\"\\nfrom {xs: [1, 2, 3]}\\nlowest = xs.min()\\n```\\n\\n```tql\\n{xs: [1, 2, 3], lowest: 1}\\n```\\n\\n```tql title=\\"Distinct values in a list\\"\\nfrom {xs: [1, 2, 2, 3]}\\nunique = xs.distinct()\\n```\\n\\n```tql\\n{xs: [1, 2, 2, 3], unique: [1, 2, 3]}\\n```\\n\\n:::info Aggregation Functions\\nWith this change, all aggregation functions work with lists. Check the\\n[functions reference](/tql2/functions#aggregation) for a full list of available\\nfunctions.\\n:::\\n\\n## Contexts in TQL2\\n\\nContexts have arrived in TQL2, bringing powerful enrichment capabilities to your\\npipelines. Our new [Enrichment](/next/enrichment) documentation explains how\\nthey work in detail, but here\'s a quick overview of what you can do with\\ncontexts.\\n\\nContexts are stateful objects that allow you to add contextual data to your\\nevents. You can use them to:\\n\\n- Build lookup tables for fast IP-to-asset mapping\\n- Create Bloom filters for efficient membership testing of large sets\\n- Leverage GeoIP databases for geographic IP enrichment\\n\\nHere\'s a simple example of using a lookup table context:\\n\\n```tql title=\\"Create a lookup table context\\"\\ncontext::create_lookup_table \\"ip-to-hostname\\"\\n```\\n\\n```tql title=\\"Populate lookup table with data\\"\\nsubscribe \\"suricata\\"\\nwhere event_type == \\"dns\\"\\nunroll dns.answers\\nwhere dns.answers.rrtype in [\\"A\\", \\"AAAA\\"]\\ncontext::update \\"ip-to-hostname\\", key: dns.answers.rdata\\n```\\n\\n```tql title=\\"Enrich other data with context\\"\\nsubscribe \\"zeek\\"\\ncontext::enrich \\"ip-to-hostname\\", key=id.resp_h, into=hostname\\n\u2026\\n```\\n\\nEach context type has its own strengths:\\n\\n- **Lookup Tables** excel at key-value mappings with features like subnet\\n  matching and value aggregation\\n- **Bloom Filters** provide space-efficient set membership testing\\n- **GeoIP Databases** offer specialized geographic information lookup for IP\\n  addresses\\n\\nStay tuned for the next Tenzir Platform release, which adds support for\\ncontexts.\\n\\n## Other Improvements\\n\\nWe\'ve completed the migration of all connectors and formats to TQL2, making them\\nmore powerful and easier to use than ever. Every connector and format now fully\\nsupports TQL2\'s expressive syntax and features. At this point, just a handful of\\noperators remain to be migrated to TQL2: `chart`, `deduplicate`, `lookup`,\\n`parse`, and `print`. We\'re working hard to complete this migration, but think\\nthat at this point we\'re already covering 99% of our use cases with TQL2.\\n\\nIn addition to the above, the release also contains numerous quality-of-life\\nimprovements and bug fixes, so be sure to check out the [changelog][changelog].\\n\\n## Let\'s Connect!\\n\\nWe\'re excited to connect with our community! Join us every second Tuesday at 5\\nPM CET for our office hours on [Discord][discord]. It\'s a great opportunity to\\nshare your ideas, get a sneak peek at upcoming features, or just chat with\\nfellow Tenzir users and our team. Bring your questions, use cases, or simply\\ndrop by to say hello!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4240"},{"id":"tenzir-platform-v1.4","metadata":{"permalink":"/releases/tenzir-platform-v1.4","source":"@site/releases/tenzir-platform-v1.4/index.md","title":"Tenzir Platform v1.4: Platform Alerts","description":"We\'re excited to announce that Tenzir Platform v1.4 brings you","date":"2024-11-19T00:00:00.000Z","formattedDate":"November 19, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":1.035,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir Platform v1.4: Platform Alerts","slug":"tenzir-platform-v1.4","authors":["lava"],"date":"2024-11-19T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Node v4.24: List Manipulation","permalink":"/releases/tenzir-node-v4.24"},"nextItem":{"title":"Tenzir Node v4.23","permalink":"/releases/tenzir-node-v4.23"}},"content":"We\'re excited to announce that [Tenzir Platform v1.4][github-release] brings you\\nmonitoring capabilities with alerts for offline nodes, along with a bundle of\\ndelightful improvements throughout the application.\\n\\n![Tenzir Platform v1.4](tenzir-platform-v1.4.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.4.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Platform Alerts\\n\\nWe\'ve introduced a critical monitoring feature to ensure the reliability of your\\ninfrastructure. The platform automatically alerts you when nodes are offline for\\nlonger than a configured threshold.\\n\\nThese alerts provide essential visibility into the health of your nodes and\\npipelines, enabling proactive management of system availability and rapid\\nresponse to potential outages.\\n\\nFor complete implementation details and configuration examples, refer to our\\n[Platform CLI](/platform-cli) documentation.\\n\\n## Other Improvements\\n\\nWe\'ve been busy sprinkling some improvements throughout the platform:\\n\\n- Your workspace switching experience is now lightning-fast \u26a1\\n- We\'ve added a sleek new diagnostics drawer to the dashboard, making it just as\\n  easy to spot warnings or errors on the dashboard as in the explorer\\n- Chart images now download correctly even on the dashboard page\\n\\n## Join Us for Office Hours\\n\\nWe\'d love to see your friendly face at our office hours, held every second\\nTuesday at 5 PM CET on our [Discord server][discord]. Drop by for a chat\u2014we\\nalways enjoy hearing what\'s on your mind!\\n\\n[discord]: /discord"},{"id":"/tenzir-node-v4.23","metadata":{"permalink":"/releases/tenzir-node-v4.23","source":"@site/releases/tenzir-node-v4.23/index.md","title":"Tenzir Node v4.23","description":"Tenzir Node v4.23 comes with a new load_balance operator,","date":"2024-11-07T00:00:00.000Z","formattedDate":"November 7, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":1.405,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir Node v4.23","authors":["IyeOnline"],"date":"2024-11-07T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.4: Platform Alerts","permalink":"/releases/tenzir-platform-v1.4"},"nextItem":{"title":"Tenzir Platform v1.3","permalink":"/releases/tenzir-platform-v1.3"}},"content":"[Tenzir Node v4.23][github-release] comes with a new `load_balance` operator,\\na dedicated `to_splunk` sink, Universal Function Call Syntax and much more!\\n\\n![Tenzir Node v4.23](tenzir-node-v4.23.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.23.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Load Balancing\\n\\nWith the new [`load_balance`](../next/tql2/operators/load_balance) operator, you\\ncan distribute the output of a Tenzir pipeline over multiple sinks. This allows\\nyou to balance the output load, which is especially useful for network connections.\\n\\nThe operator accepts a pipeline and a list of values, which\\ncan be used in the pipeline:\\n\\n```tql title=\\"Load balance over multiple TCP endpoints\\"\\nlet $cfg = [\\"192.168.0.30:8080\\", \\"192.168.0.30:8081\\"]\\n\\nsubscribe \\"input\\"\\nload_balance $cfg {\\n  write_json\\n  save_tcp $cfg\\n}\\n```\\n\\n## Splunk Sink\\n\\nThe new [`to_splunk`](../next/tql2/operators/to_splunk) operator allows you to\\nsend events to a Splunk HTTP Event Collector. Events are send in batches as\\ncompressed JSON.\\n\\n```tql title=\\"Send events to a Splunk HEC\\"\\nload_file \\"example.json\\"\\nread_json\\nto_splunk \\"https://localhost:8088\\", hec_token=\\"example-token-1234\\"\\n```\\n\\n## Universal Function Call Syntax\\n\\n[UCFS]: https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax\\n\\nAdditionally, Tenzir now supports [Universal Function Call Syntax][UCFS],\\nallowing for more natural use of functions using a method call syntax or a free\\nfunction style, depending on the situation. In our documentation we make use of\\nthe more appropriate form based on the function.\\n\\nThe TQL2 Documentation is making great progress. In this release we have added\\nthe documentation pages for [functions](../next/tql2/functions).\\n\\n## Other Changes\\n\\nIn addition to the above, the release also contains a number of small changes\\nand fixes, so be sure to check out the [changelog][changelog].\\n\\n## Join Us for Office Hours\\n\\nEvery second Tuesday at 5 PM CET, we hold our office hours on our [Discord\\nserver][discord]. Whether you have ideas for new packages or want to discuss\\nupcoming features\u2014join us for a chat!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4230"},{"id":"/tenzir-platform-v1.3","metadata":{"permalink":"/releases/tenzir-platform-v1.3","source":"@site/releases/tenzir-platform-v1.3/index.md","title":"Tenzir Platform v1.3","description":"Tenzir Platform v1.3 brings a redesigned explorer page,","date":"2024-11-07T00:00:00.000Z","formattedDate":"November 7, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":0.99,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir Platform v1.3","authors":["lava"],"date":"2024-11-07T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Node v4.23","permalink":"/releases/tenzir-node-v4.23"},"nextItem":{"title":"Tenzir Platform v1.2","permalink":"/releases/tenzir-platform-v1.2"}},"content":"[Tenzir Platform v1.3][github-release] brings a redesigned explorer page,\\nbetter behavior of the event inspector, and many other fixes.\\n\\n![Tenzir Platform v1.3](tenzir-platform-v1.3.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.3.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Vertical Layout\\n\\nIn the explorer page, you can now find a new button to switch between\\na horizontal and vertical layout.\\n\\n![Vertical Layout](./tenzir-platform-v1.3.png)\\n\\nThe new vertical layout was designed to maximize the amount of screen\\nspace available for the most important part of the explorer - the data!\\n\\nAnother goal was to make it easier to write long and complex pipelines\\nby increasing the number of lines that can be displayed on screen\\nat the same time.\\n\\n## Other Fixes\\n\\nWe\u2019ve implemented several other noteworthy fixes and enhancements:\\n\\n- The explorer event inspector now automatically selects the first event if it is open.\\n- We fixed an issue in the detailed activity charts in the pipelines page where the ingress and egress activities were mistakenly swapped.\\n- Charts in the dashboard show up to 10,000 events now.\\n\\n## Join Us for Office Hours\\n\\nEvery second Tuesday at 5 PM CET, we hold our office hours on our [Discord\\nserver][discord]. We love hearing your feedback\u2014come join us for a chat!\\n\\n[discord]: /discord"},{"id":"/tenzir-platform-v1.2","metadata":{"permalink":"/releases/tenzir-platform-v1.2","source":"@site/releases/tenzir-platform-v1.2/index.md","title":"Tenzir Platform v1.2","description":"Tenzir Platform v1.2 brings improvements to diagnostics in the","date":"2024-10-23T00:00:00.000Z","formattedDate":"October 23, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":1.54,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir Platform v1.2","authors":["dominiklohmann"],"date":"2024-10-23T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.3","permalink":"/releases/tenzir-platform-v1.3"},"nextItem":{"title":"Tenzir Node v4.22","permalink":"/releases/tenzir-node-v4.22"}},"content":"[Tenzir Platform v1.2][github-release] brings improvements to diagnostics in the\\nExplorer, the ability to download charts, and many stability improvements.\\n\\n![Tenzir Platform v1.2](tenzir-platform-v1.2.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.2.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Simplified Diagnostics Flow\\n\\nIn our previous version, the diagnostics pane would automatically open when a\\npipeline encountered an error or completed without results but with at least one\\nwarning. While this feature aimed to provide immediate feedback, it often led to\\nconfusion as the pane never closed automatically. We recognized that\\nautomatically closing it could disrupt your workflow by causing layout shifts\\nwhile iterating on your pipelines.\\n\\nWith the new update, we\u2019ve simplified the user experience: the diagnostics pane\\nwill no longer open or close automatically. Instead, you can easily access it by\\nclicking on the diagnostics count or the newly added toasts for errors. This\\nchange ensures a smoother and more intuitive interaction with the diagnostics\\nfeature.\\n\\n## Downloading Charts\\n\\nWe\u2019ve also made it easier for you to manage your visual data. You can now save\\ncharts in the Explorer or on Dashboards in either SVG or PNG formats.\\nAdditionally, we\u2019ve updated the chart colors to improve contrast, making it\\neasier to distinguish between different data points.\\n\\n## Other Fixes\\n\\nWe\u2019ve implemented several other noteworthy fixes and enhancements:\\n\\n- The pesky 408 Proxy Timeout errors in the Explorer for pipelines that run for\\n  a longer period of time no longer exist.\\n- We fixed a bug that caused the detailed pipeline activity to render\\n  incorrectly. Note that rendering the activity chart now requires running at\\n  least Tenzir Node v4.22.\\n- For Sovereign Edition users, the env variable `PUBLIC_OIDC_SCOPES` allows for\\n  overriding the default OIDC scopes.\\n\\n## Join Us for Office Hours\\n\\nEvery second Tuesday at 5 PM CET, we hold our office hours on our [Discord\\nserver][discord]. We love hearing your feedback\u2014come join us for a chat!\\n\\n[discord]: /discord"},{"id":"/tenzir-node-v4.22","metadata":{"permalink":"/releases/tenzir-node-v4.22","source":"@site/releases/tenzir-node-v4.22/index.md","title":"Tenzir Node v4.22","description":"Tenzir Node v4.22 comes with documentation for the new version","date":"2024-10-18T00:00:00.000Z","formattedDate":"October 18, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":1.465,"hasTruncateMarker":true,"authors":[{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"},{"name":"Raghav Narang","title":"Software Engineer","url":"https://github.com/raxyte","email":"raghav@tenzir.com","imageURL":"https://github.com/raxyte.png","key":"raxyte"}],"frontMatter":{"title":"Tenzir Node v4.22","authors":["jachris","raxyte"],"date":"2024-10-18T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.2","permalink":"/releases/tenzir-platform-v1.2"},"nextItem":{"title":"Tenzir Node v4.21","permalink":"/releases/tenzir-node-v4.21"}},"content":"[Tenzir Node v4.22][github-release] comes with documentation for the new version\\nof the Tenzir Query Language, connectors for Google Cloud Pub/Sub and various\\nbug fixes.\\n\\n![Tenzir Node v4.22](tenzir-node-v4.22.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.22.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## TQL2 Documentation\\n\\nWe are thrilled to share that the new version of our query language, TQL2,\\nhas taken exciting strides forward! If you\u2019ve been using the Tenzir Platform,\\nyou might have already noticed the toggle to enable TQL2,\\nallowing you to explore its potential.\\n\\nToday marks a significant milestone as we release the initial version of our\\n[TQL2 documentation](../next/tql2/operators). You can now dive deep into\\nlearning TQL2 and unlock its full capabilities!\\n\\nWe\u2019re committed to continuously enhancing the documentation, ensuring you have\\nall the resources you need to make the most of TQL2. Stay tuned for more\\nupdates!\\n\\n## Google Cloud Pub/Sub Integration\\n\\nIntroducing our latest connector: Google Cloud Pub/Sub! The new integration\\nallows users to seamlessly subscribe and publish to Google Cloud Pub/Sub.\\n\\n```text{0} title=\\"Subscribe to \'my-subscription\'\\"\\nload google-cloud-pubsub \\"amazing-project-123456\\" \\"my-subscription\\"\\n| parse syslog\\n...\\n```\\n\\n```text{0} title=\\"Publish events to \'alerts-topic\'\\"\\n...\\n| write json --ndjson\\n| save google-cloud-pubsub \\"amazing-project-123456\\" \\"alerts-topic\\"\\n```\\n\\nThe connector is also available in TQL2 as\\n[`load_google_cloud_pubsub`](../next/tql2/operators/load_google_cloud_pubsub) and\\n[`save_google_cloud_pubsub`](../next/tql2/operators/save_google_cloud_pubsub):\\n\\n```tql title=\\"Using Tenzir to filter and convert events\\"\\nload_google_cloud_pubsub \\"amazing-project-123456\\", \\"my-subscription\\"\\nread_syslog\\ncontent = content.parse_grok(\\"{%WORD:type} %{IP:source}\\")\\nwhere content.type == \\"alert\\"\\nwrite_json ndjson=true\\nsave_google_cloud_pubsub \\"amazing-project-123456\\", \\"alerts-topic\\"\\n```\\n\\n## Other Changes\\n\\nThis release additionally includes numerous small bug fixes and under-the-hood\\nimprovements. For a detailed list of changes, be sure to check out the\\n[changelog][changelog].\\n\\n## Join Us for Office Hours\\n\\nEvery second Tuesday at 5 PM CET, we hold our office hours on our [Discord\\nserver][discord]. Whether you have ideas for new packages or want to discuss\\nupcoming features\u2014join us for a chat!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4220"},{"id":"/tenzir-node-v4.21","metadata":{"permalink":"/releases/tenzir-node-v4.21","source":"@site/releases/tenzir-node-v4.21/index.md","title":"Tenzir Node v4.21","description":"Parsing is now easier, faster, and better than before with [Tenzir Node","date":"2024-10-04T00:00:00.000Z","formattedDate":"October 4, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"node","permalink":"/releases/tags/node"}],"readingTime":2.835,"hasTruncateMarker":true,"authors":[{"name":"Raghav Narang","title":"Software Engineer","url":"https://github.com/raxyte","email":"raghav@tenzir.com","imageURL":"https://github.com/raxyte.png","key":"raxyte"},{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir Node v4.21","authors":["raxyte","IyeOnline"],"date":"2024-10-04T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["release","node"],"comments":true},"prevItem":{"title":"Tenzir Node v4.22","permalink":"/releases/tenzir-node-v4.22"},"nextItem":{"title":"Tenzir Platform v1.1","permalink":"/releases/tenzir-platform-v1.1"}},"content":"Parsing is now easier, faster, and better than before with [Tenzir Node\\nv4.21][github-release]. Also: introducing an all-new integration with Azure Blob\\nStorage.\\n\\n![Tenzir Node v4.21](tenzir-node-v4.21.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.21.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Reading in Fortinet and Sophos logs\\n\\nTenzir now supports reading Fortinet and Sophos logs. This feature is powered\\nby an update to the KV-Parser, which has been upgraded to allow quoted values.\\nFor example, here\'s how you can use the new parser to take apart a Fortinet\\nFortiGate log file:\\n\\n```text {0} title=\\"Parse a Fortinet log file\\"\\nfrom \\"fortinet.log\\" read kv\\n| parse rawdata kv \\"\\\\|\\" \\"=\\"\\n```\\n\\nUsing the above pipeline to parse an example file:\\n\\n```text {0} title=\\"fortinet.log\\"\\nlogver=700120523 timestamp=1694183556 devname=\\"HA-Cluster\\" devid=\\"FG200E4Q1790000\\" vd=\\"root\\" date=2023-09-08 time=14:32:36 eventtime=1694176357211540851 tz=\\"+0200\\" logid=\\"0317013312\\" type=\\"utm\\" subtype=\\"webfilter\\" eventtype=\\"ftgd_allow\\" level=\\"notice\\" policyid=2 poluuid=\\"2b9647ee-70cb-51ed-d1c3-8e08a2e5fec0\\" policytype=\\"proxy-policy\\" sessionid=2096597111 user=\\"user1\\" group=\\"group1\\" authserver=\\"DC20\\" srcip=192.168.0.1 srcport=57642 srccountry=\\"Reserved\\" srcintf=\\"port1\\" srcintfrole=\\"lan\\" dstip=1.2.3.4 dstport=443 dstcountry=\\"Germany\\" dstintf=\\"wan1\\" dstintfrole=\\"wan\\" proto=6 service=\\"HTTPS\\" hostname=\\"www.example1.com\\" profile=\\"REDACTED\\" action=\\"passthrough\\" reqtype=\\"referral\\" url=\\"https://www.example1.com/\\" referralurl=\\"https://www.example1.com/page\\" sentbyte=1713 rcvdbyte=238 direction=\\"outgoing\\" msg=\\"URL belongs to an allowed category in policy\\" method=\\"domain\\" cat=75 catdesc=\\"Internet Radio and TV\\" rawdata=\\"Method=CONNECT|User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.81\\"\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"logver\\": 700120523,\\n  \\"timestamp\\": 1694183556,\\n  \\"devname\\": \\"HA-Cluster\\",\\n  // ...\\n  \\"catdesc\\": \\"Internet Radio and TV\\",\\n  \\"rawdata\\": {\\n    \\"Method\\": \\"CONNECT\\",\\n    \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.81\\"\\n  }\\n}\\n```\\n\\n## Parsing Improvements\\n\\nWe\u2019ve made significant improvements to the accuracy of all parsers that deal\\ninputs of different schemas. Previously, parsers would often attempt to merge\\ninput schemas, which could lead to additional null fields in the data. Now,\\nparsers strictly adhere to the input data. Basically, you now exactly get out\\nwhat you put in.\\n\\nAlongside this, we also improved the user\'s control over the schema produced by the\\nparsers. For example, you can now give a `schema` to the `kv` parser, which gives\\nyou the ability to specify the schema it should produce.\\n\\n```text {0} title=\\"Parse a Fortinet log file adhering to a manually specified schema\\"\\nfrom \\"fortinet.log\\" read kv --schema=my_schema\\n| parse rawdata kv \\"\\\\|\\" \\"=\\"\\n| \u2026\\n```\\n\\nYou can find more information about these new options in [our\\ndocumentation](/next/formats#parser-schema-inference).\\n\\n## Azure Blob Storage Integration\\n\\nTenzir now has an integration with Azure Blob Storage! This integration enables\\nusers to securely load log files and export processed data back to Azure Blob\\nStorage, all while benefiting from both the flexibility and scalability of cloud\\nstorage and the efficiency of Tenzir\'s data pipelines.\\n\\n```text {0} title=\\"Load \'suricata.json\', authorized as \'tenzirdev\'\\"\\nfrom \\"abfss://tenzirdev@container/suricata.json\\" read suricata\\n\u2026\\n```\\n\\n```text {0} title=\\"Save events as csv to \'data.csv\', authorized as \'tenzirdev\'\\"\\n\u2026\\n| to \\"abfss://tenzirdev@container/data.csv\\"\\n```\\n\\n:::note Authenticate with the Azure CLI\\nRun `az login` on the command-line to authenticate the current user with Azure\'s\\ncommand-line arguments.\\n:::\\n\\n## Other Changes\\n\\nWe have made great progress on TQL2 in the last month. A significant amount of\\nthe language is already ported and a lot new and exciting functionality has been\\nimplemented. You can expect the documentation to be available with the next\\nrelease.\\n\\nThis release additionally includes numerous small bug fixes and under-the-hood\\nimprovements. For a detailed list of changes, be sure to check out the\\n[changelog][changelog].\\n\\n## Join Us for Office Hours\\n\\nEvery second Tuesday at 5 PM CET, we hold our office hours on our\\n[Discord server][discord]. Whether you have ideas for new packages or want to\\ndiscuss upcoming features\u2014join us for a chat!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4210"},{"id":"/tenzir-platform-v1.1","metadata":{"permalink":"/releases/tenzir-platform-v1.1","source":"@site/releases/tenzir-platform-v1.1/index.md","title":"Tenzir Platform v1.1","description":"Tenzir Platform v1.1 is here! This release brings key","date":"2024-10-04T00:00:00.000Z","formattedDate":"October 4, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"platform","permalink":"/releases/tags/platform"}],"readingTime":1.415,"hasTruncateMarker":true,"authors":[{"name":"Danyl Fernandes","title":"Software Engineer","url":"https://github.com/gitryder","email":"danyl@tenzir.com","imageURL":"https://github.com/gitryder.png","key":"gitryder"}],"frontMatter":{"title":"Tenzir Platform v1.1","authors":["gitryder"],"date":"2024-10-04T00:00:00.000Z","tags":["release","platform"],"comments":true},"prevItem":{"title":"Tenzir Node v4.21","permalink":"/releases/tenzir-node-v4.21"},"nextItem":{"title":"Tenzir Node v4.20","permalink":"/releases/tenzir-node-v4.20"}},"content":"[Tenzir Platform v1.1][github-release] is here! This release brings key\\nenhancements, including improved diagnostics, authentication updates, and\\nvarious bug fixes for a smoother user experience.\\n\\n![Tenzir Platform v1.1](tenzir-platform-v1.1.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/platform/releases/tag/v1.1.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Diagnostics in the Explorer\\n\\nDiagnostics now appear directly in the Explorer, with a new badge indicating the\\nnumber of errors and warnings directly in the editor. Clicking the badge,\\nencountering an error, or encountering warnings for pipelines with no results\\nopens the diagnostics panel.\\n\\n![Explorer Diagnostics](explorer-diagnostics.png)\\n\\n## Refresh Token Support\\n\\nWe have revamped how we internally handle authentication, and also added support\\nfor OAuth refresh tokens.\\n\\n:::note Configuration Changes for Sovereign Edition Users\\nUsers of the Sovereign Edition that self-host the Tenzir Platform must set an\\nadditional environment variable in their Docker Compose configuration for the\\n`app` service:\\n\\n```diff {0} title=\\n@@ -110,6 +117,9 @@ services:\\n       - PRIVATE_WEBAPP_KEY=${TENZIR_PLATFORM_INTERNAL_APP_API_KEY}\\n       - AUTH_SECRET=${TENZIR_PLATFORM_INTERNAL_AUTH_SECRET}\\n       - PUBLIC_DISABLE_DEMO_NODE_AND_TOUR=${TENZIR_PLATFORM_DISABLE_LOCAL_DEMO_NODES}\\n+      - PRIVATE_DRIZZLE_DATABASE_URL=postgres://${TENZIR_PLATFORM_POSTGRES_USER}:${TENZIR_PLATFORM_POSTGRES_PASSWORD}@${TENZIR_PLATFORM_POSTGRES_HOSTNAME}/${TENZIR_PLATFORM_POSTGRES_DB}\\n+    depends_on:\\n+      - postgres\\n     ports:\\n       - \\"3000:3000\\"\\n```\\n:::\\n\\n## Other Fixes\\n\\nAs always, we fixed a bunch of smaller issues on the way:\\n\\n- Resolved intermittent node disconnections from the platform.\\n- We fixed the incorrect x-axis positioning when chart values are all zeroes.\\n- Fixed flashing content after clicking \\"Load More\\" in tables and updated the\\n  button styling.\\n- Resolved a bug preventing pipeline editing in the details view.\\n- Improved UX for adding self-hosted nodes\u2014clear instructions are now visible on\\n  the pipelines page when a node is created and disconnected.\\n\\n## Join Us for Office Hours\\n\\nEvery second Tuesday at 5 PM CET, we hold our office hours on our\\n[Discord server][discord]. Whether you have ideas for new packages or want to\\ndiscuss upcoming features\u2014join us for a chat!\\n\\n[discord]: /discord"},{"id":"/tenzir-node-v4.20","metadata":{"permalink":"/releases/tenzir-node-v4.20","source":"@site/releases/tenzir-node-v4.20/index.md","title":"Tenzir Node v4.20","description":"Tenzir Node v4.20 is here, bringing a host of under-the-hood","date":"2024-08-30T00:00:00.000Z","formattedDate":"August 30, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"}],"readingTime":2.14,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir Node v4.20","authors":["dominiklohmann"],"date":"2024-08-30T00:00:00.000Z","tags":["release"],"comments":true},"prevItem":{"title":"Tenzir Platform v1.1","permalink":"/releases/tenzir-platform-v1.1"},"nextItem":{"title":"Tenzir v4.19","permalink":"/releases/tenzir-v4.19"}},"content":"[Tenzir Node v4.20][github-release] is here, bringing a host of under-the-hood\\nimprovements that pave the way for exciting upcoming changes to the Tenzir\\nPlatform.\\n\\n![Tenzir Node v4.20](tenzir-node-v4.20.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.20.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Under-the-hood Improvements\\n\\nIn this release, we\'ve focused on improvements that will enable upcoming\\nfeatures of the Tenzir Platform.\\n\\n### Package Management\\n\\nWe\'ve enhanced package management in the node, enabling seamless package\\nupgrades on [app.tenzir.com](https://app.tenzir.com) without the need to\\nuninstall them first. This improvement sets the stage for smoother and more\\nefficient updates in the future.\\n\\n:::tip Slack Integration\\nIntroducing the new _Send to Slack_ package! Now, sending a message to a Slack\\nchannel from a Tenzir Node is as simple as publishing events on the `slack`\\ntopic:\\n\\n```tql\\n// tql2\\nfrom {\\n  message: \\"Hello, world!\\"\\n}\\npublish \\"slack\\"\\n```\\n:::\\n\\n### Caching\\n\\nWe\'re also happy to unveil the experimental `cache` operator. This operator\\ncreates an in-memory cache of events, allowing subsequent uses of the same cache\\nto skip running the pipeline leading up to it. This enhancement promises to\\ndrive significant improvements to the Explorer and Dashboards, making data\\nexploration more interactive and efficient.\\n\\n## Stabilizing the `metrics` Operator\\n\\nSince the introduction of the `metrics` operator in Tenzir Node v4.8, our goal\\nhas been to replace the old metrics subsystem with a new, pipeline-first\\napproach that enhances our dashboards. With the release of `metrics rebuild`,\\nwe\'ve successfully ported all metrics to the new system. The old metrics\\nsubsystem configured under `tenzir.metrics` in the configuration file is now\\nobsolete.\\n\\n:::info Migrating to the `metrics` operator\\nIf you\'re still using the old metrics subsystem, be sure to check out our guide\\non [collecting metrics](/usage/collect-metrics) and the [`metrics` operator\\nreference](/tql2/operators/metrics), which includes a comprehensive list of\\navailable metrics and their values.\\n:::\\n\\n## Progress on TQL2\\n\\nWe\'ve made significant strides in enhancing TQL2 over the past month. New\\nfeatures include string classification and transformation\\n(`<string>.to_upper()`, `<string>.is_lower()`), computing CommunityID flow\\nhashes, indexing into lists, and accessing secrets and environment variables\\n(`secret(<string>)`, `env(<string>)`), among others.\\n\\n## Other Changes\\n\\nThis release also includes numerous small bug fixes and additional\\nunder-the-hood improvements. For a detailed list of changes, be sure to check\\nout the [changelog][changelog].\\n\\n:::info Talk to us\\nJoin us for our office hours every second Tuesday at 8 AM EST / 11 AM EST / 5 PM\\nCET / 9:30 PM IST on [our Discord server][discord]. Whether you have ideas for\\nnew packages, want to preview our plans for them in the app, or have an idea\\nyou\'d like to discuss with us\u2014come chat with us!\\n:::\\n\\n[discord]: /discord  \\n[changelog]: /changelog#v4200"},{"id":"/tenzir-v4.19","metadata":{"permalink":"/releases/tenzir-v4.19","source":"@site/releases/tenzir-v4.19/index.md","title":"Tenzir v4.19","description":"Tenzir v4.19 now supports installing pipelines and contexts","date":"2024-07-26T00:00:00.000Z","formattedDate":"July 26, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"packages","permalink":"/releases/tags/packages"},{"label":"python","permalink":"/releases/tags/python"}],"readingTime":5.54,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"Tenzir v4.19","authors":["lava"],"date":"2024-07-26T00:00:00.000Z","tags":["release","packages","python"],"comments":true},"prevItem":{"title":"Tenzir Node v4.20","permalink":"/releases/tenzir-node-v4.20"},"nextItem":{"title":"Tenzir v4.18","permalink":"/releases/tenzir-v4.18"}},"content":"[Tenzir v4.19][github-release] now supports installing pipelines and contexts\\ntogether in packages, an all-new mechanism that makes installing integrations\\neasier than before.\\n\\n![Tenzir v4.19](tenzir-v4.19.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.19.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Packages\\n\\nPackages are the evolution of [Pipelines as Code][pipelines-as-code]. The idea\\nis simple: Take a set of pipelines and contexts that thematically belong\\ntogether, and deploy them together in one unit.\\n\\nInstalling a package is as simple as running this pipeline:\\n\\n```text {0} title=\\"Install a package from a file\\"\\nfrom path/to/package.yaml\\n| package add\\n```\\n\\nThis leverages the `package` operator, which has two modes of operation:\\n`package add` and `package remove`.\\n\\nTo list all installed packages, run `show packages`. Listing pipelines or\\ncontexts with `show pipelines` and `show contexts` contains an additional\\n`package` field to identify pipelines and packages installed through a package.\\n\\nIf you prefer infrastructure as code for your deployments, you can install any\\npackage into `<config-dir>/package/<package-name>/package.yaml`, which the node\\nreads when starting up.\\n\\nLet\'s walk through this by writing a package that offers a neat integration with\\nthe [Feodo Tracker Blocklist](https://feodotracker.abuse.ch/blocklist/) by\\nintegrating the data into a context.\\n\\nWe start our package by assigning some metadata:\\n\\n```yaml {0} title=\\"feodo/package.yaml [1/5]\\"\\nid: feodo\\nname: Feodo Abuse Blocklist\\nauthor: Tenzir\\nauthor_icon: https://github.com/tenzir.png\\npackage_icon: null\\ndescription: |\\n  Feodo Tracker is a project of abuse.ch with the goal of sharing botnet C&C\\n  servers associated with Dridex, Emotet (aka Heodo), TrickBot, QakBot (aka\\n  QuakBot / Qbot) and BazarLoader (aka BazarBackdoor). It offers various\\n  blocklists, helping network owners to protect their users from Dridex and\\n  Emotet/Heodo.\\n```\\n\\nEvery package must have a unique identifier. We recommend setting the package\\nname, description and author, and we also recommend setting an author and a\\npackage icon where possible.\\n\\nPackages may define inputs, which are user-defined variables that can\\nbe referenced in pipeline and context definitions. For this package,\\nwe don\'t define any inputs:\\n\\n```yaml {0} title=\\"feodo/package.yaml [2/5]\\"\\ninputs: {}\\n```\\n\\nPackages may define any number of contexts. For our Feodo Abuse Blocklist\\npackage we\'ll define a context named `feodo` as a Lookup Table. We recommend\\nwriting a description for every context.\\n\\n```yaml {0} title=\\"feodo/package.yaml [3/5]\\"\\ncontexts:\\n  feodo:\\n    type: lookup-table\\n    description: |\\n      A lookup table that contains the elements of the feodo IP blocklist.\\n```\\n\\nPackages may define any number of pipelines. These pipelines get automatically\\nstarted when the package is installed. For our example, let\'s add a pipeline\\nthat ensures that our `feodo` context is continuously updated:\\n\\n```yaml {0} title=\\"feodo/package.yaml [4/5]\\"\\npipelines:\\n  update-context:\\n    name: Update Feodo Context\\n    description: |\\n      Periodically refresh the Feodo lookup-table context.\\n    definition: |\\n      every 1 hour from https://feodotracker.abuse.ch/downloads/ipblocklist_aggressive.csv read csv --allow-comments\\n      | context update feodo --key dst_ip\\n```\\n\\nThe format for pipelines matches the format for [Pipelines as\\nCode][pipelines-as-code].\\n\\nLastly, we recommend adding snippets to your package that show how to use\\nit:\\n\\n```yaml {0} title=\\"feodo/package.yaml [5/5]\\"\\nsnippets:\\n  - name: Match historical and live data against the `feodo` context\\n    description: |\\n      Find persisted events that have an IP address matching the `feodo`\\n      context.\\n    definition: |\\n      lookup feodo --field :ip\\n  - name: Visualize successful lookups with the `feodo` context in the last week\\n    description: |\\n      Creates a stacked area chart that shows the number of hourly hits of\\n      pipelines using the `lookup` operator with the `feodo` context.\\n    definition: |\\n      metrics lookup\\n      | where context == \\"feodo\\"\\n      | where timestamp > 7d ago\\n      | summarize retro_hits=sum(retro.hits), live_hits=sum(live.hits) by timestamp resolution 1h\\n      | sort timestamp\\n      | chart area --position stacked\\n```\\n\\nThat\'s it! Our own package, all done and wrapped up.\\n\\n[pipelines-as-code]: /next/usage/run-pipelines#as-code\\n[feodotracker-blocklist]: https://feodotracker.abuse.ch/blocklist\\n\\n## Improving the Python Operator\\n\\nThe `python` operator got a revamp. It now relies on the excellent\\n[uv](https://github.com/astral-sh/uv) package installer, rather than\\n`python-pip`, reducing startup time significantly.\\n\\nIf your installation relies on a custom `pip.conf` file, we recommend [migrating\\nto a `uv.toml` configuration file](https://github.com/astral-sh/uv/issues/1404).\\n\\nBecause of the reduced startup time, the operator no longer shares virtual\\nenvironments between pipelines. This means that on every start of your pipeline,\\nwe will ensure that the most recent versions of all packages are installed.\\n\\n:::tip Clean up old virtual environments\\nPrevious versions of the operator did not clean up virtual environments on their\\nown. If you installed Tenzir on bare metal, we recommend removing old virtual\\nenvironments manually. They are located at\\n`<cache-directory>/tenzir/python/venvs`, which will be\\n`/var/cache/tenzir/python/venvs` for most deployments.\\n:::\\n\\n## Surviving Spikes with the Buffer Operator\\n\\nThe `buffer` operator is a new addition that makes it possible to break back\\npressure in pipelines.\\n\\n:::info What is back pressure?\\nOperators in a pipeline communicate in both directions: The operator\'s output is\\nsent downstream to the next operator, and an operator can emit demand to the\\nupstream operator. Demand controls whether an operator gets scheduled\u2014that is,\\nan operator that has no demand to produce any output just doesn\'t run at all\\nanymore. This mechanism is called back pressure.\\n:::\\n\\nMost of the time, back pressure is very useful: It makes it so that your\\npipeline does no unnecessary work, and so that events do not pile up in memory\\nwhen an operator is slow.\\n\\nHowever, some data sources really do not like to be throttled. For example, when\\nreading from a UDP connection, throttling the source effectively means losing\\nevents.\\n\\nThe `buffer` operator is a special operator that doesn\'t quite follow the rules\\nother operators need to abide by. The operator has two policies: `block` and\\n`drop`. With the `block` policy, the operator stops emitting demand upstream\\nonly when the buffer is full. With the `drop` policy, the operator never stops\\nemitting demand upstream, but then drops events if the buffer is full.\\n\\nFor example, let\'s say we acquire syslog messages with a very high speed over UDP:\\n\\n```text {0} title=\\"Acquire data from syslog, buffering up to 1M events\\"\\nfrom udp://localhost:514 read syslog\\n| buffer 1M --policy drop\\n| \u2026\\n```\\n\\nThe `buffer` operator emits metrics, so now we can also set up a chart that\\nmonitors our buffer utilization:\\n\\n```text {0}\\nmetrics buffer\\n| where timestamp > 1 day ago\\n// substitute the id of the syslog pipeline here\\n| where pipeline_id == \\"<pipeline-id>\\"\\n| summarize used=max(used), free=min(free) by timestamp resolution 15min\\n| sort timestamp\\n| chart area --position stacked\\n```\\n\\n## Other Changes\\n\\nAs usual, the [changelog][changelog] contains a full list of features, changes,\\nand bug fixes in this release.\\n\\nEvery second Tuesday at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we hold\\noffice hours in [our Discord server][discord]. Whether you have ideas for\\npackages, want to see a preview of what we plan to do with them in the app, an\\nidea that you\'d like to discuss with us\u2014come join and have a chat with us!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4190"},{"id":"/tenzir-v4.18","metadata":{"permalink":"/releases/tenzir-v4.18","source":"@site/releases/tenzir-v4.18/index.md","title":"Tenzir v4.18","description":"Monitoring Tenzir nodes is easier than before with [Tenzir","date":"2024-07-11T00:00:00.000Z","formattedDate":"July 11, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"health-metrics","permalink":"/releases/tags/health-metrics"},{"label":"tql2","permalink":"/releases/tags/tql-2"}],"readingTime":2.7,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.18","authors":["dominiklohmann"],"date":"2024-07-11T00:00:00.000Z","tags":["release","health-metrics","tql2"],"comments":true},"prevItem":{"title":"Tenzir v4.19","permalink":"/releases/tenzir-v4.19"},"nextItem":{"title":"Tenzir v4.17","permalink":"/releases/tenzir-v4.17"}},"content":"Monitoring Tenzir nodes is easier than before with [Tenzir\\nv4.18][github-release] and its new health metrics.\\n\\n![Tenzir v4.18](tenzir-v4.18.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.18.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Monitor Node Health With Metrics\\n\\nThe `metrics` operator now additionally takes a positional argument for the\\nmetrics name. Now, `metrics cpu` is equivalent to `metrics | where #schema ==\\n\\"tenzir.metrics.cpu\\"`. That\'s a lot easier to write!\\n\\nTenzir nodes now collect more metrics than before. In particular, the `import`,\\n`export`, `publish`, `subscribe`, `enrich`, and `lookup` operators now emit\\nmetrics, and nodes additionally collect `api` metrics for every API call and\\n`platform` metrics that record the connection status to the Tenzir Platform from\\nthe node\'s perspective. These are best explained on examples:\\n\\n```text {0} title=\\"Show imported events per schema and day for the last month\\"\\nmetrics import\\n| where timestamp > 30d ago\\n| summarize events=sum(events) by timestamp, schema resolution 1d\\n| sort timestamp, schema\\n```\\n\\n```text {0} title=\\"Calculate the rate of context hits for the context \'iocs\'\\"\\nmetrics enrich\\n| where context == \\"iocs\\"\\n| summarize events=sum(events), hits=sum(hits)\\n| python \'self.rate = self.hits / self.events\'\\n```\\n\\n```text {0} title=\\"Show the most commonly used APIs in the last hour\\"\\nmetrics api\\n| where timestamp > 1 day ago\\n| top path\\n```\\n\\nThese are just three examples to get you started with monitoring your node.\\nWe\'re planning to make more data available from more operators in the future,\\nand have built a new framework that makes emitting custom metrics from operators\\na breeze.\\n\\n:::tip Want to learn more?\\nTake a look at the [`metrics` operator\'s\\ndocumentation](/tql2/operators/metrics), which details all the available metrics\\nand their schema.\\n:::\\n\\n## Play With TQL2\\n\\nAt this point it\'s an open secret that we\'re working working on a major revamp\\nto the Tenzir Query Language. We still have quite a way to go before\\nmaking the next version the new default, but we\'re excited to announce that as\\nof Tenzir v4.18, it is now possible to try it out without being a Tenzir developer.\\n\\nTo use TQL2 on [app.tenzir.com](https://app.tenzir.com), for pipelines\\nconfigured in the `tenzir.yaml` configuration file, or through the API, start\\nthe pipeline with a `// experimental-tql2` comment. For example:\\n\\n```\\n// experimental-tql2\\nexport live=true\\nwhere @name == \\"zeek.http\\"\\nif method == \\"GET\\" {\\n  where request_body.len > 0\\n  publish \\"weird\\"\\n} else {\\n  where uri.ends_with(\\".exe\\") and status_code < 400\\n  publish \\"suspicious\\"\\n}\\n```\\n\\nUse `tenzir --tql2 <pipeline>` to use TQL2 with the `tenzir` binary on the\\ncommand-line.\\n\\n:::warning Experimental\\nMany things in TQL2 are not implemented, not documented, or not yet working\\ncorrectly. We are still making breaking changes to it, so we would like to ask\\nyou not to use it in production.\\n\\nGot feedback? Head over to the `#developers` channel in our [community\\nDiscord](/discord).\\n:::\\n\\n## Other Changes\\n\\nAs usual, the [changelog][changelog] contains a full list of features, changes,\\nand bug fixes in this release.\\n\\nEvery second Tuesday at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we hold\\noffice hours in [our Discord server][discord]. Whether you want to participate\\nin the TQL2 discussion, have ideas for further metrics, feedback of any kind, a\\nwild idea that you\'d like to bring up, or just want to hang out\u2014come join us!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4180"},{"id":"/tenzir-v4.17","metadata":{"permalink":"/releases/tenzir-v4.17","source":"@site/releases/tenzir-v4.17/index.md","title":"Tenzir v4.17","description":"The new Tenzir v4.17 brings an integration with Azure Log","date":"2024-06-21T00:00:00.000Z","formattedDate":"June 21, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"azure-log-analytics","permalink":"/releases/tags/azure-log-analytics"},{"label":"lookup-table","permalink":"/releases/tags/lookup-table"}],"readingTime":2.945,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.17","authors":["dominiklohmann"],"date":"2024-06-21T00:00:00.000Z","tags":["release","azure-log-analytics","lookup-table"],"comments":true},"prevItem":{"title":"Tenzir v4.18","permalink":"/releases/tenzir-v4.18"},"nextItem":{"title":"Tenzir v4.16","permalink":"/releases/tenzir-v4.16"}},"content":"The new [Tenzir v4.17][github-release] brings an integration with Azure Log\\nAnalytics and adds support for expiring entries in lookup tables.\\n\\n![Tenzir v4.17](tenzir-v4.17.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.17.1\\n\\n\x3c!-- truncate --\x3e\\n\\n## Send Events to Azure Log Analytics\\n\\nThe shining star of Tenzir v4.17 is the new `azure-log-analytics` sink\\noperator, which sends events to [Log Analytics in Azure\\nMonitor][log-analytics-overview].\\n\\n[log-analytics-overview]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-overview\\n\\n## Lookup Table Timeouts\\n\\nThe `context update` operator gained two new options when used together with\\nlookup table contexts: `--create-timeout <duration>` and\\n`--update-timeout <duration>`.\\n\\nBoth new options cause individual events to expire in the lookup table. Create\\ntimeouts specify the time after which entries in the lookup table expire, and\\nupdate timeouts specify the time after which entries in the lookup table expire\\nwhen they\'re not accessed.\\n\\nThe following example adds lookup table entries that expire after a week at the\\nlatest, or when they were not accessed for a day, whichever comes first:\\n\\n```\\n\u2026\\n| context update my-lookup-table --create-timeout 1w --update-timeout 1d\\n```\\n\\n## Print Individual Fields in Events\\n\\nThe `print <field> <format>` operator is the counterpart to the `parse`\\noperator. Given a field of type record within an event, it replaces it with a\\nstring containing the formatted representation. This is best explained on an\\nexample:\\n\\n```json {0} title=\\"Input\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": {\\n    \\"pkts_toserver\\": 1,\\n    \\"pkts_toclient\\": 0,\\n    \\"bytes_toserver\\": 54,\\n    \\"bytes_toclient\\": 0\\n  }\\n}\\n```\\n\\n```text {0} title=\\"Render the field flow as CSV\\"\\nfrom input.json\\n| print flow csv --no-header\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": \\"1,0,54,0\\"\\n}\\n```\\n\\nThe `print` operator is especially useful when working with third-party APIs\\nthat often do not support deeply nested data structures in their data model.\\n\\n## Changes to Built-in Type Aliases\\n\\nWe removed the built-in `timestamp` and `port` type aliases for `time` and\\n`uint64`, respectively.\\n\\nThese types were relics of Tenzir\'s past, when onboarding data required\\nspecifying a schema explicitly. Back then, we started using type aliases to\\nfurther categorize parts of the onboarded data. With Tenzir today, automatic\\nschema inference is the modus operandi. This caused data that was imported with\\na schema to sometimes use a `timestamp` type, but all automatically inferred\\ndata used the underlying `time` type. This caused issues down the line, because\\noperators like `summarize` by design do not group fields together with distinct\\ntypes. To users, this showed as duplicate values that were supposed to be\\ngrouped by in summarized results.\\n\\n:::warning Required Configuration Changes\\nIf you have custom schemas installed in `/opt/tenzir/etc/tenzir/schemas` or\\n`~/.config/tenzir/schemas`, you will need to adapt them in one of two ways:\\n1. Replace all `timestamp` types with `time` and all `port` types with `uint64`\\n   (recommended).\\n2. Add the aliases back to your own schemas by defining `type timestamp = time`\\n   and `type port = uint64`, respectively.\\n:::\\n\\n## Edit Pipelines in the Tenzir Platform\\n\\nYou can now change pipelines on [app.tenzir.com][tenzir-app] more quickly.\\nSimply click on any pipeline on the overview page to open a detailed view. In\\nthis view, you can directly edit the definition or options. The new action menu\\nallows you to quickly start, pause, stop, duplicate, or delete a pipeline.\\n\\n## Other Changes\\n\\nFor a full list of changes in this release, please check our\\n[changelog][changelog], and play with the new changes at\\n[app.tenzir.com][tenzir-app].\\n\\nEvery second Tuesday at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we hold\\noffice hours in [our Discord server][discord]. Join us next week for an\\nexclusive sneak peek with our designer into upcoming changes to\\n[app.tenzir.com][tenzir-app]!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4170\\n[tenzir-app]: https://app.tenzir.com"},{"id":"/tenzir-v4.16","metadata":{"permalink":"/releases/tenzir-v4.16","source":"@site/releases/tenzir-v4.16/index.md","title":"Tenzir v4.16","description":"Pipelines now connect more flexibly than ever before with [Tenzir","date":"2024-06-05T00:00:00.000Z","formattedDate":"June 5, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"context","permalink":"/releases/tags/context"},{"label":"publish","permalink":"/releases/tags/publish"},{"label":"subscribe","permalink":"/releases/tags/subscribe"}],"readingTime":2.445,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.16","authors":["dominiklohmann"],"date":"2024-06-05T00:00:00.000Z","tags":["release","context","publish","subscribe","context"],"comments":true},"prevItem":{"title":"Tenzir v4.17","permalink":"/releases/tenzir-v4.17"},"nextItem":{"title":"Tenzir v4.15","permalink":"/releases/tenzir-v4.15"}},"content":"Pipelines now connect more flexibly than ever before with [Tenzir\\nv4.16](https://github.com/tenzir/tenzir/releases/tag/v4.16.0) and its upgraded\\n[`publish`](/tql2/operators/publish) and\\n[`subscribe`](/tql2/operators/subscribe) operators.\\n\\n![Tenzir v4.16](tenzir-v4.16.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Multi-Producer Multi-Consumer\\n\\nThe introduction of the `publish` operator with Tenzir v4.12 enabled\\nsplit-routing of events. We frequently saw users write pipelines like this:\\n\\n```text {0} title=\\"Pipeline 1: Publish alerts\\"\\n\u2026\\n| publish alerts\\n```\\n\\n```text {0} title=\\"Pipeline 2: Save alerts with a high risk score to Splunk\\"\\nsubscribe alerts\\n| where risk_score >= 90\\n| fluent-bit splunk \u2026\\n```\\n\\n```text {0} title=\\"Pipeline 3: Save all alerts to S3 for later reference\\"\\nsubscribe alerts\\n| to s3://bucket/alerts.json.zst write json --compact-output\\n```\\n\\nThis approach, however, fell apart as soon as another data source tried to\\npublish to the topic `alerts`. Trying to do so just displayed an  error.\\nFundamentally, the `publish` and `subscribe` operators were single-producer,\\nmulti-consumer (SPMC).\\n\\nWith Tenzir v4.16, the `publish` operator\'s topics no longer have to be unique,\\nmaking it possible to easily merge data flows back together:\\n\\n```text {0} title=\\"Pipeline 4: Publish further alerts\\"\\n\u2026\\n| publish alerts\\n```\\n\\nThis seemingly small change makes Tenzir\'s pipelines more flexible than ever\\nbefore. Now, you can write modular pipelines for individual parts of your use\\ncases.\\n\\nFor example, imagine that you have a pipeline that persists events to \\"cold\\nstorage\\" by writing them to S3 in a strongly compressed format:\\n\\n```text\\nsubscribe to-cold-storage\\n| to s3://bucket/cold_storage.json.zst write json --compact-output\\n```\\n\\nNow, you can re-use this building block easily from any pipeline:\\n\\n```text\\n\u2026\\n| publish to-cold-storage\\n```\\n\\n## Aggregation Functions for Percentiles\\n\\nWe\'ve added new aggregation functions for calculating percentiles: `p99`, `p95`,\\n`p90`, `p75`, and `p50`. For example, to plot the 99th percentile of the number\\nof packets sent per flow, you can now write:\\n\\n```text\\n\u2026\\n| where #schema == \\"suricata.flow\\"\\n| summarize p99(flow.pkts_toserver)\\n```\\n\\nWe\'ve additionally renamed the `approximate_median` function to `median`. We\\nfound the longer name to be unintuitive and cumbersome to write, so we decided\\nto simplify it.\\n\\n## Erase Lookup Table Entries\\n\\nA user recently showed me this abomination consisting of three pipelines:\\n\\n```text {0} title=\\"Pipeline 1: Save lookup table\\"\\ncontext inspect my-lookup-table\\n| to /tmp/my-lookup-table.json\\n```\\n\\n```text {0} title=\\"Pipeline 2: Wipe the lookup table\\"\\ncontext reset my-lookup-table\\n```\\n\\n```text {0} title=\\"Pipeline 3: Restore the lookup table without some keys\\"\\nfrom /tmp/my-lookup-table.json\\n| yield value\\n| where key !in [\\"foo\\", \\"bar\\", \\"baz\\"]\\n| context update my-lookup-table\\n```\\n\\nThis is a lot of work just to erase three values from a lookup table. With\\nTenzir v4.16, you can now erase entries from a lookup table directly.\\n\\n```text\\ncontext inspect my-lookup-table\\n| yield value\\n| where key in [\\"foo\\", \\"bar\\", \\"baz\\"]\\n| context update my-lookup-table --erase\\n```\\n\\n## Other Changes\\n\\nFor a full list of changes in this release, please check our\\n[changelog](/changelog#v4160), and play with the new changes at\\n[app.tenzir.com](https://app.tenzir.com).\\n\\nAre you using `publish` and `subscribe` to connect your pipelines already? We\'d\\nlike to hear your thoughts! Join [our Discord server](/discord)."},{"id":"/tenzir-v4.15","metadata":{"permalink":"/releases/tenzir-v4.15","source":"@site/releases/tenzir-v4.15/index.md","title":"Tenzir v4.15","description":"Tenzir v4.15 is now","date":"2024-05-31T00:00:00.000Z","formattedDate":"May 31, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"context","permalink":"/releases/tags/context"},{"label":"rpm","permalink":"/releases/tags/rpm"}],"readingTime":2.025,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.15","authors":["dominiklohmann"],"date":"2024-05-31T00:00:00.000Z","tags":["release","context","rpm"],"comments":true},"prevItem":{"title":"Tenzir v4.16","permalink":"/releases/tenzir-v4.16"},"nextItem":{"title":"Tenzir v4.14","permalink":"/releases/tenzir-v4.14"}},"content":"[Tenzir v4.15](https://github.com/tenzir/tenzir/releases/tag/v4.15.0) is now\\navailable for download. The Tenzir Platform now shows live-updating pipeline\\nactivity, and the Tenzir Node has improved support for subnet keys in lookup\\ntables, and installs natively for RedHat Linux and its derivatives.\\n\\n![Tenzir v4.15](tenzir-v4.15.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Pipeline Activity\\n\\nThe pipeline overview page on [app.tenzir.com](https://app.tenzir.com) now\\nfeatures a live-updating view of recent pipeline activity.\\n\\nGrab some popcorn, sit back, and watch the data flow! \ud83c\udf7f\\n\\n:::info Join the conversation!\\nWe plan to make pipelines easier to introspect and to manage. Do you have ideas,\\nor want to take a sneak peek at what\'s coming up?\\n\\nEvery Tuesday at at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we\'re hosting\\n[Office Hours](/archive/introducing-office-hours) in our [Discord](/discord). Come\\njoin the discussion.\\n:::\\n\\n## RedHat Linux Support\\n\\nInstalling a Tenzir Node is now easier than ever on RedHat Linux and its\\nderivatives. As of this release, it is now possible to install Tenzir as an RPM\\npackage. Just run the installer script to get started:\\n\\n```bash\\ncurl https://get.tenzir.app | sh\\n```\\n\\n## Subnet Keys in Lookup Tables\\n\\nThe `lookup-table` context now handles subnet keys correctly. Lookups with an IP\\naddress or subnet value now match if the key is within any of the subnets used\\nas keys. If multiple subnets match, then the best match is returned.\\n\\n:::info Want to learn more?\\nWe wrote a new user guide about using subnet keys in lookup tables: [Enrich with\\nNetwork Inventory](/next/usage/enrich-with-network-inventory).\\n:::\\n\\n## Sort by Multiple Fields\\n\\nThe `sort` operator now supports sorting by multiple fields, specified in order\\nof precedence. The following two examples have the exact same behavior, making\\nsorting by multiple fields much easier:\\n\\n```text {0} title=\\"Before: sorting was limited to one field at a time\\"\\nsort baz\\n| sort --stable bar\\n| sort --stable foo\\n```\\n\\n```text {0} title=\\"After: sorting now supports multiple fields\\"\\nsort foo, bar, baz\\n```\\n\\n## Edit Pipelines\\n\\nThe `/pipeline/update` API endpoint now supports updating the definition of a\\npipeline. This functionality will soon be available in the app for nodes that\\nsupport it. Gone are the times where editing a pipeline required copying its\\ndefinition, redeploying it, deleting the old and starting the new version.\\n\\n## Other Changes\\n\\nFor a full list of enhancements, adjustments, and bug fixes in this release,\\nplease check our [changelog](/changelog#v4150).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\njoin the conversation on [our Discord server](/discord)."},{"id":"/tenzir-v4.14","metadata":{"permalink":"/releases/tenzir-v4.14","source":"@site/releases/tenzir-v4.14/index.md","title":"Tenzir v4.14","description":"Introducing [Tenzir","date":"2024-05-17T00:00:00.000Z","formattedDate":"May 17, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"slice","permalink":"/releases/tags/slice"},{"label":"summarize","permalink":"/releases/tags/summarize"},{"label":"streaming-aggregation","permalink":"/releases/tags/streaming-aggregation"}],"readingTime":1.42,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir v4.14","authors":["IyeOnline"],"date":"2024-05-17T00:00:00.000Z","tags":["release","slice","summarize","streaming-aggregation"],"comments":true},"prevItem":{"title":"Tenzir v4.15","permalink":"/releases/tenzir-v4.15"},"nextItem":{"title":"Tenzir v4.13","permalink":"/releases/tenzir-v4.13"}},"content":"Introducing [Tenzir\\nv4.14](https://github.com/tenzir/tenzir/releases/tag/v4.14.0): A major update to\\nthe `summarize` operator with new aggreagtion functions, and support for slicing\\nwith strides.\\n\\n![Tenzir v4.14](tenzir-v4.14.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introducing Streaming Aggregation\\n\\nOur `summarize` operator just got a whole lot smarter! With new `timeout` and\\n`update-timeout` options, you can now perform streaming aggregations. These\\nsettings determine how long a bucket remains active based on when the first and\\nlast events arrive. `timeout` keeps events for a specified duration, while\\n`update-timeout` helps finalize buckets sooner when grouped events arrive\\nquickly.\\n\\n## New Statistical Aggregation Functions\\n\\nGet ready to enhance your data analysis with four new aggregation functions:\\n\\n- `mean`: Calculates the average of grouped numeric values.\\n- `approximate_median`: Uses the t-digest algorithm to find an approximate\\n  median for grouped numbers.\\n- `stddev`: Computes the standard deviation of grouped numeric values.\\n- `variance`: Calculates the variance within the grouped data.\\n\\nThese functions will help you gain more insights and precision in your data\\nsummaries.\\n\\n## Enhanced Slicing with Strides\\n\\nThe `slice` operator now has a more flexible argument format: `<begin>:<end>`.\\nHere are some examples:\\n\\n- `slice 10:`: Skips the first ten events.\\n- `slice 10:20`: Includes events from 10 to 19.\\n- `slice :-10`: Omits the last ten events.\\n\\nWe\'ve also added support for strides. Use `slice <begin>:<end>:<stride>` to\\nspecify steps between events. Want to reverse the event order? The new `reverse`\\noperator does just that, equivalent to `slice ::-1`.\\n\\n## More Updates and Improvements\\n\\nFor detailed information on all the enhancements, adjustments, and fixes in this\\nrelease, check out our [changelog](/changelog#v4140).\\n\\nDive into the new features at [app.tenzir.com](https://app.tenzir.com), and be\\nsure to join the conversation on [our Discord server](/discord).\\n\\nWe hope you enjoy the enhancements in Tenzir v4.14!"},{"id":"/tenzir-v4.13","metadata":{"permalink":"/releases/tenzir-v4.13","source":"@site/releases/tenzir-v4.13/index.md","title":"Tenzir v4.13","description":"We\'ve just released [Tenzir","date":"2024-05-10T00:00:00.000Z","formattedDate":"May 10, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"maintenance","permalink":"/releases/tags/maintenance"},{"label":"performance","permalink":"/releases/tags/performance"},{"label":"leef","permalink":"/releases/tags/leef"},{"label":"syslog","permalink":"/releases/tags/syslog"}],"readingTime":2.125,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.13","authors":["dominiklohmann"],"date":"2024-05-10T00:00:00.000Z","tags":["release","maintenance","performance","leef","syslog"],"comments":true},"prevItem":{"title":"Tenzir v4.14","permalink":"/releases/tenzir-v4.14"},"nextItem":{"title":"Tenzir v4.12","permalink":"/releases/tenzir-v4.12"}},"content":"We\'ve just released [Tenzir\\nv4.13](https://github.com/tenzir/tenzir/releases/tag/v4.13.0), a release\\nfocusing on stability and incremental improvements over the feature-packed past\\nreleases.\\n\\n![Tenzir v4.13](tenzir-v4.13.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Acquire LEEF Over Syslog\\n\\nTenzir now speaks LEEF out of the box. The [Log Event Extended Format\\n(LEEF)][leef] is an event representation popularized by IBM QRadar. Many tools\\nsend LEEF over [Syslog](/formats/syslog).\\n\\n[leef]: https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components\\n\\nLEEF is a line-based format and every line begins with a *header* that is\\nfollowed by *attributes* in the form of key-value pairs.\\n\\nLEEF v1.0 defines 5 header fields and LEEF v2.0 has an additional field to\\ncustomize the key-value pair separator, which can be a single character or the\\nhex value prefixed by `0x` or `x`:\\n\\n```\\nLEEF:1.0|Vendor|Product|Version|EventID|\\nLEEF:2.0|Vendor|Product|Version|EventID|DelimiterCharacter|\\n```\\n\\nFor LEEF v1.0, the tab (`\\\\t`) character is hard-coded as attribute separator.\\n\\nHere are some real-world LEEF events:\\n\\n```\\nLEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1\\tdst=2.10.20.20\\tspt=1200\\nLEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=10.0.1.8^dst=10.0.0.5^sev=5^srcPort=81^dstPort=21\\n```\\n\\nTenzir translates the event attributes into a nested record, where the key-value\\npairs map to record fields. Here is an example of the parsed events from above:\\n\\n```json\\n{\\n  \\"leef_version\\": \\"1.0\\",\\n  \\"vendor\\": \\"Microsoft\\",\\n  \\"product_name\\": \\"MSExchange\\",\\n  \\"product_version\\": \\"2016\\",\\n  \\"attributes\\": {\\n    \\"src\\": \\"10.50.1.1\\",\\n    \\"dst\\": \\"2.10.20.20\\",\\n    \\"spt\\": 1200,\\n  }\\n}\\n{\\n  \\"leef_version\\": \\"2.0\\",\\n  \\"vendor\\": \\"Lancope\\",\\n  \\"product_name\\": \\"StealthWatch\\",\\n  \\"product_version\\": \\"1.0\\",\\n  \\"attributes\\": {\\n    \\"src\\": \\"10.0.1.8\\",\\n    \\"dst\\": \\"10.0.0.5\\",\\n    \\"sev\\": 5,\\n    \\"srcPort\\": 81,\\n    \\"dstPort\\": 21\\n  }\\n}\\n```\\n\\nLEEF events typically transmit through Syslog as demonstrated here:\\n\\n```syslog\\n<12>Nov 21 13:44:35 LAPTOP-45Q5L6E5 Microsoft-Windows-Security-Mitigations[4340]: LEEF:2.0|Microsoft|Microsoft-Windows-Security-Mitigations|4.6.4640-trial|10|0x09|devTime=2019-11-21 \u2026 (truncated for brevity)\\n```\\n\\nWith Tenzir\'s [`parse`](/operators/parse) operator, parsing nested data\\nstructures like LEEF in Syslog becomes straightforward. For instance, the\\nfollowing pipeline reads Syslog containing LEEF over UDP:\\n\\n```\\nfrom udp://127.0.0.1:514 -n read syslog\\n| parse content leef\\n```\\n\\n```json\\n{\\n  \\"facility\\": 1,\\n  \\"severity\\": 4,\\n  \\"timestamp\\": \\"Nov 21 13:44:35\\",\\n  \\"hostname\\": \\"LAPTOP-45Q5L6E5\\",\\n  \\"app_name\\": \\"Microsoft-Windows-Security-Mitigations\\",\\n  \\"process_id\\": \\"4340\\",\\n  \\"content\\": {\\n    \\"leef_version\\": \\"2.0\\",\\n    \\"vendor\\": \\"Microsoft\\",\\n    \\"product_name\\": \\"Microsoft-Windows-Security-Mitigations\\",\\n    \\"product_version\\": \\"4.6.4640-trial\\",\\n    \\"attributes\\": {\\n      \\"devTime\\": \\"2019-11-21T13:44:35.000000\\",\\n      // \u2026 (truncated for brevity)\\n    }\\n  }\\n}\\n```\\n\\n## Cron Scheduling\\n\\nExpanding on our scheduling capabilities, Tenzir now includes a\\n[`cron`](https://docs.tenzir.com/next/language/operator-modifiers#cron) operator\\nmodifier enabling precise scheduling using [Crontab\\nsyntax](https://crontab.guru) instead of fixed intervals.\\n\\nFor example, the following pipeline does an API request every Sunday at 04:05 in\\nthe morning:\\n\\n```\\ncron \\"0 5 4 * * SUN\\" from https://example.com/api\\n```\\n\\n## Performance Improvements for Imports\\n\\nThe [`import`](/tql2/operators/import) operator introduces a slight delay (up to\\none second) in event handling to batch events by schema, substantially enhancing\\nperformance.\\n\\n![Import Reordering](import-reordering.excalidraw.svg)\\n\\n## Other Changes\\n\\nFor a full list of enhancements, adjustments, and bug fixes in this release,\\nplease check our [changelog](/changelog#v4130).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\njoin the conversation on [our Discord server](/discord)."},{"id":"/tenzir-v4.12","metadata":{"permalink":"/releases/tenzir-v4.12","source":"@site/releases/tenzir-v4.12/index.md","title":"Tenzir v4.12","description":"We are thrilled to announce Tenzir","date":"2024-04-24T00:00:00.000Z","formattedDate":"April 24, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"tcp","permalink":"/releases/tags/tcp"},{"label":"udp","permalink":"/releases/tags/udp"},{"label":"publish","permalink":"/releases/tags/publish"},{"label":"subscribe","permalink":"/releases/tags/subscribe"},{"label":"deduplicate","permalink":"/releases/tags/deduplicate"},{"label":"unroll","permalink":"/releases/tags/unroll"},{"label":"syslog","permalink":"/releases/tags/syslog"},{"label":"every","permalink":"/releases/tags/every"}],"readingTime":4.385,"hasTruncateMarker":true,"authors":[{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"}],"frontMatter":{"title":"Tenzir v4.12","authors":["jachris"],"date":"2024-04-24T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["release","tcp","udp","publish","subscribe","deduplicate","unroll","syslog","every"],"comments":true},"prevItem":{"title":"Tenzir v4.13","permalink":"/releases/tenzir-v4.13"},"nextItem":{"title":"Tenzir v4.11","permalink":"/releases/tenzir-v4.11"}},"content":"We are thrilled to announce Tenzir\\n[v4.12](https://github.com/tenzir/tenzir/releases/tag/v4.12.1), a feature-packed\\nrelease introducing numerous enhancements. Notable additions include list\\nunrolling, event deduplication, and the deployment of advanced pipeline\\narchitectures with publish-subscribe. We\'ve also added a download button,\\nextended support for UDP, and implemented many other refinements to improve your\\nexperience.\\n\\n![Tenzir v4.12](tenzir-v4.12.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Unroll and Deduplicate Events\\n\\nIn today\'s data-driven world, many data sources deliver information in the form\\nof lists/arrays. While seemingly simple, working with these lists can sometimes\\npose challenges, particularly when the data structure becomes intricate. Let\'s\\ntake the example of a connection summary stream in JSON format:\\n\\n```json\\n{\\n  \\"src\\": \\"192.0.2.1\\",\\n  \\"dst\\": [\\n    \\"192.0.2.143\\",\\n    \\"203.0.113.2\\"\\n  ]\\n}\\n// after 1min:\\n{\\n  \\"src\\": \\"192.0.2.1\\",\\n  \\"dst\\": [\\n    \\"203.0.113.2\\",\\n    \\"172.16.76.150\\",\\n    \\"192.0.2.143\\"\\n  ]\\n}\\n```\\n\\nTo overcome the hurdles of JSON list manipulation, we introduce the new\\n`unroll` operator, allowing the creation of an event for each item in the list.\\nLet\'s `unroll dst`:\\n\\n```json\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"192.0.2.143\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"203.0.113.2\\"}\\n// after 1min:\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"203.0.113.2\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"172.16.76.150\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"192.0.2.143\\"}\\n```\\n\\nThe data is now significantly easier to work with.\\n\\nDo you see the duplicate host pairs? Let\'s remove them with the new\\n`deduplicate` operator. Run `deduplicate src, dst --timeout 24h` to condense the\\nabove output to:\\n\\n```json\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"192.0.2.143\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"203.0.113.2\\"}\\n// after 1min:\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"172.16.76.150\\"}\\n```\\n\\nThe `--timeout` option is useful for controlling expiration of entries. In this\\nexample, if the same connection tuple doesn\'t come up within a 24h interval, the\\ncorresponding entry is removed from the operator\'s internal state.\\n\\nWe delved deeper into the power of the `deduplicate` operator in a [previous\\nblog post](/archive/reduce-cost-and-noise-with-deduplication).\\n\\nBuilding on this, the `every` operator (prominently featured in the [previous\\nrelease](tenzir-v4.11#execute-sources-on-a-schedule)) can now also accompany\\ntransformations and sinks. To illustrate, let\'s answer this question: \\"With how\\nmany new destinations did each device communicate in the last minute?\\"\\n\\nUsing `every 1min summarize num=count(.) by src`, we get:\\n\\n```json\\n{\\"src\\": \\"192.0.2.1\\", \\"num\\": 2}\\n// after 1min:\\n{\\"src\\": \\"192.0.2.1\\", \\"num\\": 1}\\n```\\n\\nIn summary, these transformations provide powerful in-band capabilities leading\\nto substantial data reduction, simplifying the analysis, and making data shaping\\nmore efficient.\\n\\n## Publish and Subscribe\\n\\nExciting are also the new `publish` and `subscribe` operators, which open up\\nendless possibilities for creating arbitrary dataflow topologies. For instance,\\nyou can set a publishing point within your data stream. It\'s as simple as `from\\ntcp://0.0.0.0:8000 | publish input`. This defines a channel `input` that you can\\nnow subscribe to with `subscribe`.\\n\\nLet\'s consider a case where we aim to route all alerts into Splunk, and\\nconcurrently import all other non-alert events into Tenzir\'s storage for further\\nanalysis and monitoring:\\n\\n``` title=\\"1st subscriber\\"\\nsubscribe input\\n| where alert == true\\n| to splunk\\n```\\n\\n``` title=\\"2nd subscriber\\"\\nsubscribe input\\n| where alert == false\\n| import\\n```\\n\\nHere, the first subscriber takes the events from the `input` channel where the\\nalert field is `true` and routes them to Splunk. In parallel, the second\\nsubscriber takes the remaining events with the alert field marked as `false` and\\nimports them into Tenzir\u2019s storage. Our new feature enables this precise,\\ndynamic routing, making data management more efficient and streamlined.\\n\\n## Contexts as Code\\n\\n[Tenzir v4.10](tenzir-v4.10) introduced introduced the ability to statically\\ndefine pipelines in Tenzir\'s configuration file: **Pipelines as Code (PaC)**.\\nThis release expands upon that capability by also allowing static configuration\\nof contexts.\\n\\n```yaml title=\\"tenzir.yaml\\"\\ntenzir:\\n  contexts:\\n    # A unique name for the context that\'s used in the context, enrich, and\\n    # lookup operators to refer to the context.\\n    indicators:\\n      # The type of the context (e.g., `lookup-table`, `geoip`, ...).\\n      type: bloom-filter\\n      # Arguments for creating the context, as described by the documentation of\\n      # the chosen context type.\\n      arguments:\\n        capacity: 1B\\n        fp-probability: 0.001\\n```\\n\\nOn a related note: The operators `context create`, `context reset`,\\n`context update`, and `context load` were changed to no longer return\\ninformation about the associated context. Instead, they now act as a sink.\\n\\n## Download Button\\n\\nEver wanted to just save the output from Explorer to your computer? With the new\\n**Download** button, you can do precisely that:\\n\\n![Download Button](download-button.png)\\n\\nJust select one of the available formats and you\'re good to go!\\n\\n## Other Changes\\n\\n- There\'s a new `udp` connector for sending and receiving UDP datagrams.\\n  Finally, you can now receive Syslog natively.\\n- Speaking of Syslog: we\'ve enhanced our parser to be *multi-line*. In case the\\n  next line isn\'t a valid Syslog message by itself, we interpret it as the\\n  continuation of the previous message.\\n- The `tcp` loader now accepts multiple connections in parallel, e.g., when used\\n  as `from tcp://127.0.0.1:8000 read json`.\\n- We\'ve massively improved performance of our Parquet and Feather formats for\\n  large files. For writing, they now both support streaming row groups and\\n  record batches, respectively, and for reading Feather now supports streaming\\n  via the Arrow IPC format as well. This comes in handy for those of you working\\n  in the Apache Arrow ecosystem and seeking seamless interoperability without\\n  loss of rich typing.\\n\\nAs usual, the complete list of bug fixes, adjustments, and enhancements\\ndelivered with this version can be found in our [changelog](/changelog#v4120).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\nchat with us at [our Discord server](/discord)."},{"id":"/tenzir-v4.11","metadata":{"permalink":"/releases/tenzir-v4.11","source":"@site/releases/tenzir-v4.11/index.md","title":"Tenzir v4.11","description":"Our latest v4.11","date":"2024-03-22T00:00:00.000Z","formattedDate":"March 22, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"contexts","permalink":"/releases/tags/contexts"},{"label":"every","permalink":"/releases/tags/every"},{"label":"set","permalink":"/releases/tags/set"},{"label":"email","permalink":"/releases/tags/email"},{"label":"sqs","permalink":"/releases/tags/sqs"}],"readingTime":5.07,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.11","authors":["dominiklohmann"],"date":"2024-03-22T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["release","contexts","every","set","email","sqs"],"comments":true},"prevItem":{"title":"Tenzir v4.12","permalink":"/releases/tenzir-v4.12"},"nextItem":{"title":"Tenzir v4.10","permalink":"/releases/tenzir-v4.10"}},"content":"Our latest [v4.11](https://github.com/tenzir/tenzir/releases/tag/v4.11.1)\\nrelease delivers powerful automation features, such as scheduling pipelines in a\\ngiven time interval and sending pipeline data as emails.\\n\\n![Tenzir v4.11](tenzir-v4.11.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Execute Sources on a Schedule\\n\\nOne feedback we\'ve heard often from users is that the `from <url>` invocation is\\nindeed handy, but it\'s more practical when it\'s not a one-time gig. Users\\nexpressed a need to retrieve data from various sources more than just once,\\nindicating a requirement for a more cyclical or scheduled approach.\\n\\nGiven these requirements, we initially considered adding options for continuous\\ndata retrieval or polling for specific connectors, such as `http`. However, we\\nrealized that the need for such functionality ranged beyond a limited number of\\nconnectors. Hence, any solution we developed would ideally adapt to any source\\noperator, providing wider functionality.\\n\\nIn response to these needs, we developed a new operator modifier that empowers\\nany source operator to execute at regular intervals: `every <interval>`.\\n\\nFor instance, the operator `every 1s from <url>` will enable the system to poll\\nthe specified URL every single second. The capability delivers continuous,\\nreal-time data access, considerably improving the feasibility and efficiency of\\ntasks requiring frequent data updates.\\n\\nOne area where we\'ve found the `every <interval>` modifier to be especially\\nvaluable is in the context of updating contexts. Consider a pipeline designed to\\nupdate a lookup-table context titled `threatfox-domains` once every hour. This\\noperation, which fetches IOCs (Indicators of Compromise) from the ThreatFox API,\\ncan be achieved using the following pipeline:\\n\\n```\\nevery 1 hour from https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n| yield data[]\\n| where ioc_type == \\"domain\\"\\n| context update threatfox-domains --key ioc\\n```\\n\\nThis pipeline initiates a query to retrieve the IOCs for the day from the\\nThreatFox API. The pipeline subsequently filters out the data relevant to\\ndomains and updates the `threatfox-domains` context. The entire pipeline\\nrefreshes every hour as specified by the `every 1 hour` operator modifier.\\n\\nThe `every <interval>` operator modifier thus adds a powerful tool to our\\narsenal, increasing our capabilities by adapting to any source operator for\\nscheduled execution.\\n\\n## Enrich More Flexibly\\n\\nA customer of ours asked a seemingly simple question: If I have a lookup-table\\ncontext that contains entries in the form `{\\"key\\": \\"DE\\", \\"context\\": {\\"flag\\":\\n\\"\ud83c\udde9\ud83c\uddea\\"}}` and want to use it to replace country short codes with their respective\\nflag as an emoji, how can I do that?\\n\\nIf you\'re just replacing the value of a single field then it\'s easy\u2014you can just\\nuse `put` to replace the input value with its context after the enrichment. But\\nthis user wanted to look into every single string in every event, and replace\\nall country short codes that it contained.\\n\\nTwo newly added options for the `enrich` operator make this easily possible:\\n\\n```\\n\u2026\\n| enrich country-flags --field :string --yield flag --replace\\n```\\n\\nThe `--replace` flag causes `enrich` to replace fields with their context, if\\nthey exists. The option `--yield <field>` trims down the enrichment to just a\\nspecific field within the context. The `--yield` option is also available for\\nthe `lookup` operator.\\n\\n```json title=\\"Before\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\"\\n}\\n```\\n\\n```json title=\\"After\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"\ud83c\udde9\ud83c\uddea\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"\ud83c\uddfa\ud83c\uddf8\\"\\n}\\n```\\n\\nThe other new option of `lookup` and `enrich` is `--separate`, which creates\\nseparate events for every enrichment. This causes events to be duplicated for\\nevery enrichment from a context that applies, with one enrichment per event in\\nthe result. This is particularly useful in `lookup` when evaluating a large set\\nof IOCs to create separate alerts per IOC even within a single event.\\n\\n```json title=\\"Enriched as one event\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\",\\n  \\"flags\\": {\\n    \\"source_ip\\": {\\n      \\"value\\": \\"212.12.56.176\\"\\n      \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n      \\"mode\\": \\"enrich\\",\\n      \\"context\\": {\\n        \\"flag\\": \\"\ud83c\udde9\ud83c\uddea\\"\\n      }\\n    }\\n    \\"dest_ip\\": {\\n      \\"value\\": \\"8.8.8.8\\"\\n      \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n      \\"mode\\": \\"enrich\\",\\n      \\"context\\": {\\n        \\"flag\\": \\"\ud83c\uddfa\ud83c\uddf8\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\n```json title=\\"Enriched as separate events\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\",\\n  \\"flags\\": {\\n    \\"path\\": \\"source_ip\\",\\n    \\"value\\": \\"212.12.56.176\\"\\n    \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n    \\"mode\\": \\"enrich\\",\\n    \\"context\\": {\\n      \\"flag\\": \\"\ud83c\udde9\ud83c\uddea\\"\\n    }\\n  }\\n}\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\",\\n  \\"flags\\": {\\n    \\"path\\": \\"source_ip\\",\\n    \\"value\\": \\"8.8.8.8\\"\\n    \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n    \\"mode\\": \\"enrich\\",\\n    \\"context\\": {\\n      \\"flag\\": \\"\ud83c\uddfa\ud83c\uddf8\\"\\n    }\\n  }\\n}\\n```\\n\\n## The Sweet Spot Between Extend and Replace\\n\\nThe `set` operator \\"upserts\\" into events. Its syntax exactly matches the syntax\\nof the existing `extend`, `replace`, and `put` operators.\\n\\nIf a specified field already exists, the `set` operator replaces its value. If\\nit does not, the `set` operator extends the event with new field. We found this\\nbehavior to be quite intuitive, and in most cases we now reach for `set` instead\\nof `replace` and `extend`.\\n\\n:::tip Setting the Schema Name\\nThe `set`, `put`, and `replace` operator support changing the schema name of\\nevents. For example, `set #schema=\\"foo.bar\\"` will show up as a schema `bar` in\\nthe category `foo` in the Explorer on [app.tenzir.com](https://app.tenzir.com).\\n:::\\n\\n## Send Emails from a Pipeline\\n\\nThe new `email` saver sends away pipeline contents as mails. This is especially\\nhandy for integrating with traditional escalation pathways that rely on\\nemail-based dispatching methods.\\n\\nFor example, to send all Suricata alerts arriving at a node via email, use:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.alert\\"\\n| write json\\n| save email alerts@example.org --from \\"tenzir@example.org\\" --subject Alert\\n```\\n\\nThe `email` saver supports both SMTP and SMTPS. The default endpoint is\\n`smtp://localhost:25`, but you can provide any other server. Instead of copying\\nthe rendered JSON directly into the email body, you can also provide the\\n`--mime` to send a MIME-encoded chunk that uses the MIME type according to the\\nformat you provided.\\n\\n## Working with Amazon SQS Queues\\n\\nThe new `sqs` enables reading from and writing to Amazon SQS queues. For\\nexample, importing JSON from an SQS queue named `tenzir` into a node looks like\\nthis:\\n\\n```\\nfrom sqs://tenzir | import\\n```\\n\\n## Other Changes\\n\\nAs usual, the complete list of bug fixes, adjustments, and enhancements\\ndelivered with this version can be found in [the changelog](/changelog#v4110).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\nconnect with us on [our Discord server](/discord)."},{"id":"/tenzir-v4.10","metadata":{"permalink":"/releases/tenzir-v4.10","source":"@site/releases/tenzir-v4.10/index.md","title":"Tenzir v4.10","description":"Today, we\'re releasing [Tenzir","date":"2024-03-11T00:00:00.000Z","formattedDate":"March 11, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"pipelines-as-code","permalink":"/releases/tags/pipelines-as-code"},{"label":"contexts","permalink":"/releases/tags/contexts"},{"label":"arm64","permalink":"/releases/tags/arm-64"}],"readingTime":4.065,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.10","authors":["dominiklohmann"],"date":"2024-03-11T00:00:00.000Z","tags":["release","pipelines-as-code","contexts","arm64"],"comments":true},"prevItem":{"title":"Tenzir v4.11","permalink":"/releases/tenzir-v4.11"},"nextItem":{"title":"Tenzir v4.9","permalink":"/releases/tenzir-v4.9"}},"content":"Today, we\'re releasing [Tenzir\\nv4.10](https://github.com/tenzir/tenzir/releases/tag/v4.10.0), which improves\\nhow Tenzir integrates with modern deployment practices.\\n\\n![Tenzir v4.10](tenzir-v4.10.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Pipelines as Code\\n\\nIn today\'s deployment landscape, best practices emphasize GitOps in synergy with\\nInfrastructure as Code (IaC). With the goal of integrating our services into\\nthese existing mechanisms, we\'re excited to introduce Pipelines as Code (PaC) in\\nTenzir v4.10.\\n\\nPaC differs from traditional deployment methods in two key aspects. Firstly,\\npipelines deployed as code always start with the Tenzir node, ensuring\\ncontinuous operation. Secondly, to safeguard them, deletion via the user\\ninterface is disallowed for pipelines deployed as code.\\n\\nHere\'s a simple example to get you started:\\n\\n```yaml {0} title=\\"<prefix>/etc/tenzir/tenzir.yaml\\"\\ntenzir:\\n  pipelines:\\n    suricata-over-tcp:\\n      name: Import Suricata from TCP\\n      definition: |\\n        from tcp://0.0.0.0:34343 read suricata\\n        | import\\n      start:\\n        failed: true  # always restart on failure\\n```\\n\\n:::tip Want to learn more?\\nRead our guide on PaC: \ud83d\udc49 [Deploy Pipelines as\\nCode](/usage/run-pipelines#as-code)\\n:::\\n\\n## arm64 Docker Images\\n\\nDid you ever try to run Tenzir in Docker on a new-ish MacBook and encountered\\nthis error?\\n\\n```text {0} title=\\"\u276f docker run tenzir/tenzir:v4.9.0 version\\"\\nWARNING: The requested image\'s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\\ntenzir: error while loading shared libraries: libfluent-bit.so: cannot enable executable stack as shared object requires: Invalid argument\\n```\\n\\nNow, this works as expected:\\n\\n```json {0} title=\\"\u276f docker run tenzir/tenzir:v4.10.0 version\\"\\n{\\n  \\"version\\": \\"4.10.0\\",\\n  \\"build\\": \\"\\",\\n  \\"major\\": 4,\\n  \\"minor\\": 10,\\n  \\"patch\\": 0\\n}\\n```\\n\\nThis works because the Tenzir Docker images now are multi-archecture images\\nbuilt natively for both `linux/amd64` and `linux/arm64/v8`. In addition to\\nsupporting M-series MacBooks, this also allows the Docker images to run without\\nemulation on other arm64-based systems like AWS Graviton.\\n\\n## Reimagining Unsafe Pipelines\\n\\nWe\'ve substituted the `tenzir.allow-unsafe-pipelines` feature with\\n`tenzir.no-location-overrides`, flipping the default set-up and enhancing user\\nexperience.\\n\\n`tenzir.allow-unsafe-pipelines` had been historically puzzling for newcomers\\ngiven its seemingly fearsome name and ambiguous implications. Why would someone\\nconsciously permit unsafe pipelines? And why have we now defaulted to allowing\\nthem?\\n\\nPipelines have the ability to execute in multiple processes. For instance,\\nexecuting `tenzir \'from file.json | import\'` would prompt `from file.json` to\\nrun in the `tenzir` process, and `import` in the connected `tenzir-node`\\nprocess. An operator\'s _location_ can be assigned as local, anywhere, or remote.\\nOn initializing a pipeline, Tenzir\'s executor intelligently divides the pipeline\\naccording to location change between local and remote, starts separated\\npipelines at their respective locations, connects them to one another.\\n\\nHowever, operator locations can also be manually manipulated. For instance, when\\ncapturing PCAPs, users might desire to prevent unnecessary inter-process\\ncommunication and directly connect the Tenzir Node to the network\\ninterface\u2014achieved by executing `tenzir \'remote from nic \u2026\'`. This command\\ninstructs the executor to consistently run `from nic \u2026` directly at the node.\\nWhen introducing this feature during the Tenzir v4.0 release, we wanted to be\\ncautious about unrestricted use of this feature, leading to the creation of the\\n`tenzir.allow-unsafe-pipelines` option, which by default was set to false. This\\noption prohibits the use of location overrides when enabled but simultaneously\\nposed puzzlement to new users being the lone feature disallowed in an \\"unsafe\\"\\npipeline.\\n\\nIn response to feedback, we\'ve improved our approach. Location overrides are now\\npermitted by default and can be disallowed by using the new option\\n`tenzir.no-location-overrides`.\\n\\n## Apply Contexts to Multiple Fields\\n\\nDid you ever want to act on multiple fields in `enrich` or `lookup`? Now you\\ncan!\\n\\nFor example, you can now use a GeoIP context on all IP addresses in your data as\\nsimple as this:\\n\\n```text {0} title=\\"Enrich with a geoip context named country\\"\\n\u2026\\n| enrich country --field :ip\\n```\\n\\nYou can also specify multiple fields explicitly:\\n\\n```\\n\u2026\\n| enrich country --field src_ip,dest_ip\\n```\\n\\nThe output of `lookup` and `enrich` changed slightly to accomodate multiple\\ncontexts in the same event. Under the output field (that defaults to the context\\nname), there is now a new record named `context`, under which we replicate the\\npath to the enriched fields for placing the context. That is, the context of\\n`id.orig_h` in this example is accessible as `country.context.id.orig_h`:\\n\\n```json {0} title=\\"export | enrich country\\"\\n{\\n  \\"ts\\": \\"2021-11-17T13:53:51.022351\\",\\n  \\"uid\\": \\"CVtvt83MWz8MBNTWWd\\",\\n  \\"id\\": {\\n    \\"orig_h\\": \\"244.69.36.0\\",\\n    \\"orig_p\\": 45228,\\n    \\"resp_h\\": \\"242.239.167.49\\",\\n    \\"resp_p\\": 34774\\n  },\\n  \\"proto\\": \\"udp\\",\\n  // ...\\n  \\"country\\": {\\n    \\"timestamp\\": \\"2024-03-11T15:58:00.596027\\",\\n    \\"mode\\": \\"enrich\\",\\n    \\"context\\": {\\n      \\"id\\": {\\n        \\"orig_h\\": {\\n          \\"country\\": {\\n            \\"geoname_id\\": 1861060,\\n            \\"iso_code\\": \\"JP\\",\\n            \\"names\\": {\\n              \\"de\\": \\"Japan\\",\\n              \\"en\\": \\"Japan\\",\\n              \\"es\\": \\"Jap\xf3n\\",\\n              \\"fr\\": \\"Japon\\",\\n              \\"ja\\": \\"\u65e5\u672c\\",\\n              \\"pt-BR\\": \\"Jap\xe3o\\",\\n              \\"ru\\": \\"\u042f\u043f\u043e\u043d\u0438\u044f\\",\\n              \\"zh-CN\\": \\"\u65e5\u672c\\"\\n            }\\n          }\\n        },\\n        \\"resp_h\\": {\\n          \\"country\\": {\\n            \\"geoname_id\\": 1861060,\\n            \\"iso_code\\": \\"JP\\",\\n            \\"names\\": {\\n              \\"de\\": \\"Japan\\",\\n              \\"en\\": \\"Japan\\",\\n              \\"es\\": \\"Jap\xf3n\\",\\n              \\"fr\\": \\"Japon\\",\\n              \\"ja\\": \\"\u65e5\u672c\\",\\n              \\"pt-BR\\": \\"Jap\xe3o\\",\\n              \\"ru\\": \\"\u042f\u043f\u043e\u043d\u0438\u044f\\",\\n              \\"zh-CN\\": \\"\u65e5\u672c\\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\n## Other Changes\\n\\nFor the curious, [the changelog](/changelog#v4100) includes the full list of bug\\nfixes, changes and improvements introduced with this release.\\n\\nPlay with the new features at [app.tenzir.com](https://app.tenzir.com) and join\\nus on [our Discord server](/discord)."},{"id":"/tenzir-v4.9","metadata":{"permalink":"/releases/tenzir-v4.9","source":"@site/releases/tenzir-v4.9/index.md","title":"Tenzir v4.9","description":"We\'re thrilled to announce the release of [Tenzir","date":"2024-02-21T00:00:00.000Z","formattedDate":"February 21, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"context","permalink":"/releases/tags/context"},{"label":"bloom-filter","permalink":"/releases/tags/bloom-filter"},{"label":"chart","permalink":"/releases/tags/chart"},{"label":"dashboard","permalink":"/releases/tags/dashboard"}],"readingTime":3.22,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.9","authors":["dominiklohmann"],"date":"2024-02-21T00:00:00.000Z","tags":["release","context","bloom-filter","chart","dashboard"],"comments":true},"prevItem":{"title":"Tenzir v4.10","permalink":"/releases/tenzir-v4.10"},"nextItem":{"title":"Tenzir v4.8","permalink":"/releases/tenzir-v4.8"}},"content":"We\'re thrilled to announce the release of [Tenzir\\nv4.9](https://github.com/tenzir/tenzir/releases/tag/v4.9.0), enhancing the\\nExplorer further to empower you with the capability of rendering your data as a\\nchart.\\n\\n![Tenzir v4.9](tenzir-v4.9.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Chart Operator\\n\\nThe new [`chart`](/next/operators/chart) operator transforms the way you\\nvisualize your data on [app.tenzir.com](https://app.tenzir.com). It lets you\\ndepict your events graphically instead of in table form.\\n\\nCharting integrates seamlessly into your pipelines by simply adding the `chart`\\noperator. For instance, plotting a bar chart representing the frequency of\\noccurrences for each protocol in `zeek.conn` events can be as simple as this:\\n\\n```\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| top proto\\n| chart bar --title \\"Protocols\\"\\n```\\n\\n![Protocols](https://github.com/tenzir/tenzir/assets/4488655/075cf3af-ed51-4aca-8885-6f682284831c)\\n\\nThis line chart depicts the load average over 15 minutes, making use of the\\nrecently added `metrics` operator:\\n\\n```\\nmetrics\\n| where #schema == \\"tenzir.metrics.cpu\\"\\n| sort timestamp\\n| chart line -x timestamp -y loadavg_15m --title \\"Load Average (15 min)\\"\\n```\\n\\n![Load Average (15 min)](https://github.com/tenzir/tenzir/assets/4488655/453bc8da-4be8-4a2c-9ef2-10328f02d682)\\n\\nThis area chart displays the total ingress across all pipelines for the past 10\\nminutes in MiB/s.\\n\\n```\\nmetrics\\n| where #schema == \\"tenzir.metrics.operator\\"\\n| where timestamp > 10 min ago\\n| where source == true\\n| where internal == false\\n| sort timestamp\\n| python \'self.egress_rate = self.output.approx_bytes / self.duration.total_seconds() / 2**20\'\\n| chart area -x timestamp -y egress_rate --title \\"Total Ingress (MiB/s)\\"\\n```\\n\\n![Total Ingress (MiB/s)](https://github.com/tenzir/tenzir/assets/4488655/a5313261-fe5d-413c-a7d9-8da781871aba)\\n\\nThis pie chart shows the distribution of events stored at the node by disk\\nusage:\\n\\n```\\nshow partitions\\n| summarize diskusage=sum(diskusage) by schema\\n| chart pie --title \\"Disk Usage (bytes)\\"\\n```\\n\\n![Disk Usage](https://github.com/tenzir/tenzir/assets/4488655/103bdb72-7708-414b-ac8c-d19562295ea3)\\n\\nWe\'re just getting started with charting! If you want to see further chart types\\nadded, have feedback on charting, or want to share examples of your\\nvisualizations with the chart operator, we would love to [hear from\\nyou](/discord).\\n\\n:::info Coming Soon: Dashboards\\nThe `chart` operator is a first step towards having dashboards directly in\\nTenzir. Any result that you see in the Explorer you will soon be able to pin and\\nfreely arrange on a customizable dashboard.\\n:::\\n\\n## Bloom Filter Context\\n\\nThe new `bloom-filter` context makes it possible to use large datasets for\\nenrichment. It uses a [Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter)\\nto store sets in a compact way, at the cost of potential false positives when\\nlooking up an item.\\n\\nIf you have massive amounts of indicators or a large amount of things you would\\nlike to contextualize, this feature is for you.\\n\\nCreate a Bloom filter context by using `bloom-filter` as context type:\\n\\n```\\ncontext create indicators bloom-filter\\n```\\n\\nThen populate it with a pipeline, exactly like a lookup table:\\n\\n```\\nfrom /tmp/iocs.csv\\n| context update bloom-filter --key ioc\\n```\\n\\nThereafter use it for enrichment, e.g., in this example pipeline:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.dns\\"\\n| enrich indicators --field dns.rrname\\n```\\n\\nThe `enrich` operator gained a new `--filter` option to remove events it could\\nnot enrich. Use the new option to remove anything that is not included in the\\nBloom filter:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.dns\\"\\n| enrich indicators --field dns.rrname --filter\\n```\\n\\n## Housekeeping\\n\\nOther noteworthy changes and improvements:\\n- `tenzir.db-directory` is now `tenzir.state-directory`. The old option remains\\n  functional, but will be phased out in an upcoming release.\\n- On the command-line, Tenzir now respects [`NO_COLOR`](https://no-color.org)\\n  when printing diagnostics. Additionally, colors are automatically disabled\\n  when the output device is not a terminal.\\n- RFC 5424-style Syslog parsing now emits a record with the structured data\\n  fields.\\n- The `--selector` option for the JSON parser now works with nested and\\n  non-string fields.\\n- The `python` operator gained a `--file` option to read from a file instead of\\n  expecting the Python code as a positional argument.\\n- The `csv`, `tsv`, and `ssv` parsers now fill in nulls for missing values.\\n\\nFor the curious, [the changelog](/changelog#v490) has the full scoop.\\n\\nExperience the new features at [app.tenzir.com](https://app.tenzir.com) and join\\nus on [our Discord server](/discord)."},{"id":"/tenzir-v4.8","metadata":{"permalink":"/releases/tenzir-v4.8","source":"@site/releases/tenzir-v4.8/index.md","title":"Tenzir v4.8","description":"Hot off the press: [Tenzir","date":"2024-01-22T00:00:00.000Z","formattedDate":"January 22, 2024","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"fluent-bit","permalink":"/releases/tags/fluent-bit"},{"label":"http","permalink":"/releases/tags/http"},{"label":"context","permalink":"/releases/tags/context"},{"label":"lookup","permalink":"/releases/tags/lookup"},{"label":"performance","permalink":"/releases/tags/performance"}],"readingTime":2.83,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"},{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"}],"frontMatter":{"title":"Tenzir v4.8","authors":["dominiklohmann","jachris"],"date":"2024-01-22T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["release","fluent-bit","http","context","lookup","performance"],"comments":true},"prevItem":{"title":"Tenzir v4.9","permalink":"/releases/tenzir-v4.9"},"nextItem":{"title":"Tenzir v4.7","permalink":"/releases/tenzir-v4.7"}},"content":"Hot off the press: [Tenzir\\nv4.8](https://github.com/tenzir/tenzir/releases/tag/v4.8.0). This release is\\nfilled with goodness.\\n\\n![lookup](lookup.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Lookup Operator\\n\\nThe new `lookup` operator is a unique vehicle to perform live- and\\nretro-matching simultaneously. Think of it as enrichment of all data that gets\\ningested into a node, plus a historical query for every change in the enrichment\\ncontext.\\n\\n## Graylog Support\\n\\nThe new `gelf` parser makes it possible to read a stream of [Graylog Extended\\nLog Format\\n(GELF)](https://go2docs.graylog.org/5-0/getting_in_log_data/gelf.html) messages.\\n\\nYou can now point your GELF feed to a Tenzir pipeline. Read our [Graylog\\nintegration page](/next/integrations/graylog) for the details. The TL;DR is:\\n\\n```\\nfrom tcp://0.0.0.0:12201 read gelf\\n| import\\n```\\n\\n## Shift Timestamps and Delay Events\\n\\nThe new `timeshift` and `delay` operators make it possible to rewrite timestamps\\nand act on them to replay data flexibly.\\n\\nThe `timeshift` operator adjusts a series of time values by anchoring them\\naround a given start time. You can rewrite and scale timestamps:\\n\\n![timeshift](timeshift.excalidraw.svg)\\n\\nFor example, use `timeshift` to re-align our Zeek example dataset to January 1,\\n1984, and make the trace 100x slower:\\n\\n```\\nfrom https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst read zeek-tsv\\n| timeshift --start 1984-01-01 --speed 0.01 ts\\n```\\n\\nWhile `timeshift` rewrites timestamps, `delay` acts on them by yielding events\\naccording to a given time field. Delaying events comes in handy when replaying a\\ntrace or logs. Delaying means effectively introducing sleeping periods\\nproportional to the inter-arrival times of the events. As with `timeshift`, you\\ncan scale the behavior with a multiplicative constant to speed things up.\\n\\nHere is visual explanation of how `delay` works:\\n\\n![delay](delay.excalidraw.svg)\\n\\n## HTTP Saver\\n\\nThe `http` connector now also has a saver in addition to the already existing\\nloader. Here\'s how they work in a nutshell:\\n\\n![HTTP Connector](http.excalidraw.svg)\\n\\nFor the loader, you specify the request body and the response body is input for\\nthe pipeline. For the saver, the pipeline contents determine the request body\\nand the response body isn\'t processed.\\n\\n## Fluent Bit Performance\\n\\nThe `fluent-bit` source operator got a significant performance boost as a\\nbyproduct of changing the Fluent Bit data exchange format from JSON to MsgPack:\\n\\n![Fluent Bit Performance](fluent-bit-speedup.svg)\\n\\nRead the [dedicated blog post on this\\nissue](/archive/switching-fluentbit-from-json-to-msgpack).\\n\\nThanks to Christoph Lobmeyer and Yannik Meinhardt for reporting this issue! \ud83d\ude4f\\n\\n## Improved Pipeline State Persistence\\n\\nWe\'ve improved the [state management of\\npipelines](/next/usage/manage-a-pipeline) when nodes restart or crash.\\nRecall the state machine of a pipeline:\\n\\n![Pipeline States](pipeline-states.excalidraw.svg)\\n\\nThe gray buttons on the state transition arrows correspond to actions you can\\ntake.\\n\\nHere\'s what changed on node restart and/or crash:\\n\\n- Running pipelines remain in *Running* state. Previously, the node stopped all\\n  running pipelines when shutting down. The unexpected behavior was that a\\n  restart of a node didn\'t automatically resume previously running pipelines.\\n  This is now the case.\\n- Paused pipelines transition to the *Stopped* state. The difference between\\n  *Paused* and *Stopped* is that paused pipelines can be quickly resumed without\\n  losing in-memory state. Stopping a pipeline fully evicts it. A node restart\\n  necessarily evicts the state of a pipeline, hence the transition from *Paused*\\n  to *Stopped*. Previously, paused pipelines were considered *Failed* after a\\n  node restart.\\n\\n## Here & There\\n\\nLots of smaller bug fixes landed in this release. We urge everyone to upgrade.\\nIf you\'re curious, [our changelog](/changelog#v480) has the full list of\\nchanges.\\n\\nVisit [app.tenzir.com](https://app.tenzir.com) to try the new\\nfeatures and swing by [our Discord server](/discord) to get help and talk about\\nyour use cases."},{"id":"/tenzir-v4.7","metadata":{"permalink":"/releases/tenzir-v4.7","source":"@site/releases/tenzir-v4.7/index.md","title":"Tenzir v4.7","description":"Tenzir v4.7 brings a new","date":"2023-12-19T00:00:00.000Z","formattedDate":"December 19, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"grok","permalink":"/releases/tags/grok"},{"label":"kv","permalink":"/releases/tags/kv"},{"label":"geoip","permalink":"/releases/tags/geoip"},{"label":"performance","permalink":"/releases/tags/performance"}],"readingTime":3.54,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.7","authors":["dominiklohmann"],"date":"2023-12-19T00:00:00.000Z","tags":["release","grok","kv","geoip","performance"],"comments":true},"prevItem":{"title":"Tenzir v4.8","permalink":"/releases/tenzir-v4.8"},"nextItem":{"title":"Tenzir v4.6","permalink":"/releases/tenzir-v4.6"}},"content":"[Tenzir v4.7](https://github.com/tenzir/tenzir/releases/tag/v4.7.0) brings a new\\ncontext type, two parsers, four new operators, improvements to existing parsers,\\nand a sizable under-the-hood performance improvement.\\n\\n![Tenzir v4.7](tenzir-v4.7.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Enrich with the GeoIP context\\n\\nUse the `geoip` context to enrich events with information from a MaxMind GeoIP\xae\\ndatabase.\\n\\nTo get started, [download the freely available GeoLite2 MaxMind\\ndatabase](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data), or use\\nany other MaxMind database. We\'ll use the country database file\\n`GeoLite2-Country.mmdb`.\\n\\n```text {0} title=\\"Create a \'geoip\' context named \'country\'\\"\\ncontext create country geoip --db-path /path/to/GeoLite2-Country.mmdb\\n```\\n\\n```text {0} title=\\"Enrich Suricata events with the \'country\' context\\"\\nexport\\n| where #schema == /suricata.*/\\n/* Apply the context to both source and destination IP address fields */\\n| enrich src_country=country --field src_ip\\n| enrich dest_country=country --field dest_ip\\n/* Use just the country\'s isocode, and discard the rest of the information */\\n| replace src_country=src_country.context.country.iso_code,\\n          dest_country=dest_country.context.country.iso_code\\n```\\n\\n```json {0} title=\\"Possible output\\"\\n{\\n  \\"timestamp\\": \\"2021-11-17T14:02:38.165570\\",\\n  \\"flow_id\\": 1837021175481117,\\n  \\"pcap_cnt\\": 357,\\n  \\"vlan\\": null,\\n  \\"in_iface\\": null,\\n  \\"src_ip\\": \\"45.137.23.27\\",\\n  \\"src_port\\": 47958,\\n  \\"dest_ip\\": \\"198.71.247.91\\",\\n  \\"dest_port\\": 53,\\n  \\"proto\\": \\"UDP\\",\\n  \\"event_type\\": \\"dns\\",\\n  \\"community_id\\": \\"1:0nZC/6S/pr+IceCZ04RjDZbX+KI=\\",\\n  \\"dns\\": {\\n    // ...\\n  },\\n  \\"src_country\\": \\"NL\\",\\n  \\"dest_country\\": \\"US\\"\\n}\\n```\\n\\nThe `geoip` context is a powerful building block for in-band enrichments.\\nBesides country codes and country names you can add region codes, region names,\\ncities, zip codes, and geographic coordinates. With the flexibility of the\\ncontextualization framework this information you can now get this information in\\nreal-time.\\n\\n:::info Follow our Blog Post Series\\nRead more about contexts in our blog post series:\\n1. [Enrichment Complexity in the Wild](/archive/enrichment-complexity-in-the-wild)\\n2. [Contextualization Made Simple](/archive/contextualization-made-simple)\\n:::\\n\\n## Grok and KV Parsers\\n\\nThe `kv` and `grok` parsers combine well with the `parse` operator introduced\\nwith Tenzir v4.6. The former reads key-value pairs by splitting strings based on\\nregular expressions, and the latter uses a parser modeled after the [Logstash\\n`grok` plugin][logstash-grok] in Elasticsearch.\\n\\n[logstash-grok]: https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html\\n\\nParse a fictional HTTP request log with `grok`:\\n\\n```json {0} title=\\"Example input\\"\\n{\\n  \\"message\\": \\"55.3.244.1 GET /index.html 15824 0.043\\"\\n}\\n```\\n\\n```text {0} title=\\"Parse with grok\\"\\nparse message grok \\"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\\"\\n```\\n\\n```json {0} title=\\"Example output\\"\\n{\\n  \\"message\\": {\\n    \\"client\\": \\"55.3.244.1\\",\\n    \\"method\\": \\"GET\\",\\n    \\"request\\": \\"/index.html\\",\\n    \\"bytes\\": 15824,\\n    \\"duration\\": 0.043\\n  }\\n}\\n```\\n\\nExtract space-separated `key=value` pairs with `kv`:\\n\\n```json {0} title=\\"Example input\\"\\n{\\n  \\"message\\": \\"foo=1 bar=2 baz=3 qux=4\\"\\n}\\n```\\n\\n```text {0} title=\\"Parse with kv\\"\\nparse message kv \\"\\\\s+\\" \\"=\\"\\n```\\n\\n```json {0} title=\\"Example output\\"\\n{\\n  \\"message\\": {\\n    \\"foo\\": 1,\\n    \\"bar\\": 2,\\n    \\"baz\\": 3,\\n    \\"qux\\": 4\\n  }\\n}\\n```\\n\\n## Slice and Dice Events\\n\\nThe `slice` operator is a more powerful version of the `head` and `tail`\\noperators. It allows for selecting a contiguous range of events given a\\nhalf-closed interval.\\n\\n```text {0} title=\\"Get the second 100 events\\"\\nslice --begin 100 --end 200\\n```\\n\\nNegative values for the interval count from the end rather than from the start:\\n\\n```text {0} title=\\"Get the last 5 events\\"\\nslice --begin -5\\n```\\n\\nPositive and negative values can also be combined:\\n\\n```text {0} title=\\"Get everything but the first 10 and the last 10 events\\"\\nslice --begin 10 --end -10\\n```\\n\\n## Lightweight Endpoint Snapshot\\n\\nUse the `processes` `sockets`, and `nics` sources to get a snapshot of running\\nprocesses, sockets, and available network interfaces, respectively.\\n\\n```text {0} title=\\"Top three running processes by name\\"\\nprocesses\\n| top name\\n| head 3\\n```\\n\\n```json {0} title=\\"Possible output\\"\\n{\\n  \\"name\\": \\"MTLCompilerService\\",\\n  \\"count\\": 24\\n}\\n{\\n  \\"name\\": \\"zsh\\",\\n  \\"count\\": 16\\n}\\n{\\n  \\"name\\": \\"VTDecoderXPCServ\\",\\n  \\"count\\": 9\\n}\\n```\\n\\n## Performance Improvements\\n\\nWe\'ve fixed a long-standing bug in Tenzir\'s pipeline execution engine that\\nimprove performance for some operators:\\n\\n1. Operators and loaders that interface with blocking third-party APIs sometimes\\n   delayed partial results until the next partial result arrived through the\\n   blocking API. This bug affected the `tcp`, `zmq`, `kafka`, and `nic` loaders\\n   and the `shell`, `fluent-bit`, `velociraptor`, and `python` operators. These\\n   loaders and operators are now generally more responsive.\\n2. The time-to-first-result for pipelines with many operators is now shorter,\\n   and the first result no longer takes an additional 20ms per operator in the\\n   pipeline to arrive.\\n\\n## Want More?\\n\\nWe provide a full list of changes [in our changelog](/changelog#v470).\\n\\nHead over to [app.tenzir.com](https://app.tenzir.com) to play with the new\\nfeatures and join [our Discord server](/discord)\u2014the perfect place to ask\\nquestions, chat with Tenzir users and developers, and to discuss your feature\\nideas!"},{"id":"/tenzir-v4.6","metadata":{"permalink":"/releases/tenzir-v4.6","source":"@site/releases/tenzir-v4.6/index.md","title":"Tenzir v4.6","description":"Tenzir v4.6 is here, and","date":"2023-12-01T00:00:00.000Z","formattedDate":"December 1, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"context","permalink":"/releases/tags/context"},{"label":"enrich","permalink":"/releases/tags/enrich"},{"label":"syslog","permalink":"/releases/tags/syslog"},{"label":"tcp","permalink":"/releases/tags/tcp"},{"label":"parse","permalink":"/releases/tags/parse"},{"label":"python","permalink":"/releases/tags/python"}],"readingTime":5.745,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.6","authors":["dominiklohmann"],"date":"2023-12-01T00:00:00.000Z","tags":["release","context","enrich","syslog","tcp","parse","python"],"comments":true},"prevItem":{"title":"Tenzir v4.7","permalink":"/releases/tenzir-v4.7"},"nextItem":{"title":"Tenzir v4.5","permalink":"/releases/tenzir-v4.5"}},"content":"[Tenzir v4.6](https://github.com/tenzir/tenzir/releases/tag/v4.6.0) is here, and\\nit is our biggest release yet. The headlining feature is the all-new **context**\\nfeature, powered by the `context` and `enrich` operators and the new **context\\nplugin** type.\\n\\n![Tenzir v4.6](tenzir-v4.6.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Enrich with Contexts\\n\\nContexts enable enriching events in real-time with additional information.\\n\\nThis is best explained on an example. We\'ll use the publicly available [Feodo IP\\nBlock List](https://feodotracker.abuse.ch) to enrich events with information\\nthat have a source IP address that is on the block list.\\n\\n```text {0} title=\\"Create a \'lookup-table\' context named \'feodo\'\\"\\ncontext create feodo lookup-table\\n```\\n\\n```text {0} title=\\"Fill the \'feodo\' context with information\\"\\nfrom https://feodotracker.abuse.ch/downloads/ipblocklist_aggressive.csv read csv --allow-comments\\n| context update feodo --key dst_ip\\n```\\n\\n```text {0} title=\\"Enrich Suricata events with the \'feodo\' context\\"\\nexport\\n| where #schema == /suricata.*/\\n| enrich feodo --field dest_ip\\n```\\n\\n```json {0} title=\\"Possible output\\"\\n{\\n  \\"timestamp\\": \\"2023-11-30T14:52:57.716360+0200\\",\\n  \\"event_type\\": \\"alert\\",\\n  \\"dest_ip\\": \\"167.179.103.206\\",\\n  // ...\\n  \\"feodo\\": {\\n    \\"key\\": \\"167.179.103.206\\",\\n    \\"context\\": {\\n      \\"first_seen_utc\\": \\"2023-11-03T07:55:23\\",\\n      \\"dst_ip\\": \\"167.179.103.206\\",\\n      \\"dst_port\\": 2083,\\n      \\"c2_status\\": \\"offline\\",\\n      \\"last_online\\": \\"2023-11-03T00:00:00\\",\\n      \\"malware\\": \\"Pikabot\\"\\n    },\\n    \\"timestamp\\": \\"2023-11-30T14:58:23.172832+0200\\"\\n  }\\n}\\n```\\n\\nContexts are\u2014just like formats, connectors, and operators\u2014designed to be a\\nbuilding block of TQL. We\'re starting out with just the open-source\\n`lookup-table` context plugin, and already have plans for more context plugins\\nand other operators besides `enrich` that use context plugins in the near\\nfuture. Stay tuned!\\n\\n:::info Follow our Blog Post Series\\nWe\'re so excited about enrichments that we wrote an entire blog post series on\\nit. The [first post](/archive/enrichment-complexity-in-the-wild) is already live,\\nand sets the scene. The second post will soon follow to explain in-depth just\\nhow Tenzir makes enrichments easy. Stay tuned!\\n:::\\n\\n## Onboard Data Faster Than Ever\\n\\nWant to read CEF over syslog from TCP? Not a problem with four all-new features\\nof Tenzir v4.6\u2014are you able to spot all four?\\n\\n```\\nfrom tcp://localhost:514 read syslog\\n| parse content cef\\n```\\n\\nHere\'s an example input and output:\\n\\n```syslog {0} title=\\"Input\\"\\nNov 13 16:59:59 host123 FOO: CEF:0|FORCEPOINT|Firewall|6.6.1|0|Generic|0|deviceExternalId=Master FW node 1 dvc=10.1.1.40 dvchost=10.1.1.40 msg=log server connection established deviceFacility=Logging System rt=Jan 17 2020 08:52:10\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"facility\\": null,\\n  \\"severity\\": null,\\n  \\"timestamp\\": \\"Nov 13 16:59:59\\",\\n  \\"hostname\\": \\"host123\\",\\n  \\"tag\\": \\"FOO\\",\\n  \\"content\\": {\\n    \\"cef_version\\": 0,\\n    \\"device_vendor\\": \\"FORCEPOINT\\",\\n    \\"device_product\\": \\"Firewall\\",\\n    \\"device_version\\": \\"6.6.1\\",\\n    \\"signature_id\\": \\"0\\",\\n    \\"name\\": \\"Generic\\",\\n    \\"severity\\": \\"0\\",\\n    \\"extension\\": {\\n      \\"deviceExternalId\\": \\"Master FW node 1\\",\\n      \\"dvc\\": \\"10.1.1.40\\",\\n      \\"dvchost\\": \\"10.1.1.40\\",\\n      \\"msg\\": \\"log server connection established\\",\\n      \\"deviceFacility\\": \\"Logging System\\",\\n      \\"rt\\": \\"Jan 17 2020 08:52:10\\"\\n    }\\n  }\\n}\\n```\\n\\n### URLs and Paths\\n\\n`from`, `load`, `to` and `save` now support working with URLs and paths\\ndirectly, and no longer require specifying a connector and format explicitly.\\nAdditionally, they now support automatic compression and decompression.\\n\\nFor example, this pipeline reads events from a Zeek TSV log file in the local\\nfile system, and stores them as Zstd-compressed CSV file in an S3 bucket:\\n\\n![URL Expansion](tenzir-v4.6-url-expansion.excalidraw.svg)\\n\\n### TCP Connector\\n\\nAcquire data over TCP (or TLS) directly with the new `tcp` loader. Use the\\n`--tls` option to read from TLS instead, and `--listen` to open a server rather\\nthan connect as a client.\\n\\n### Syslog Format\\n\\nRead syslog RFC 3164 and RFC 5424 with the new `syslog` parser. The parser\\nautomatically disambiguates between the two common syslog standards.\\n\\n### Parse Operator\\n\\nUse the `parse` operator to structurally decompose fields with any parser. This\\nenables parsing of structured data embedded as strings inside another format.\\n\\nOnboarding custom data sources is a pain for every SOC operations team. We\'ve\\nseen CSV where some columns are NDJSON, CEF in syslog, and grok patterns in CEF\\nin syslog. With the `parse` operator, this no longer has to be as painful, and\\nyou can finally spend more time working with your data than onboarding it.\\n\\n## Rapid Prototyping with the Python Operator\\n\\nThe `python` operator allows for modifying events using Python. Here are some\\ncool things that we\'ve done in the first days of playing with the operator:\\n\\n```text {0} title=\\"Calculate the square root of a field\\"\\npython \'\\n  import math\\n  self.sq_x = math.sqrt(self.x)\\n\'\\n```\\n\\n```text {0} title=\\"Add a duration field to Suricata flow events\\"\\nwhere #schema == \\"suricata.flow\\"\\n| python \'self.flow.duration = self.flow.end - self.flow.start\'\\n```\\n\\n```text {0} title=\\"Parse a URL into components\\"\\nwhere #schema == \\"suricata.flow\\"\\n| python \'\\n  import urllib\\n  from collections import namedtuple\\n  self.http.url = f\\"http://{self.http.hostname}{self.http.url}\\"\\n  self.http.parsed = urllib.parse.urlsplit(self.http.url)._asdict()\\n  self.http.parsed.qs = urllib.parse.parse_qs(self.http.parsed.query)\\n\'\\n```\\n\\n:::caution Renamed Python Package\\nAs part of this release, we completely remodeled our Python package and renamed\\nit from `pytenzir` to `tenzir`. The old package continues to work, but is\\ndeprecated and no longer maintained.\\n:::\\n\\n## Long-Poll Support for Serve\\n\\nThe `/serve` endpoint, which allows for fetching events from a REST API for\\npipelines with the `serve` sink operator, now supports long-polling.\\n\\nPreviously, the endpoint had `timeout` and `max_events` parameters. The latter\\ndefines how many events the response may contain at most. The former defines an\\nupper bound for the duration that the endpoint waits before it returns less than\\nthe desired number of events.\\n\\nIn addition, the endpoint now supports a `min_events` parameter, which makes it\\nreturn eagerly as soon as at least the specified number of events arrived at the\\nsink of the pipeline. Setting a low value for the minimum number of events in\\ncombination with a high timeout effectively enables long-polling.\\n\\nPipelines run in the Explorer on [app.tenzir.com](https://app.tenzir.com) use an\\nimplicit `serve` sink to transport events to the results table. For slow-running\\npipelines with few results we found long-polling to improve responsiveness of\\nthe Explorer noticeably, as first results will be displayed much earlier now.\\n\\n## Changes to Ingress and Egress\\n\\nInteracting with the node no longer counts as pipeline ingress or egress. This\\nis best explained visually:\\n\\n![Ingress & Egress Changes](tenzir-v4.6-ingress-egress-changes.excalidraw.svg)\\n\\n## Want More?\\n\\n- The CSV, SSV, and TSV parsers are now more robust and consistent with their\\n  respective printers for null values, empty strings, and lists of values. A new\\n  `--allow-comments` option allows for stripping away lines that begin with `#`.\\n\\n- The JSON parser supports a new option `--arrays-of-objects` to read a stream\\n  of arrays of objects rather than a stream of objects. This is particularly\\n  useful when fetching events from REST APIs, which usually aim to return all\\n  events at in one array rather than stream objects one at a time.\\n\\n- The new `yield` operator \\"zooms in\\" on a record field.\\n\\n- Use the `show` operator without any arguments to see all aspects of a node\\n  instead of just the specified aspect.\\n\\n- The new `apply` operator includes a pipeline defined in an external file. For\\n  example, `apply frobnify` will search for a file named `frobnify.tql`, first\\n  in the current directory, and then in the `apply/` sub-directories of the\\n  config directory of Tenzir.\\n\\nWe provide a full list of changes [in our changelog](/changelog#v460).\\n\\nHead over to [app.tenzir.com](https://app.tenzir.com) to play with the new\\nfeatures and join [our Discord server](/discord)\u2014the perfect place to ask\\nquestions, chat with Tenzir users and developers, and to discuss your feature\\nideas!"},{"id":"/tenzir-v4.5","metadata":{"permalink":"/releases/tenzir-v4.5","source":"@site/releases/tenzir-v4.5/index.md","title":"Tenzir v4.5","description":"Here comes Tenzir v4.5!","date":"2023-11-16T00:00:00.000Z","formattedDate":"November 16, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"operators","permalink":"/releases/tags/operators"},{"label":"expression","permalink":"/releases/tags/expression"},{"label":"index","permalink":"/releases/tags/index"},{"label":"api","permalink":"/releases/tags/api"},{"label":"demo-node","permalink":"/releases/tags/demo-node"}],"readingTime":3.05,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.5","authors":["dominiklohmann"],"date":"2023-11-16T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","operators","expression","index","api","demo-node"],"comments":true},"prevItem":{"title":"Tenzir v4.6","permalink":"/releases/tenzir-v4.6"},"nextItem":{"title":"Tenzir v4.4","permalink":"/releases/tenzir-v4.4"}},"content":"Here comes [Tenzir v4.5](https://github.com/tenzir/tenzir/releases/tag/v4.5.0)!\\nThis release ships a potpourri of smaller improvements that result in faster\\nhistorical query execution and better deployability.\\n\\n![Tenzir v4.5](tenzir-v4.5.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## More Robust Numeric Query Expressions\\n\\nEver scratched your head what numeric literal to use in a query so that you hit\\nthe right fields in events? No more! You can now write `x < 42`, `x < +42`, or\\n`x < 42.0` in an expression to filter events where the field `x` is less than\\n42, regardless of whether `x` is of type `uint64`, `int64`, or `double`.\\nPreviously, you had to know the exact number type for the expression to bind\\nproperly to the event schema.\\n\\nThis incremental improvement is part of a larger thrust to improve the language.\\nWe have plans to convert additional literals during expression bindings to\\nprovide deeper reach into the data, without users really having to deal with\\nschemas.\\n\\n## Sparse Indexes for Number Types\\n\\nTenzir\'s storage engine builds sparse indexes per partition. These sketch\\ndata structures, like Bloom filters and min-max synopses, accelerate historical\\nqueries by ensuring that only relevant partitions get loaded into memory.\\n\\nWe\'ve now added additional sketches for types `bool`, `time`, `duration`,\\n`int64`, `uint64`, and `double` to accelerate a broader range of queries. For\\nexample, this query now runs faster:\\n\\n```\\nexport\\n| where :time > 1 hour ago && dest_port == 80\\n```\\n\\n## The Rest API as an Operator\\n\\nWe exposed [the Rest API](/api) as a new operator called `api`. The benefit\\nprimarily materializes for developers, who can now rapidly prototype\\nintegrations by using the app or `tenzir` command line tool, without having to\\nspin up the integrated web server and do gymnastics with `curl` and `jq`.\\n\\nFor example, to list all pipelines that were created through the API:\\n\\n```\\napi /pipeline/list\\n```\\n\\nThis creates a new pipeline and starts it immediately:\\n\\n```\\napi /pipeline/create \'{\\"name\\": \\"Suricata Import\\", \\"definition\\": \\"from file /tmp/eve.sock read suricata\\", \\"autostart\\": {\\"created\\": true}}\'\\n```\\n\\n## Fine-Grained Operator, Format, and Connector Block Lists\\n\\nWe made it easier to disallow potentially unsafe operators, formats, and\\nconnectors. The new `tenzir.disable-plugins` option is a list of names of\\nplugins to explicitly forbid from being used. For example, adding `export` will\\nprohibit use of the `export` operator builtin, thereby disabling the ability to\\nrun historical queries. This method allows for a more fine-grained control than\\nthe coarse `tenzir.allow-unsafe-pipelines` option.\\n\\nWhy does it matter? Well, when running pipelines in a node, some operators allow\\nyou to fully interact with the system through a pipeline. The `shell` operator\\nis the best example, which allows for arbitrary command execution. This can be\\nboth a huge relief and serve as escape hatch to integrate third-party tools, but\\nit is equally a security risk.\\n\\n## This & That\\n\\n- When you deploy a demo node at [app.tenzir.com](https://app.tenzir.com), it\\n  now starts up faster, and the pre-loaded pipelines come with labels and have been\\n  ported to use the new `api` operator instead of relying on `curl` for setup.\\n\\n- It is now possible to reference nested records in many operators that wrangle\\n  data, such as `select`, `extend`, `put`, and `replace`.\\n\\n- The `summarize` operator now yields a result even if it receives no input\\n  (assuming there is no grouping with `by`). For example, `summarize\\n  num=count(foo)` returns `{\\"num\\": 0}` instead of returning nothing.\\n\\n- The `import` operator now flushes events to disk automatically before\\n  returning, ensuring that they are available immediately for subsequent uses of\\n  the `export` operator.\\n\\nWe provide a full list of changes [in our changelog](/changelog#v450).\\n\\nHead over to [app.tenzir.com](https://app.tenzir.com) to check out what\'s new.\\nGot questions? Swing by our friendly [our Discord server](/discord) and let us\\nknow."},{"id":"/tenzir-v4.4","metadata":{"permalink":"/releases/tenzir-v4.4","source":"@site/releases/tenzir-v4.4/index.md","title":"Tenzir v4.4","description":"Tenzir v4.4 is out!","date":"2023-11-06T00:00:00.000Z","formattedDate":"November 6, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"operators","permalink":"/releases/tags/operators"},{"label":"velociraptor","permalink":"/releases/tags/velociraptor"},{"label":"yara","permalink":"/releases/tags/yara"},{"label":"amqp","permalink":"/releases/tags/amqp"}],"readingTime":2.535,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.4","authors":["dominiklohmann"],"date":"2023-11-06T00:00:00.000Z","last_updated":"2024-12-10T00:00:00.000Z","tags":["release","operators","velociraptor","yara","amqp"],"comments":true},"prevItem":{"title":"Tenzir v4.5","permalink":"/releases/tenzir-v4.5"},"nextItem":{"title":"Tenzir v4.3","permalink":"/releases/tenzir-v4.3"}},"content":"[Tenzir v4.4](https://github.com/tenzir/tenzir/releases/tag/v4.4.0) is out!\\nWe\'ve focused this release on integrations with two pillars of the digital\\nforensics and incident response (DFIR) ecosystem: [YARA][yara] and\\n[Velociraptor][velociraptor].\\n\\n[yara]: https://yara.readthedocs.io\\n[velociraptor]: https://docs.velociraptor.app\\n\\n![Tenzir v4.4](tenzir-v4.4.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## YARA Operator\\n\\nThe star feature of this release is the new `yara` operator. You can now match\\n[YARA][yara] rules directly within byte pipelines. This is a game-changer for\\nthreat intelligence and cybersecurity workflows, as it brings together all of\\nTenzir\'s connectors with the community\'s rich ecosystem of YARA rules for\\nefficient malware detection and analysis. Evaluating a set of rules on a file\\nlocated in an S3 bucket has never been easier:\\n\\n```\\nload s3 bucket/file.exe\\n| yara path/to/rules/\\n```\\n\\n:::info\\nWe\'ve written a blog post on the YARA operator that shows just how it works and\\nexplains in-depth how you can use it: [Matching YARA Rules in Byte\\nPipelines](/archive/matching-yara-rules-in-byte-pipelines)\\n:::\\n\\n## Velociraptor Operator\\n\\n[Velociraptor][velociraptor] is an advanced DFIR tool that enhances your\\nvisibility into your endpoints. Not unlike our own TQL, Velociraptor comes with\\nits own language for interacting with it programmatically: VQL. The\\n`velociraptor` operator makes it possible to submit VQL queries to a\\nVelociraptor server, as well as subscribe to artifacts in hunt flows over a\\nlarge fleet of assets, making endpoint telemetry collection and processing a\\nbreeze.\\n\\n:::info\\nRead our blog post on how we built this integration and how you can utilize it:\\n[Integrating Velociraptor into Tenzir\\nPipelines](/archive/integrating-velociraptor-into-tenzir-pipelines)\\n:::\\n\\n## AMQP Connector\\n\\nThe new `amqp` connector brings a full-fledged AMQP 0-9-1 client to the table.\\nRelying on the battle-proven [RabbitMQ C client\\nlibrary](https://github.com/alanxz/rabbitmq-c), the operator makes it possible\\nyou to interact with queues and exchanges as shown in the diagram below:\\n\\n![AMQP](amqp.excalidraw.svg)\\n\\n## Noteworthy Improvements\\n\\nBesides the new operators, I would like to highlight the following changes:\\n\\n- **Live Exports:** Start your pipeline with `export --live` to get all events\\n  in one pipeline as they are imported.\\n\\n- **Blob Type:** We\'ve added a new `blob` type that allows you to handle binary\\n  data. Use the `blob` type over the `string` type for binary payloads that are\\n  not UTF8-encoded.\\n\\n- **Rich Schema Inference for CSV:** Inferring schemas for CSV files has been\\n  significantly enhanced. It now provides more precise types, leading to more\\n  insightful analysis.\\n\\n- **Automated Pipeline Management:** New controls for auto-restart, auto-delete\\n  and a runtime limit are now available when creating a pipeline. For a more\\n  granular control of the auto-restart and auto-delete behavior, the _Stopped_\\n  state for pipeline has now been divided into _Stopped_, _Completed_, and\\n  _Failed_. The states reflect whether a pipeline was manually stopped, ended\\n  naturally, or encountered an error, respectively.\\n\\n- **Label Support for Pipelines:** You can now visually group related pipelines\\n  using the new labels feature. This helps you in organizing your pipelines\\n  better for improved visibility and accessibility.\\n\\nWe provide a full list of changes [in our changelog](/changelog#v440).\\n\\nCheck out the new features on [app.tenzir.com](https://app.tenzir.com). We\'re\\nexcited to see the amazing things you will accomplish with them!\\n\\nYour feedback matters and drives our growth. Join the discussion in [our\\nDiscord](/discord)!"},{"id":"/tenzir-v4.3","metadata":{"permalink":"/releases/tenzir-v4.3","source":"@site/releases/tenzir-v4.3/index.md","title":"Tenzir v4.3","description":"Exciting times, Tenzir v4.3 is out! The headlining feature is [Fluent","date":"2023-10-10T00:00:00.000Z","formattedDate":"October 10, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"operators","permalink":"/releases/tags/operators"},{"label":"observability","permalink":"/releases/tags/observability"},{"label":"fluent-bit","permalink":"/releases/tags/fluent-bit"},{"label":"json","permalink":"/releases/tags/json"},{"label":"yaml","permalink":"/releases/tags/yaml"},{"label":"labels","permalink":"/releases/tags/labels"}],"readingTime":6.575,"hasTruncateMarker":true,"authors":[{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir v4.3","authors":["jachris","mavam"],"date":"2023-10-10T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","operators","observability","fluent-bit","json","yaml","labels"]},"prevItem":{"title":"Tenzir v4.4","permalink":"/releases/tenzir-v4.4"},"nextItem":{"title":"Tenzir v4.2","permalink":"/releases/tenzir-v4.2"}},"content":"Exciting times, Tenzir v4.3 is out! The headlining feature is [Fluent\\nBit][fluentbit] support with the `fluent-bit` source and sink operators. Imagine\\nyou can use all Fluent Bit connectors *plus* what Tenzir already offers. What a\\ntreat!\\n\\n[fluentbit]: https://fluentbit.io/\\n\\n![Tenzir v4.3](tenzir-v4.3.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Fluent Bit on Steroids\\n\\n[Fluent Bit][fluentbit] is a remarkable piece of open source software that\\noffers observability pipelines\u2014quite similar to Tenzir actually. That said, our\\ntarget audience is different: rather than targeting observability, we focus on\\nthe intersection of the security and data.\\n\\nBy bringing the two ecosystems together, you, dear user, benefit from the union\\nof features. Before diving into some examples, let\'s briefly compare the tech.\\nFluent Bit features [inputs][inputs] and [outputs][outputs] to get data in and\\nout of the ecosystem. These are equivalent to Tenzir\'s connectors. Fluent Bit\\nalso has [parsers][parsers] that map to the equally named concept of Tenzir\\nparsers. Fluent Bit\'s [filters][filters] would be implemented as\\n*transformations* in Tenzir, i.e., operators that have a non-void input and\\noutput. The diagram illustrates these relationships:\\n\\n[inputs]: https://docs.fluentbit.io/manual/pipeline/inputs\\n[outputs]: https://docs.fluentbit.io/manual/pipeline/outputs\\n[parsers]: https://docs.fluentbit.io/manual/pipeline/parsers\\n[filters]: https://docs.fluentbit.io/manual/pipeline/filters\\n\\n![Tenzir vs. Fluent Bit](tenzir-vs-fluentbit.excalidraw.svg)\\n\\nIt\'s important to note that Tenzir pipelines separate I/O and computation.\\nConnectors do I/O and formats are responsible for (un)structuring data. The data\\npaths are symmetric in that a loader ships bytes to a parser that in turn\\nproduces events, and a printer accepts events and turns them in to bytes that\\na corresponding saver sends away.\\n\\nWe implemented the `fluent-bit` operator as a fusion of connector and format. We\\ndid not integrate Fluent Bit\'s powerful parser abstraction, as we have an\\nexisting framework in place for that. Similarly, we did not integrate Fluent\\nBit\'s filters, as we have a variety of transformation operators for that.\\n\\n### How do I use it?\\n\\nThe `fluent-bit` source and sink operators are where the action happens. They\\nhave the following syntax:\\n\\n```\\nfluent-bit <plugin> [<key=value>..]\\n```\\n\\nBoth operators are very similar to the `fluent-bit` command line tool, which\\nhas the usage `fluent-bit -i <input> -o <output> -p key=value`. In Tenzir, the\\nsource operator implies the options `-i` and the sink `-o`, so you don\'t have to\\nwrite them. Similarly, appending properties in the form of key-value pairs is\\nso common that you can omit the `-p` options.\\n\\nLet\'s walk through some examples. Say you want to sample three values with\\nFluent Bit\'s [`random`][random] input:\\n\\n[random]: https://docs.fluentbit.io/manual/pipeline/inputs/random\\n\\n```bash\\ntenzir \'fluent-bit random | head 3 | write json -c\'\\n```\\n\\nThis prints:\\n\\n```json\\n{\\"timestamp\\": \\"2023-09-23T07:56:47.957369\\", \\"message\\": {\\"rand_value\\": 8106944690543729752}}\\n{\\"timestamp\\": \\"2023-09-23T07:56:48.959997\\", \\"message\\": {\\"rand_value\\": 2072095294278847853}}\\n{\\"timestamp\\": \\"2023-09-23T07:56:49.959988\\", \\"message\\": {\\"rand_value\\": 5606209024700423100}}\\n```\\n\\nRegarding the framing: [Fluent Bit\'s event format][event-format] produces events\\nin the form of an array that can have one of two possible shapes:\\n\\n1. `[[TIMESTAMP, METADATA], MESSAGE]`\\n2. `[TIMESTAMP, MESSAGE]`\\n\\n[event-format]: https://docs.fluentbit.io/manual/concepts/key-concepts#event-format\\n\\nWe convert this into an event record with 3 fields:\\n\\n1. `timestamp`: the event timestamp\\n2. `metadata`: object with key-value pairs\\n3. `message`: arbitrary object with inferred schema\\n\\nThe field `metadata` is optional, as shown in the above example.\\n\\nMany Fluent Bit inputs perform network I/O. Here\'s a TCP socket example:\\n\\n```bash\\n# Terminal A\\ntenzir \'fluent-bit tcp\'\\n# Terminal B\\necho \'{\\"foo\\": {\\"bar\\": 42}}\' | nc 127.0.0.1 5170\\n```\\n\\nThis outputs in terminal A:\\n\\n```json\\n{\\n  \\"timestamp\\": \\"2023-09-23T09:35:10.623745\\",\\n  \\"message\\": {\\n    \\"foo\\": {\\n      \\"bar\\": 42\\n    }\\n  }\\n}\\n```\\n\\nLet\'s pick another input, [`opentelemetry`][opentelemetry]:\\n\\n[opentelemetry]: https://docs.fluentbit.io/manual/pipeline/inputs/opentelemetry\\n\\n```bash\\ntenzir \'fluent-bit opentelemetry\'\\n```\\n\\nThis opens a socket on port 4318 that you can send now telemetry to. Instead of\\n`curl`, we\'re using our own HTTPie-like `http` connector to issue a POST\\nrequest:\\n\\n```bash\\ntenzir \'from http POST 127.0.0.1:4318/v1/logs resourceLogs:=[{\\"resource\\":{},\\"scopeLogs\\":[{\\"scope\\":{},\\"logRecords\\":[{\\"timeUnixNano\\":\\"1660296023390371588\\",\\"body\\":{\\"stringValue\\":\\"{\\\\\\"message\\\\\\":\\\\\\"dummy\\\\\\"}\\"},\\"traceId\\":\\"\\",\\"spanId\\":\\"\\"}]}]}]\'\\n```\\n\\nYou should then see:\\n\\n```json\\n{\\n  \\"timestamp\\": \\"2022-08-12T09:20:24.698112\\",\\n  \\"message\\": {\\n    \\"log\\": {\\n      \\"message\\": \\"dummy\\"\\n    }\\n  }\\n}\\n```\\n\\nMore powerful inputs mimic other applications, like [Splunk][splunk] or\\n[ElasticSearch][elasticsearch]. Want Tenzir to be like Splunk via Fluent Bit?\\nHere you go:\\n\\n[splunk]: https://docs.fluentbit.io/manual/pipeline/inputs/splunk\\n[elasticsearch]: https://docs.fluentbit.io/manual/pipeline/inputs/elasticsearch\\n\\n```bash\\ntenzir \'fluent-bit splunk\'\\n```\\n\\nYou just got a Splunk HEC API waiting for you at port 9880. This is one of the\\nmost amazing things about this integration. The entire Fluent Bit connector\\necosystem is now at your fingertips!\\n\\nThis extends to the outputs as well. Most mundanely, you can use the `stdout`\\noutput from Fluent Bit as follows:\\n\\n```bash\\ntenzir \'show operators | head 3 | fluent-bit stdout\'\\n```\\n\\nThis yields:\\n\\n```\\n[0] lib.0: [[1695494117.866096973, {}], {\\"name\\"=>\\"batch\\", \\"source\\"=>false, \\"transformation\\"=>true, \\"sink\\"=>false}]\\n[1] lib.0: [[1695494117.866101980, {}], {\\"name\\"=>\\"compress\\", \\"source\\"=>false, \\"transformation\\"=>true, \\"sink\\"=>false}]\\n[2] lib.0: [[1695494117.866103887, {}], {\\"name\\"=>\\"decapsulate\\", \\"source\\"=>false, \\"transformation\\"=>true, \\"sink\\"=>false}]\\n```\\n\\nWant to send Suricata alerts to Slack? Here is your pipeline:\\n\\n```\\nfrom file --follow eve.json\\n| where #schema == \\"suricata.alert\\"\\n| fluent-bit slack webhook=https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\\n```\\n\\nOr send \'em to Splunk by changing the sink to:\\n\\n```\\nfluent-bit splunk host=127.0.0.1 port=8088 tls=on tls.verify=off\\n```\\n\\nOh wait, Elastic? Here you go:\\n\\n```\\nfluent-bit es host=192.168.2.3 port=9200 index=my_index type=my_type\\n```\\n\\nHopefully the general pattern is clear now.\\n\\nFinally, there\'s a cool property of Fluent Bit: it\'s symmetric like Tenzir.\\nRemember how you can use ZeroMQ sockets to bridge pipelines?\\n\\n```bash\\n# Terminal A\\ntenzir \'from zmq\'\\n# Terminal B\\ntenzir \'show version | to zmq\'\\n```\\n\\nYou can do the same with Fluent Bit\'s *Forward* protocol:\\n\\n```bash\\n# Terminal A\\ntenzir \'fluent-bit forward\'\\n# Terminal B\\ntenzir \'show version | fluent-bit forward\'\\n```\\n\\n(We\'ll leave it to you to do the same with Kafka.)\\n\\n:::info Implementation: MsgPack vs. Arrow\\nFluent Bit uses [MsgPack](https://msgpack.org/) internally, a binary version of\\nJSON with a slightly richer set of types. Once a Fluent Bit input onboards\\ndata into the internal format, all operations compute on MsgPack. And before\\ndata exits Fluent Bit, it gets converted from MsgPack to the native format of\\nthe output.\\n\\nIncidentally, Tenzir also had an optional MsgPack implementation of its data\\nplane. However, we dropped the MsgPack encoding and switched to [Apache\\nArrow](https://arrow.apache.org) exclusively. The reason is that most of our\\nworkloads are analytical, where a columnar representation (especially with large\\nbatching) outperforms due data locality and the ability to tap into vectorized\\ncomputations. Moreover, our objective is to soon integrate natively with several\\ndata tools, such as DuckDB, pandas, polars, etc.\u2014all of which speak Arrow.\\n:::\\n\\nWant to try it yourself? Head over to [app.tenzir.com](https://app.tenzir.com)\\nwhere you start for free and manage Tenzir nodes and run Tenzir and Fluent Bit\\npipelines.\\n\\n## Tidbits\\n\\nBesides Fluent Bit, the team at Tenzir has been working on some other\\nnoteworthy improvements and features that we would like to share:\\n\\n### JSON Parser Improvements\\n\\nWe\'ve revamped our JSON parser to be a lot faster and more accurate in type\\ninference.\\n\\n![Tenzir v4.3 JSON Improvements](tenzir-v4.3-json-improvements.excalidraw.svg)\\n\\nSchema inference now supports empty records and empty lists. Previously both\\nwere indistinguishable from `null` values. This is best illustrated on an\\nexample:\\n\\n```json\\n{\\"foo\\": [], \\"bar\\": {}, \\"baz\\": \\"127.0.0.1\\"}\\n{\\"foo\\": [null], \\"bar\\": null, \\"baz\\": \\"::1\\"}\\n{\\"foo\\": null, \\"bar\\": {}, \\"baz\\": \\"localhost\\"}\\n```\\n\\nWith Tenzir v4.2, The fields `foo` and `bar` would\'ve been dropped from the\\ninput, and `baz` had the type `string` for all three events.\\n\\nWith Tenzir v4.3, `foo` is of type `list<null>`, `bar` of type `record {}`, and\\nbaz of type `ip` for the first two events, and of type `string` for the third.\\n\\n### YAML Format\\n\\nThe new `yaml` format supports reading and writing YAML documents and document\\nstreams.\\n\\nFor example, you can now render the configuration of the current node as valid\\nYAML:\\n\\n```\\nshow config | write yaml\\n```\\n\\nThis yields:\\n\\n```yaml\\n---\\ntenzir:\\n  allow-unsafe-pipelines: true\\n  operators:\\n    suricata: \\"shell \'suricata -r /dev/stdin --set outputs.1.eve-log.filename=/dev/stdout --set logging.outputs.0.console.enabled=no\' | read suricata\\\\n\\"\\n    zeek: \\"shell \'eval \\\\\\"$(zkg env)\\\\\\" && zeek -r - LogAscii::output_to_stdout=T JSONStreaming::disable_default_logs=T JSONStreaming::enable_log_rotation=F json-streaming-logs\' | read zeek-json --no-infer\\\\n\\"\\n...\\n```\\n\\nAnother example, perhaps just a party tricks, is converting YAML to JSON:\\n\\n```bash\\ntenzir \'read yaml | write json\' < input.json\\n```\\n\\n### Pipeline Labels\\n\\nNodes now support setting labels for pipelines. This feature isn\'t yet enabled\\nin the app, but will be available soon for all nodes updated to v4.3 or newer."},{"id":"/tenzir-v4.2","metadata":{"permalink":"/releases/tenzir-v4.2","source":"@site/releases/tenzir-v4.2/index.md","title":"Tenzir v4.2","description":"We\'ve just released Tenzir v4.2 that introduces two new connectors: S3 and","date":"2023-09-19T00:00:00.000Z","formattedDate":"September 19, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"pipelines","permalink":"/releases/tags/pipelines"},{"label":"connectors","permalink":"/releases/tags/connectors"},{"label":"s3","permalink":"/releases/tags/s-3"},{"label":"gcs","permalink":"/releases/tags/gcs"},{"label":"zmq","permalink":"/releases/tags/zmq"}],"readingTime":6.91,"hasTruncateMarker":true,"authors":[{"name":"Daniel Kostuj","title":"Software Engineer","url":"https://github.com/dakostu","email":"daniel@tenzir.com","imageURL":"https://github.com/dakostu.png","key":"dakostu"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir v4.2","authors":["dakostu","mavam"],"date":"2023-09-19T00:00:00.000Z","tags":["release","pipelines","connectors","s3","gcs","zmq"]},"prevItem":{"title":"Tenzir v4.3","permalink":"/releases/tenzir-v4.3"},"nextItem":{"title":"Tenzir v4.1","permalink":"/releases/tenzir-v4.1"}},"content":"We\'ve just released Tenzir v4.2 that introduces two new connectors: [S3][s3] and\\n[GCS][gcs] for interacting with blob storage and [ZeroMQ][zeromq] for writing\\ndistributed multi-hop pipelines. There\'s also a new [`lines`][lines] parser for\\neasier text processing and a bunch of PCAP quality-of-life improvements.\\n\\n[s3]: https://aws.amazon.com/s3/\\n[gcs]: https://cloud.google.com/storage\\n[zeromq]: https://zeromq.org/\\n[lines]: /formats/lines\\n\\n![Tenzir v4.2](tenzir-v4.2.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## S3 Saver & Loader\\n\\nThe new [`s3`](/tql2/operators/load_s3) connector hooks up Tenzir to the vast\\ndata masses on [Amazon S3](https://aws.amazon.com/s3/) and S3-compatible object\\nstorage systems. With the `s3` loader, you can access objects on S3 buckets,\\nassuming you have the proper credentials provided:\\n\\n```bash\\ntenzir \'from s3 s3://bucket/mystuff/file.json\'\\n```\\n\\nInternally, we are using Arrow\'s filesystem abstraction for establishing\\nconnections. This abstraction already handles AWS\'s default credentials provider\\nchain. If you have set up your AWS account in this chain, then you don\'t need to\\nworry about setting it up again in config files or similar formats.\\n\\nS3 buckets can also be public, meaning you don\'t need any specific credentials\\nto access the objects therein. AWS offers tons of such public (read-only)\\nbuckets with scientific data on their [Marketplace][marketplace]. Tenzir can\\nalso consume public read-only data\u2014for example, e.g., some [population density &\\ndemographic estimate data][density]:\\n\\n[marketplace]: https://aws.amazon.com/marketplace/search/results?trk=8384929b-0eb1-4af3-8996-07aa409646bc&sc_channel=el&FULFILLMENT_OPTION_TYPE=DATA_EXCHANGE&CONTRACT_TYPE=OPEN_DATA_LICENSES&DATA_AVAILABLE_THROUGH=S3_OBJECTS&PRICING_MODEL=FREE&filters=FULFILLMENT_OPTION_TYPE%2CCONTRACT_TYPE%2CDATA_AVAILABLE_THROUGH%2CPRICING_MODEL\\n[density]: https://aws.amazon.com/marketplace/pp/prodview-jf2hjpr2mrj4m?sr=0-2&ref_=beagle&applicationId=AWSMPContessa#overview\\n\\n```\\nload s3 dataforgood-fb-data/csv/month=2019-06/country=VGB/type=children_under_five/VGB_children_under_five.csv.gz\\n| decompress gzip\\n| read csv\\n```\\n\\nLet\'s combine this with `aws s3 ls` to receive all [Amazon product Q&A humor\\ndetection data][humor]:\\n\\n[humor]: https://aws.amazon.com/marketplace/pp/prodview-b53zm25dl3jcc?sr=0-3&ref_=beagle&applicationId=AWSMPContessa#overview\\n\\n```bash\\naws s3 ls --no-sign-request --recursive humor-detection-pds/ |\\n  awk \'{print $4}\' |\\n  grep \\"\\\\.csv\\" |\\n  xargs -I {} tenzir \\"from s3 humor-detection-pds/{} read csv\\"\\n```\\n\\nThe original CSV data is a bit unpolished, e.g., there are line breaks\\nand superfluous commas in the middle of some values. Tenzir\'s `csv` parser\\nwill ignore those lines, but the rest of the data is at your fingertips.\\n\\nThe `s3` writer uploads the pipeline output to an object in the bucket:\\n\\n```bash\\ntenzir \\"export | to s3 s3://mybucket/folder/ok.json\\"\\n```\\n\\nYou can provide options to an S3 as [URI query parameters][uri]:\\n\\n> For S3, the options that can be included in the URI as query parameters are\\n> `region`, `scheme`, `endpoint_override`, `access_key`, `secret_key`,\\n> `allow_bucket_creation`, and `allow_bucket_deletion`.\\n\\n[uri]: https://arrow.apache.org/docs/10.0/r/articles/fs.html#uri-options\\n\\nThe most exciting of these options would be `endpoint_override`, as it allows\\nyou to connect to different endpoints of other S3-compatible storage systems:\\n\\n```\\nfrom s3 s3://examplebucket/test.json?endpoint_override=s3.us-west.mycloudservice.com\\n```\\n\\nThe `s3` connector is a huge step for Tenzir\'s capability to interact with blob\\nstorage. Our list of connectors is continuously growing and our modular\\nframework allows for cranking out many more at ease. More connectors, more data,\\nmore information, more value!\\n\\n## Google Cloud Storage (GCS) Connector\\n\\nSimilar to the `s3` connector we added the [`gcs`](/tql2/operators/load_gcs)\\nconnector that interfaces to [Google Cloud Storage\\n(GCS)](https://cloud.google.com/storage).\\n\\nThe connector tries to retrieve the appropriate credentials using Google\'s\\n[Application Default Credentials](https://google.aip.dev/auth/4110). This means\\nyou can use the connector conveniently to read from or write to a storage\\nbucket:\\n\\n```\\nfrom gcs gs://bucket/path/to/file\\nto gcs gs://bucket/path/to/file\\n```\\n\\nAs with `s3`, you can also use override the default endpoint and other options\\nby passing URI query parameters. Have a look at the [connector\\ndocumentation](/tql2/operators/load_gcs) for further details.\\n\\n## ZeroMQ Saver & Loader\\n\\nThe new [`zmq`](/tql2/operators/load_zmq) connector makes it easy to interact\\nwith the raw bytes in [ZeroMQ][zeromq] messages. We model the `zmq` *loader* as\\nsubscriber with a `SUB` socket, and the *saver* as a publisher with the `PUB`\\nsocket:\\n\\n![ZeroMQ Connector](zeromq-connector.excalidraw.svg)\\n\\nWhat\'s nice about ZeroMQ is that the directionality of connection establishment\\nis independent of the socket type. So either end can bind or connect. We opted\\nfor the subscriber to connect by default, and the publisher to bind. You can\\noverride this with the `--bind` and `--connect` flags.\\n\\nEven though we\'re using a lossy `PUB`-`SUB` socket pair, we\'ve added a thin\\nlayer of reliability in that a Tenzir pipeline won\'t send or receive ZeroMQ\\nmessages before it has at least one connected socket.\\n\\nWant to exchange and convert events with two single commands? Here\'s how you\\npublish JSON and continue as CSV on the other end:\\n\\n```bash\\n# Publish some data via a ZeroMQ PUB socket:\\ntenzir \'show operators | to zmq write json\'\\n# Subscribe to it in another process\\ntenzir \'from zmq read json | write csv\'\\n```\\n\\nYou can also work with operators that use types. Want to send away chunks of\\nnetwork packets to a remote machine? Here you go:\\n\\n```bash\\n# Publish raw bytes:\\ntenzir \'load nic eth0 | save zmq\'\\n# Tap into the raw feed at the other end and start parsing:\\ntenzir \'load zmq | read pcap | decapsulate\'\\n```\\n\\nNeed to expose the source side of a pipeline as a listening instead of\\nconnecting socket? No problem:\\n\\n```bash\\n# Bind instead of connect with the ZeroMQ SUB socket:\\ntenzir \'from zmq --bind\'\\n```\\n\\nThese examples show the power of composability: Tenzir operators work with both\\nbytes and events, enabling in-flight reshaping, format conversation, or simply\\ndata shipping at ease.\\n\\n## HTTP and FTP Loader\\n\\nWe\'ve added a new round of loaders for HTTP and FTP, named `http`, `https`,\\n`ftp`, and `ftps`. This makes it a lot easier to pull data into a pipeline that\\nlives at a remote web or file server. No more `shell curl` shenanigans!\\n\\nWe modeled the `http` and `https` loaders after [HTTPie](https://httpie.io/),\\nwhich comes with an expressive and intuitive command-line syntax. We recommend\\nto study the [HTTPie documentation](https://httpie.io/docs/cli/examples) to\\nunderstand the full extent of the command-line interface. In many cases, you can\\nperform an *exact* copy of the HTTPie command line and use it drop-in with the\\nHTTP loader, e.g., the invocation\\n\\n```bash\\nhttp PUT pie.dev/put X-API-Token:123 foo=bar\\n```\\n\\nbecomes\\n\\n```\\nfrom http PUT pie.dev/put X-API-Token:123 foo=bar\\n```\\n\\nMore generally, if your HTTPie command line is `http X` then you can write `from\\nhttp X` to obtain an event stream or `load http X` for a byte stream. (Note that\\nwe have only the parts of the HTTPie syntax most relevant to our users.)\\n\\nInternally, we rely on [libcurl](https://curl.se/libcurl/) to perform the actual\\nfile transfer. It is noteworthy that libcurl supports *a lot* of protocols:\\n\\n> libcurl is a free and easy-to-use client-side URL transfer library, supporting\\n> DICT, FILE, FTP, FTPS, GOPHER, GOPHERS, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS,\\n> MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS,\\n> TELNET and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP\\n> uploading, HTTP form based upload, proxies, HTTP/2, HTTP/3, cookies,\\n> user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file\\n> transfer resume, http proxy tunneling and more!\\n\\n[Let us know](/discord) if you have use cases for any of these. Let\'s take a\\nlook at some more that you can readily work with.\\n\\nDownload and process a [CSV](/formats/csv) file:\\n\\n```\\nfrom http http://example.org/file.csv read csv\\n```\\n\\nProcess a Zstd-compressed [Zeek TSV](/formats/zeek-tsv) file:\\n\\n```\\nload https https://example.org/gigantic.log.zst\\n| decompress zstd\\n| read zeek-tsv\\n```\\n\\nImport a [CEF](/formats/cef) log from an FTP server into a Tenzir node:\\n\\n```\\nload ftp ftp://example.org/cef.log read cef\\n| import\\n```\\n\\n## Lines Parser\\n\\nThe new [`lines`][lines] parser splits its input at newline characters and\\nproduces events with a single field representing the line. This parser is\\nespecially useful for onboarding line-based text files into pipelines.\\n\\nThe `-s|--skip-empty` flags ignores empty lines. For example, read a text file\\nas follows:\\n\\n```\\nfrom file /tmp/test.txt read lines --skip-empty\\n```\\n\\n## Concatenating PCAPs\\n\\nThe [`pcap`](/formats/pcap) parser can now read concatenated PCAP files,\\nallowing you to easily process large amounts of trace files. This comes\\nespecially handy on the command line:\\n\\n```bash\\ncat *.pcap | tenzir \'read pcap\'\\n```\\n\\nThe [`nic`](/tql2/operators/load_nic) loader has a new flag\\n`--emit-file-headers` that prepends a PCAP file header for every batch of bytes\\nthat it produces, yielding a stream of concatenated PCAP files. This gives rise\\nto creative use cases involving packet shipping. For example, to ship blocks of\\npackets as \\"micro traces\\" via 0mq, you could do:\\n\\n```\\nload nic eth0\\n| save zmq\\n```\\n\\nThis creates 0mq PUB socket where subscribes can come and go. Each 0mq message\\nis a self-contained PCAP trace, which avoids painful resynchronization logic.\\nYou can consume this feed with a remote subscriber:\\n\\n```\\nload zmq\\n| read pcap\\n```\\n\\nFinally, we also made it easier to identify available network interfaces when\\nusing the `nic` loader: `show nics` now returns a list of available interfaces."},{"id":"/tenzir-v4.1","metadata":{"permalink":"/releases/tenzir-v4.1","source":"@site/releases/tenzir-v4.1/index.md","title":"Tenzir v4.1","description":"After our successful launch of app.tenzir.com of Tenzir v4.0 at","date":"2023-08-31T00:00:00.000Z","formattedDate":"August 31, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"pipelines","permalink":"/releases/tags/pipelines"},{"label":"operators","permalink":"/releases/tags/operators"},{"label":"app","permalink":"/releases/tags/app"}],"readingTime":3.795,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.1","authors":["dominiklohmann"],"date":"2023-08-31T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","pipelines","operators","app"]},"prevItem":{"title":"Tenzir v4.2","permalink":"/releases/tenzir-v4.2"},"nextItem":{"title":"VAST v3.1","permalink":"/releases/vast-v3.1"}},"content":"After our successful launch of [app.tenzir.com][tenzir-app] of Tenzir v4.0 at\\nBlack Hat, [the new v4.1 release][github-release] continues with several\\nenhancements based on early feedback. We bring to you a (i) new mechanism to\\npause pipelines, (ii) a new operator to match Sigma rules, (iii) new operators\\nfor in-pipeline (de)compression, and (iv) a revamp of the `show` operator.\\n\\n[github-release]: https://github.com/tenzir/vast/releases/tag/v4.1.0\\n[tenzir-app]: https://app.tenzir.com\\n\\n![Tenzir v4.1](tenzir-v4.1.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Pausing Pipelines\\n\\nTenzir now supports pausing pipelines. Previously, users were able to deploy and\\nstart pipelines, and to let them run until they completed (or failed) or\\nmanually stopped. With new paused state, we now have the following five possible\\nstates of a pipeline:\\n\\n- *Created*: The pipeline was created, but never started\\n- *Running*: The pipeline is processing data\\n- *Paused*: A running pipeline has been suspended but still retains its\\n  in-memory state\\n- *Stopped*: A pipeline is not running and has no in-memory state\\n- *Failed*: The pipeline stopped unexpectedly\\n\\nPausing suspends a pipeline in an instant, causing its execution to stop. When a\\npaused pipeline is started, it resumes right where it left off as opposed to\\nrestarting all the way from the beginning.\\n\\nWe\'re also making use of this feature in the app with a new pause action button.\\nA new tab bar at the top makes it easy to see all pipeline states at a glance.\\nThese features will be enabled in the coming days in the app. Here\'s a sneak\\npreview of a mock from our designer:\\n\\n![Pause Feature Mock](pause-mock.png)\\n\\nYou can also pause a pipeline through the API. Use the [/pipeline/update\\nendpoint][update-endpoint] with the new `pause` action to suspend a pipeline.\\nThe `start` action resumes a pipeline that is currently paused.\\n\\n[update-endpoint]: https://docs.tenzir.com/api#/paths/~1pipeline~1update/post\\n\\n## Sigma Operator\\n\\nThe experimental `sigma` operator executes [Sigma rules][sigma-github] on its\\ninput and outputs matching events. The operator can work both on files and on\\ndirectories of rules. Rule directories may be updated while the operator is\\nrunning, so that adding a new rule to an already deployed pipeline is as simple\\nas dropping a Sigma rule into the configured directory.\\n\\nConsider the `sigma` operator as one concrete instance of security content\\nexecution that we enable live and retrospectively. For example, you can perform\\nhistorical matching via `export | sigma` and streaming execution over a Kafka\\nsource via `from kafka --topic events | sigma`. Now that we have the capability\\nin place, we are working on a unified interface to live and retro matching.\\n\\n[sigma-github]: https://github.com/SigmaHQ/sigma\\n\\n## Show Operator\\n\\nThe experimental `show` operator supersedes the `version` operator. Use\\n`show <aspect>` to show various aspects of a Tenzir node. The following aspects\\nare currently available:\\n\\n- `build`: shows compile-time build information.\\n- `config`: shows all current configuration options.\\n- `connectors`: shows all available connectors.\\n- `dependencies`: shows information about build-time dependencies.\\n- `fields`: shows all fields of existing tables at a remote node.\\n- `formats`: shows all available formats.\\n- `operators`: shows all available operators.\\n- `partitions`: shows all table partitions of a remote node.\\n- `pipelines`: shows all managed pipelines of a remote node.\\n- `plugins`: shows all loaded plugins.\\n- `types`: shows all known types at a remote node.\\n- `version`: shows the Tenzir version.\\n\\nThis enables powerful introspection use-cases. Here are some examples that we\\nfound useful at Tenzir.\\n\\nShow all running pipelines with an ingress of over 10 MiB:\\n\\n```\\nshow pipelines\\n| where total.ingress.num_approx_bytes > 10 Mi\\n```\\n\\nShow the memory usage of the node\'s catalog by schema in descending order:\\n\\n```\\nshow partitions\\n| summarize memory_usage=sum(memory_usage) by schema\\n| sort memory_usage desc\\n```\\n\\nPrint the configured endpoint of the node (returns `null` if not customized):\\n\\n```\\nshow config\\n| put tenzir.endpoint\\n```\\n\\n## Compress and Decompress Operators\\n\\nThe `compress` and `decompress` operators make it easy to read and write\\ncompressed data. The operator uses Apache Arrow\'s compression utilities under\\nthe hood, and transparently supports all options that Apache Arrow supports for\\nstreaming compression. The currently supported codecs are `brotli`, `bz2`,\\n`gzip`, `lz4`, and `zstd`.\\n\\nFor example, the following pipeline creates a Gzip-compressed NDJSON file that\\ncontains all events that were previously imported at the node:\\n\\n```\\nexport\\n| write json --compact-output\\n| compress gzip\\n| save file /tmp/backup.json.gz\\n```\\n\\n## Small Things\\n\\nWe\'re constantly polishing the node\'s pipeline execution engine, and improving\\nthe app\'s usability. Since the last release, we\'ve improved pipeline execution\\nto make slow pipelines return their first results faster, and have improved the\\nrendering of the Explorer\'s results table for small result sets. The inline help\\nin the Explorer\'s pipeline editor is now more reliable."},{"id":"/vast-v3.1","metadata":{"permalink":"/releases/vast-v3.1","source":"@site/releases/vast-v3.1/index.md","title":"VAST v3.1","description":"VAST v3.1 is out. This is","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"pipelines","permalink":"/releases/tags/pipelines"},{"label":"operators","permalink":"/releases/tags/operators"}],"readingTime":1.805,"hasTruncateMarker":true,"authors":[{"name":"Tobias Mayer","title":"Software Architect","url":"https://github.com/tobim","email":"tobias@tenzir.com","imageURL":"https://github.com/tobim.png","key":"tobim"}],"frontMatter":{"title":"VAST v3.1","authors":["tobim"],"image":"/img/blog/vast-v3.1.excalidraw.svg","date":"2023-05-12T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","pipelines","operators"]},"prevItem":{"title":"Tenzir v4.1","permalink":"/releases/tenzir-v4.1"},"nextItem":{"title":"VAST v3.0","permalink":"/releases/vast-v3.0"}},"content":"[VAST v3.1](https://github.com/tenzir/vast/releases/tag/v3.1.0) is out. This is\\na small checkpointing release that brings a few new changes and fixes.\\n\\n\x3c!--truncate--\x3e\\n\\n## Pipelines Reloaded\\n\\nThe old pipeline execution engine is now gone and we updated VAST to use\\nthe new engine everywhere. Most notably this applies to the `export` command,\\nthe compaction engine, and the `query` REST interface.\\n\\nFor this release, we removed support for configuration level export and import\\npipelines. This feature will make a return in the next major release.\\n\\nWe also removed the deprecated YAML-based pipeline syntax to fully concentrate\\non the VAST Language.\\n\\n## Operator Updates\\n\\nWe introduced several new operators:\\n\\n- `tail`: limits the input to the last N events.\\n- `unique`: removes adjacent duplicates\\n- `measure`: replaces the input with incremental metrics describing the input.\\n- `version`: returns a single event displaying version information of VAST. (Now\\n  `show`.)\\n- `from`: produces events by combining a connector and a format.\\n- `read`: a short form of `from` that allows for omitting the connector.\\n- `to`: consumes events by combining a connector and format.\\n- `write`: a short form of `to` that allows for\\n  omitting the connector.\\n\\nAdditionally, the `put`, `replace`, and `extend` operators have been updated to\\nwork with selectors and extractors.\\n\\n## Operator Aliases\\n\\nYou can now define aliases for operators in the configuration file. Use it to\\nassign a short and reusable name for operators that would otherwise require\\nseveral arguments. For example:\\n\\n```yaml\\nvast:\\n  operators:\\n    aggregate_flows: |\\n       summarize\\n         pkts_toserver=sum(flow.pkts_toserver),\\n         pkts_toclient=sum(flow.pkts_toclient),\\n         bytes_toserver=sum(flow.bytes_toserver),\\n         bytes_toclient=sum(flow.bytes_toclient),\\n         start=min(flow.start),\\n         end=max(flow.end)\\n       by\\n         timestamp,\\n         src_ip,\\n         dest_ip\\n       resolution\\n         10 mins\\n```\\n\\nNow use it like a regular operator in a pipeline:\\n\\n```\\nfrom file read suricata | aggregate_flows\\n```\\n\\n## Notable Fixes\\n\\n### Improved IPv6 Subnet Handling\\n\\nThe handling of subnets in the IPv6 space received multiple fixes:\\n\\n- The expression `:ip !in ::ffff:0:0/96` now finds all events that\\n  contain IPs that cannot be represented as IPv4 addresses.\\n- Subnets with a prefix above 32 are now correctly formatted with\\n  an IPv6 network part, even if the address is representable as IPv4.\\n\\n### A More Resilient Systemd Service\\n\\nThe systemd unit for VAST now automatically restarts the node in case the\\nprocess went down."},{"id":"/vast-v3.0","metadata":{"permalink":"/releases/vast-v3.0","source":"@site/releases/vast-v3.0/index.md","title":"VAST v3.0","description":"VAST Language Evolution \u2014 Dataflow Pipelines","date":"2023-03-14T00:00:00.000Z","formattedDate":"March 14, 2023","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"pipelines","permalink":"/releases/tags/pipelines"},{"label":"language","permalink":"/releases/tags/language"},{"label":"cef","permalink":"/releases/tags/cef"},{"label":"performance","permalink":"/releases/tags/performance"},{"label":"introspection","permalink":"/releases/tags/introspection"},{"label":"regex","permalink":"/releases/tags/regex"}],"readingTime":9.14,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"},{"name":"Daniel Kostuj","title":"Software Engineer","url":"https://github.com/dakostu","email":"daniel@tenzir.com","imageURL":"https://github.com/dakostu.png","key":"dakostu"}],"frontMatter":{"title":"VAST v3.0","description":"VAST Language Evolution \u2014 Dataflow Pipelines","authors":["dominiklohmann","dakostu"],"image":"/img/blog/building-blocks.excalidraw.svg","date":"2023-03-14T00:00:00.000Z","tags":["release","pipelines","language","cef","performance","introspection","regex"]},"prevItem":{"title":"VAST v3.1","permalink":"/releases/vast-v3.1"},"nextItem":{"title":"VAST v2.4.1","permalink":"/releases/vast-v2.4.1"}},"content":"[VAST v3.0][github-vast-release] is out. This release brings some major updates\\nto the the VAST language, making it easy to write down dataflow pipelines that\\nfilter, reshape, aggregate, and enrich security event data. Think of VAST as\\nsecurity data pipelines plus open storage engine.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v3.0.4\\n\\n\x3c!--truncate--\x3e\\n\\n![Pipelines and Storage](/img/blog/building-blocks.excalidraw.svg)\\n\\n## The VAST Language: Dataflow Pipelines\\n\\nStarting with v3.0, VAST introduces a new way to write pipelines, with a syntax\\nsimilar to [splunk](https://splunk.com), [Kusto][kusto],\\n[PRQL](https://prql-lang.org/), and [Zed](https://zed.brimdata.io/). Previously,\\nVAST only supported a YAML-like definition of pipelines in configuration files\\nto deploy them statically during import, export, or use them during compaction.\\n\\n[kusto]: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\\n\\nThe new syntax resembles the well-known Unix paradigm of command chaining. The\\ndifference to Unix pipelines is that VAST exchanges structured data between\\noperators. The `vast export` and `vast import` commands now accept such a\\npipeline as a string argument. Refer to the pipelines\\ndocumentation for more details on how to use the new pipeline\\nsyntax.\\n\\n:::info Pipeline YAML Syntax Deprecation\\nThis release introduces a transitional period from YAML-style to textual\\npipelines. The old YAML syntax for pipelines will be deprecated and removed\\naltogether in a future version. The new pipeline operators `head` and\\n`taste` have no YAML equivalent.\\n:::\\n\\n## Language Upgrades\\n\\nWe\'ve made some breaking changes to the the VAST language that we\'ve wanted to\\ndo for a long time. Here\'s a summary:\\n\\n1. We removed the term VASTQL: The VAST Query Language is now simply\\n   the VAST language, and \\"VAST\\" will supersede the \\"VASTQL\\" abbreviation.\\n\\n2. Several built-in types have a new name:\\n\\n   - `int` \u2192 `int64`\\n   - `count` \u2192 `uint64`\\n   - `real` \u2192 `double`\\n   - `addr` \u2192 `ip`\\n\\n   The old names are still supported for the time being, but trigger a\\n   warning on startup. We will remove support for the old names in a future\\n   release.\\n\\n3. The match operator `~` and its negated form `!~` no longer exist. Use `==`\\n   and `!=` instead to perform searches with regular expressions, e.g., `url ==\\n   /^https?.*/`. Such queries now work for all string fields in addition to the\\n   previously supported `#type` meta extractor.\\n\\n4. We removed the `#field` meta extractor. That is, queries of the form `#field\\n   == \\"some.field.name\\"` no longer work. Use `some.field.name != null` or the\\n   new short form `some.field.name` to check for field existence moving forward.\\n\\n5. We renamed the boolean literal values `T` and `F` to `true` and `false`,\\n   respectively. For example the query `suricata.alert.alerted == T` is no\\n   longer valid; use `suricata.alert.alerted == true` instead.\\n\\n6. We renamed the non-value literal value `nil` to `null`. For example the\\n   query `x != nil` is no longer valid; use `x != null` instead.\\n\\n7. The `map` type no longer exists: Instead of `map<T, U>`, use the equivalent\\n   `list<record{ key: T, value: U }>`.\\n\\nOur goal is for these changes to make the query language feel more natural to\\nour users. We\'ve got [big plans][rfc-001] on how to extend it\u2014and this felt very\\nmuch necessary as a preparatory step to making the language more useful.\\n\\n[rfc-001]: https://github.com/tenzir/vast/blob/main/rfc/001-composable-pipelines/README.md\\n\\n## Regular Expression Evaluation\\n\\nVAST now supports searching with regular expressions. For example, let\'s say you\\nare looking for all events that contain a GUID surrounded by braces whose third\\nand fourth section are `5ec2-7015`:\\n\\n```json {0} title=\\"vast export -n 1 json \'/\\\\{[0-9a-f]{8}-[0-9a-f]\\\\{4}-5ec2-7015-[0-9a-f]\\\\{12}\\\\}/\'\\"\\n{\\n  \\"RuleName\\": \\"-\\",\\n  \\"UtcTime\\": \\"2020-05-18T09:42:40.443000\\",\\n  \\"ProcessGuid\\": \\"{8bcf3217-5890-5ec2-7015-00000000b000}\\",\\n  \\"ProcessId\\": 172,\\n  \\"Image\\": \\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\conhost.exe\\",\\n  \\"FileVersion\\": \\"10.0.17763.1075 (WinBuild.160101.0800)\\",\\n  \\"Description\\": \\"Console Window Host\\",\\n  \\"Product\\": \\"Microsoft\xae Windows\xae Operating System\\",\\n  \\"Company\\": \\"Microsoft Corporation\\",\\n  \\"OriginalFileName\\": \\"CONHOST.EXE\\",\\n  \\"CommandLine\\": \\"\\\\\\\\??\\\\\\\\C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\conhost.exe 0xffffffff -ForceV1\\",\\n  \\"CurrentDirectory\\": \\"C:\\\\\\\\Windows\\",\\n  \\"User\\": \\"NT AUTHORITY\\\\\\\\SYSTEM\\",\\n  \\"LogonGuid\\": \\"{8bcf3217-54f5-5ebe-e703-000000000000}\\",\\n  \\"LogonId\\": 999,\\n  \\"TerminalSessionId\\": 0,\\n  \\"IntegrityLevel\\": \\"System\\",\\n  \\"Hashes\\": \\"SHA1=74F28DD9B0DA310D85F1931DB2749A26A9A8AB02\\",\\n  \\"ParentProcessGuid\\": \\"{8bcf3217-5890-5ec2-6f15-00000000b000}\\",\\n  \\"ParentProcessId\\": 3440,\\n  \\"ParentImage\\": \\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\OpenSSH\\\\\\\\sshd.exe\\",\\n  \\"ParentCommandLine\\": \\"\\\\\\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\OpenSSH\\\\\\\\sshd.exe\\\\\\" \\\\\\"-R\\\\\\"\\"\\n}\\n```\\n\\n:::tip Case-Insensitive Patterns\\nIn addition to writing `/pattern/`, you can specify a regular expression that\\nignores the casing of characters via `/pattern/i`. The `/i` flag is currently\\nthe only support pattern modifier.\\n:::\\n\\n## Revamped Status for Event Distribution\\n\\nThe event distribution statistics moved within the output of `vast status`.\\n\\nThey were previously available under the `index.statistics` section when using\\nthe `--detailed` option:\\n\\n```json {0} title=\\"VAST v2.4.1 \u276f vast status --detailed | jq .index.statistics\\"\\n{\\n  \\"events\\": {\\n    \\"total\\": 42\\n  },\\n  \\"layouts\\": {\\n    \\"suricata.alert\\": {\\n      \\"count\\": 1,\\n      \\"percentage\\": 2.4\\n    },\\n    \\"suricata.flow\\": {\\n      \\"count\\": 41,\\n      \\"percentage\\": 97.6\\n    }\\n  }\\n}\\n```\\n\\nIt is now under the `catalog` section and shows some additional information:\\n\\n```json {0} title=\\"VAST v3.0 \u276f vast status | jq .catalog\\"\\n{\\n  \\"num-events\\": 42,\\n  \\"num-partitions\\": 3,\\n  \\"schemas\\": {\\n    \\"suricata.alert\\": {\\n      \\"import-time\\": {\\n        \\"max\\": \\"2023-01-13T22:51:23.730183\\",\\n        \\"min\\": \\"2023-01-13T22:51:23.730183\\"\\n      },\\n      \\"num-events\\": 1,\\n      \\"num-partitions\\": 1\\n    },\\n    \\"suricata.flow\\": {\\n      \\"import-time\\": {\\n        \\"max\\": \\"2023-01-13T22:51:24.127312\\",\\n        \\"min\\": \\"2023-01-13T23:13:01.991323\\"\\n      },\\n      \\"num-events\\": 41,\\n      \\"num-partitions\\": 2\\n    }\\n  }\\n}\\n```\\n\\n## Display Schema of Stored Events\\n\\nThe `vast show schemas` command makes it easy to see the structure of events in\\nthe database at a glance.\\n\\n```yaml {0} title=\\"vast show schemas --yaml suricata.flow\\"\\n- suricata.flow:\\n    record:\\n      - timestamp:\\n          timestamp: time\\n      - flow_id:\\n          type: uint64\\n          attributes:\\n            index: hash\\n      - pcap_cnt: uint64\\n      - vlan:\\n          list: uint64\\n      - in_iface: string\\n      - src_ip: ip\\n      - src_port:\\n          port: uint64\\n      - dest_ip: ip\\n      - dest_port:\\n          port: uint64\\n      - proto: string\\n      - event_type: string\\n      - community_id:\\n          type: string\\n          attributes:\\n            index: hash\\n      - flow:\\n          suricata.component.flow:\\n            record:\\n              - pkts_toserver: uint64\\n              - pkts_toclient: uint64\\n              - bytes_toserver: uint64\\n              - bytes_toclient: uint64\\n              - start: time\\n              - end: time\\n              - age: uint64\\n              - state: string\\n              - reason: string\\n              - alerted: bool\\n      - app_proto: string\\n```\\n\\n:::tip Filter Schemas\\nThe `vast show schemas` command supports filtering not just by the exact name of\\na schema, but also by the module name. E.g., `vast show schemas zeek` will print\\na list of all schemas in the Zeek module that the VAST server holds data for.\\n:::\\n\\n## Common Event Format (CEF) Parser\\n\\nThis release includes a new reader plugin for the [Common Event Format\\n(CEF)][cef], a text-based event format that originally stems from ArcSight. This\\nline-based format consists of up to 8 pipe-separated fields, with the last field\\nbeing an optional list of key-value pairs:\\n\\n[cef]: https://www.microfocus.com/documentation/arcsight/arcsight-smartconnectors/pdfdoc/common-event-format-v25/common-event-format-v25.pdf\\n\\n```\\nCEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension]\\n```\\n\\nHere\'s a real-world instance.\\n\\n```\\nCEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8| externalId=6 clientId=2251997 scanGroupId=3 scanGroupName=Manually Installed Agents sev=High duser=tikasrv01\\\\\\\\administrator cat=END-POINT Alert dhost=TikaSrv01 src=172.31.5.93 filePath=c:\\\\\\\\windows\\\\\\\\temp\\\\\\\\javac.exe fname=javac.exe rt=3/30/2022 10:55:34 AM fileHash=2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD rtUtc=Mar 30 2022 10:55:34.688 dtUtc=Mar 30 2022 10:55:32.458 hostLS=2022-03-30 10:55:34 GMT+00:00 osVer=Windows Server 2016 Datacenter x64 1607 epsVer=4.5.5.6845 confVer=637842168250000000 prUser=tikasrv01\\\\\\\\administrator pParams=\\"C:\\\\\\\\Windows\\\\\\\\Temp\\\\\\\\javac.exe\\" sign=Not signed pct=2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609 pFileHash=1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7 pprUser=tikasrv01\\\\\\\\administrator ppParams=C:\\\\\\\\Windows\\\\\\\\Explorer.EXE pssdeep=49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3 pSign=Signed and has certificate info gpFileHash=CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F gpprUser=tikasrv01\\\\\\\\administrator gpParams=C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\userinit.exe gpssdeep=384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu gpSign=Signed actRem=Kill, Rename\\n```\\n\\nVAST\'s CEF plugin supports parsing such lines using the `cef` format:\\n\\n```\\nvast import cef < cef.log\\n```\\n\\nVAST translates the `extension` field to a nested record, where the key-value\\npairs of the extensions map to record fields. Here is an example of the above\\nevent:\\n\\n```json {0} title=\\"vast export json \'172.31.5.93\' | jq\\"\\n{\\n  \\"cef_version\\": 0,\\n  \\"device_vendor\\": \\"Cynet\\",\\n  \\"device_product\\": \\"Cynet 360\\",\\n  \\"device_version\\": \\"4.5.4.22139\\",\\n  \\"signature_id\\": \\"0\\",\\n  \\"name\\": \\"Memory Pattern - Cobalt Strike Beacon ReflectiveLoader\\",\\n  \\"severity\\": \\"8\\",\\n  \\"extension\\": {\\n    \\"externalId\\": 6,\\n    \\"clientId\\": 2251997,\\n    \\"scanGroupId\\": 3,\\n    \\"scanGroupName\\": \\"Manually Installed Agents\\",\\n    \\"sev\\": \\"High\\",\\n    \\"duser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"cat\\": \\"END-POINT Alert\\",\\n    \\"dhost\\": \\"TikaSrv01\\",\\n    \\"src\\": \\"172.31.5.93\\",\\n    \\"filePath\\": \\"c:\\\\\\\\windows\\\\\\\\temp\\\\\\\\javac.exe\\",\\n    \\"fname\\": \\"javac.exe\\",\\n    \\"rt\\": \\"3/30/2022 10:55:34 AM\\",\\n    \\"fileHash\\": \\"2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD\\",\\n    \\"rtUtc\\": \\"Mar 30 2022 10:55:34.688\\",\\n    \\"dtUtc\\": \\"Mar 30 2022 10:55:32.458\\",\\n    \\"hostLS\\": \\"2022-03-30 10:55:34 GMT+00:00\\",\\n    \\"osVer\\": \\"Windows Server 2016 Datacenter x64 1607\\",\\n    \\"epsVer\\": \\"4.5.5.6845\\",\\n    \\"confVer\\": 637842168250000000,\\n    \\"prUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"pParams\\": \\"C:\\\\\\\\Windows\\\\\\\\Temp\\\\\\\\javac.exe\\",\\n    \\"sign\\": \\"Not signed\\",\\n    \\"pct\\": \\"2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609\\",\\n    \\"pFileHash\\": \\"1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7\\",\\n    \\"pprUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"ppParams\\": \\"C:\\\\\\\\Windows\\\\\\\\Explorer.EXE\\",\\n    \\"pssdeep\\": \\"49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3\\",\\n    \\"pSign\\": \\"Signed and has certificate info\\",\\n    \\"gpFileHash\\": \\"CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F\\",\\n    \\"gpprUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"gpParams\\": \\"C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\userinit.exe\\",\\n    \\"gpssdeep\\": \\"384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu\\",\\n    \\"gpSign\\": \\"Signed\\",\\n    \\"actRem\\": \\"Kill, Rename\\"\\n  }\\n}\\n```\\n\\n:::note Syslog Header\\nSometimes CEF is prefixed with a syslog header. VAST currently only supports the\\n\\"raw\\" form without the syslog header. We are working on support for composable\\n*generic* formats, e.g., syslog, where the message can basically be any other\\nexisting format.\\n:::\\n\\n## Tidbits\\n\\nThis VAST release contains a fair amount of other changes and interesting\\nimprovements. As always, the [changelog][changelog] contains a complete list of\\nuser-facing changes since the last release.\\n\\nHere are some entries that we want to highlight:\\n\\n[changelog]: https://vast.io/changelog#v303\\n\\n### Removing Empty Fields from JSON Output\\n\\nThe `vast export json` command gained new options in addition to the already\\nexisting `--omit-nulls`: Pass `--omit-empty-records`, `--omit-empty-lists`,\\nor `--omit-empty-maps` to cause VAST not to display empty records, lists, or\\nmaps respectively.\\n\\nThe flag `--omit-empty` empty combines the three new options and `--omit-nulls`,\\nessentially causing VAST not to render empty values at all. To set these options\\nglobally, add the following to your vast.yaml configuration file:\\n\\n```yaml\\nvast:\\n  export:\\n    json:\\n      # Always omit empty records and lists when using the JSON export format,\\n      # but keep empty lists and maps.\\n      omit-nulls: true\\n      omit-empty-records: true\\n      omit-empty-maps: false\\n      omit-empty-lists: false\\n```\\n\\n### Faster Shutdown\\n\\nVAST processes now shut down faster, which especially improves the performance\\nof the `vast import` and `vast export` commands for small amounts of data\\ningested or quickly finishing queries.\\n\\nTo quantify this, we\'ve created a database with nearly 300M Zeek events, and ran\\nan export of a single event with both VAST v2.4.1 and VAST v3.0 repeatedly.\\n\\n```text {0} title=\\"\u276f vast -qq count --estimate | numfmt --grouping\\"\\n299,759,532\\n```\\n\\n```text {0} title=\\"VAST v2.4.1 \u276f hyperfine --warmup=5 --min-runs=20 \'vast -qq --bare-mode export -n1 null\'\\"\\nBenchmark 1: vast -qq --bare-mode export -n1 null\\n  Time (mean \xb1 \u03c3):     975.5 ms \xb1   4.8 ms    [User: 111.2 ms, System: 51.9 ms]\\n  Range (min \u2026 max):   966.3 ms \u2026 985.3 ms    20 runs\\n```\\n\\n```text {0} title=\\"VAST v3.0 \u276f hyperfine --warmup=5 --min-runs=20 \'vast -qq export -n1 null\'\\"\\nBenchmark 1: vast -qq --bare-mode export -n1 null\\n  Time (mean \xb1 \u03c3):     210.8 ms \xb1   3.5 ms    [User: 99.8 ms, System: 42.5 ms]\\n  Range (min \u2026 max):   204.1 ms \u2026 217.1 ms    20 runs\\n```\\n\\n### Connection Stability\\n\\nVAST clients may now be started before the VAST server: Client processes now\\nattempt to connect to server processes repeatedly until the configured\\nconnection timeout expires.\\n\\nWe found this to generally improve reliability of services with multiple VAST\\nclients, for which we often encountered problems with VAST clients being unable\\nto connect to a VAST server when started before or immediately after the VAST\\nserver.\\n\\nAdditionally, we\'ve fixed a bug that caused VAST to crash when thousands of\\nclients attempted to connect at around the same time.\\n\\n### Slim Docker Image\\n\\nThe new `tenzir/vast-slim` Docker image is an alternative to the existing\\n`tenzir/vast` Docker image that comes in at just under 40 MB in size\u2014less than a\\nthird than the regular image, making it even quicker to get started with VAST.\\n\\n### Bundled Python Bindings\\n\\nVAST installations now include Python bindings to VAST as a site package. The\\npackage is called `vast` and also available [separately on PyPI][vast-pypi].\\n\\n[vast-pypi]: https://pypi.org/project/pyvast\\n\\n### Expression Short Forms\\n\\nExtractors can now be used where predicates are expected to test for the\\nexistance of a field or type. For example, `x` and `:T` expand to `x != null`\\nand `:T != null`, respectively. This pairs nicely with the already existing\\nshort forms for values, e.g., `\\"foo\\"` expands to `:string == \\"foo`."},{"id":"/vast-v2.4.1","metadata":{"permalink":"/releases/vast-v2.4.1","source":"@site/releases/vast-v2.4.1/index.md","title":"VAST v2.4.1","description":"Faster Query Taste","date":"2022-12-19T00:00:00.000Z","formattedDate":"December 19, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"feather","permalink":"/releases/tags/feather"},{"label":"performance","permalink":"/releases/tags/performance"}],"readingTime":1.685,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4.1","description":"Faster Query Taste","authors":"dominiklohmann","date":"2022-12-19T00:00:00.000Z","tags":["release","feather","performance"]},"prevItem":{"title":"VAST v3.0","permalink":"/releases/vast-v3.0"},"nextItem":{"title":"VAST v2.4","permalink":"/releases/vast-v2.4"}},"content":"[VAST v2.4.1][github-vast-release] improves the performance of queries when VAST\\nis under high load, and significantly reduces the time to first result for\\nqueries with a low selectivity.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.1\\n\\n\x3c!--truncate--\x3e\\n\\n## Reading Feather Files Incrementally\\n\\nVAST\'s Feather store na\xefvely used the [Feather reader][feather-reader] from the\\nApache Arrow C++ library in its initial implementation. However, its API is\\nrather limited: It does not support reading record batches incrementally. We\'ve\\nswapped this out with a more efficient implementation that does.\\n\\n[feather-reader]: https://github.com/apache/arrow/blob/apache-arrow-10.0.1/cpp/src/arrow/ipc/feather.h#L57-L108\\n\\nThis is best explained visually:\\n\\n![Incremental Reads](incremental-reads.excalidraw.svg)\\n\\nWithin the scope of a single Feather store file, a single query takes the same\\namount of time overall, but there exist two distinct advantages of this\\napproach:\\n\\n1. The first result arrives much faster at the client.\\n2. Stores do less work for cancelled queries.\\n\\nOne additional benefit that is not immediately obvious comes into play when\\nqueries arrives at multiple stores in parallel: disk reads are more evenly\\nspread out now, making them less likely to overlap between stores. For\\ndeployments with slower I/O paths this can lead to a significant query\\nperformance improvement.\\n\\nTo verify and test this, we\'ve created a VAST database with 300M Zeek events\\n(33GB on disk) from a Corelight sensor. All tests were performed on a cold start\\nof VAST, i.e., we stopped and started VAST after every repetition of each test.\\n\\nWe performed three tests:\\n\\n1. Export a single event (20 times)\\n2. Export all events (20 times)\\n3. Rebuild the entire database (3 times)\\n\\nThe results are astonishingly good:\\n\\n|Test|Benchmark|v2.4.0|v2.4.1|Improvement|\\n|:-:|:-:|:-:|:-:|:-:|\\n|**(1)**|Avg. store load time|55.1ms|4.2ms|13.1x|\\n||Time to first result/Total time|19.8ms|14.5ms|1.4x|\\n|**(2)**|Avg. store load time|386.5ms|7.3ms|52.9x|\\n||Time to first result|69.2ms|25.4ms|2.7x|\\n||Total time|39.38s|33.30s|1.2x|\\n|**(3)**|Avg. store load time|480.3ms|9.1ms|52.7x|\\n||Total time|210.5s|198.0s|1.1x|\\n\\nIf you\'re using the Feather store backend (the default as of v2.4.0), you will\\nsee an immediate improvement with VAST v2.4.1. There are no other changes\\nbetween the two releases.\\n\\n:::info Parquet Stores\\nVAST also offers an experimental Parquet store backend, for which we plan to\\nmake a similar improvement in a coming release.\\n:::"},{"id":"/vast-v2.4","metadata":{"permalink":"/releases/vast-v2.4","source":"@site/releases/vast-v2.4/index.md","title":"VAST v2.4","description":"Open Storage","date":"2022-12-09T00:00:00.000Z","formattedDate":"December 9, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"frontend","permalink":"/releases/tags/frontend"},{"label":"feather","permalink":"/releases/tags/feather"},{"label":"parquet","permalink":"/releases/tags/parquet"},{"label":"docker","permalink":"/releases/tags/docker"},{"label":"python","permalink":"/releases/tags/python"},{"label":"arrow","permalink":"/releases/tags/arrow"}],"readingTime":4.195,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4","description":"Open Storage","authors":"dominiklohmann","date":"2022-12-09T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["release","frontend","feather","parquet","docker","python","arrow"]},"prevItem":{"title":"VAST v2.4.1","permalink":"/releases/vast-v2.4.1"},"nextItem":{"title":"VAST v2.3.1","permalink":"/releases/vast-v2.3.1"}},"content":"[VAST v2.4][github-vast-release] completes the switch to open storage formats,\\nand includes an early peek at three upcoming features for VAST: A web plugin\\nwith a REST API and an integrated frontend user interface, Docker Compose\\nconfiguration files for getting started with VAST faster and showing how to\\nintegrate VAST into your SOC, and new Python bindings that will make writing\\nintegrations easier and allow for using VAST with your data science libraries,\\nlike Pandas.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Preventing Vendor Lock-in with Open Storage\\n\\nVAST\'s Apache Feather (V2) and Apache Parquet storage backends are now\\nconsidered stable, and the default storage format is now Feather. This marks the\\nbeginning of a new era for VAST for all users: There is no more vendor lock-in\\nof your data!\\n\\nBoth as engineers and users of software we disdain vendor lock-in. Your data is\\nyours and no tool should hold it hostage. We want you to choose VAST because\\nit\'s the best engine when building a sustainable security data architecture. In\\nother words, *VAST decouples data acquisition from downstream security\\nanalytics*. To this end, we are not only committed to open source, but also to\\nopen standards\u2014for storage and processing.\\n\\nAs of this release, VAST no longer supports *writing* to its old proprietary\\nstorage format, but will still support *reading* from it until the next major\\nrelease. In the background, VAST transparently rebuilds old partitions to take\\nadvantage of the new format without any downtime. This may cause some additional\\nload when starting VAST first up after the update, but ensures that queries run\\nas fast as possible once all old partitions have been converted.\\n\\nIf you want to know more about Feather and Parquet, check out our in-depth blog\\npost series on them:\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-1]: /archive/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /archive/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /archive/parquet-and-feather-data-engineering-woes/\\n\\n## What\'s Next?\\n\\nVAST v2.4 contains a few new and experimental toys to play with. Here\'s an\\noverview of what they are, and how they all make it easier to integrate VAST\\nwith other security tools.\\n\\n### Docker Compose\\n\\nA new set of Docker Compose files makes it easier than ever to\\nget started with VAST. This is not designed for high-performance deployments of\\nVAST, but rather to make it easier to try VAST out\u2014all-batteries included,\\nbecause we want to use this to showcase and test the myriad of integrations\\nin a modern SOC.\\n\\nOur vision for this is to show how VAST as a modular platform can power modern\\nand sustainable approaches to composable security.\\n\\n### REST API and Frontend User Interface\\n\\nThe experimental `web` plugin adds a REST API to VAST, and also a\\nfrontend user interface we [built in Svelte][frontend-code].\\n\\nBoth the API and the frontend are still considered unstable and subject to\\nchange without notice. We plan to stabilize and version the API in the future.\\nFundamentally, the API serves two purposes:\\n\\n1. Make it easier to write integrations with VAST\\n2. Serve as a backend for VAST\'s bundled frontend\\n\\nThe frontend UI currently displays a status page for the installed VAST node.\\n\\n\x3c!--- this weird markup is to render a border around the image ---\x3e\\n![UI showing a status page](vast-ui-experimental.jpg)\\n\\nWe have some exciting features planned for both of these. Stay tuned!\\n\\n[frontend-code]: https://github.com/tenzir/vast/tree/v2.4.0/plugins/web/ui\\n\\n### Python Bindings\\n\\nWe want to make it as easy as possible to integrate VAST with other tools, so\\nwe\'re working on making that as easy as possible using VAST\'s Python bindings.\\nThe new bindings support analyzing data from VAST using industry-standard Python\\nlibraries, like Pandas.\\n\\nThis is all enabled by our commitment to open standards: VAST leverages Apache\\nArrow as its in-memory data representation. The Python bindings make it easy to\\nuse VAST\'s security-specific data types. For example, when running a query, IP\\naddresses, subnets, and patterns automatically convert to the Python-native\\ntypes, as opposed to remaining binary blobs or sheer strings.\\n\\n:::note Not yet on PyPI\\nVAST\'s new Python bindings are not yet on PyPI, as they are still heavily under\\ndevelopment. If you\'re too eager and cannot wait, go [check out the source\\ncode][python-code].\\n:::\\n\\n[python-code]: https://github.com/tenzir/vast/tree/v2.4.0/python\\n\\n## Other Noteworthy Changes\\n\\nA full list of changes to VAST since the last release is available in the\\n[changelog][changelog-2.4]. Here\'s a selection of changes that are particularly\\nnoteworthy:\\n\\n- VAST now loads all plugins by default. When asking new users for pitfalls they\\n  encountered, this ranked pretty high on the list of things we needed to\\n  change. To revert to the old behavior, set `vast.plugins: []` in your\\n  configuration file, or set `VAST_PLUGINS=` in your environment.\\n- The default endpoint changed from `localhost` to `127.0.0.1` to ensure a\\n  deterministic listening address.\\n- Exporting VAST\'s performance metrics via UDS no longer deadlocks VAST\'s\\n  metrics exporter when a listener is suspended.\\n- VAST\'s build process now natively supports building Debian packages. This\\n  makes upgrades for bare-metal deployments a breeze. As of this release, our\\n  CI/CD pipeline automatically attaches a Debian package in addition to the\\n  build archive to our releases.\\n\\n[changelog-2.4]: /changelog#v240"},{"id":"/vast-v2.3.1","metadata":{"permalink":"/releases/vast-v2.3.1","source":"@site/releases/vast-v2.3.1/index.md","title":"VAST v2.3.1","description":"VAST v2.3.1 is now available. This small bugfix release","date":"2022-10-17T00:00:00.000Z","formattedDate":"October 17, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"rebuild","permalink":"/releases/tags/rebuild"},{"label":"performance","permalink":"/releases/tags/performance"}],"readingTime":0.215,"hasTruncateMarker":false,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.3.1","authors":"lava","date":"2022-10-17T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"VAST v2.4","permalink":"/releases/vast-v2.4"},"nextItem":{"title":"VAST v2.3","permalink":"/releases/vast-v2.3"}},"content":"[VAST v2.3.1][github-vast-release] is now available. This small bugfix release\\naddresses an issue where compaction would hang if encountering\\ninvalid partitions that were produced by older versions of VAST when a large\\n`max-partition-size` was set in combination with badly compressible input data.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.1"},{"id":"/vast-v2.3","metadata":{"permalink":"/releases/vast-v2.3","source":"@site/releases/vast-v2.3/index.md","title":"VAST v2.3","description":"Automatic Rebuilds","date":"2022-09-01T00:00:00.000Z","formattedDate":"September 1, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"rebuild","permalink":"/releases/tags/rebuild"},{"label":"performance","permalink":"/releases/tags/performance"}],"readingTime":3.9,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.3","description":"Automatic Rebuilds","authors":"dominiklohmann","date":"2022-09-01T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"VAST v2.3.1","permalink":"/releases/vast-v2.3.1"},"nextItem":{"title":"VAST v2.2","permalink":"/releases/vast-v2.2"}},"content":"[VAST v2.3][github-vast-release] is now available, which introduces an automatic\\ndata defragmentation capability.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Automatic Rebuilds\\n\\nVAST server processes now continuously rebuild partitions in the background. The\\nfollowing diagram visualizes what happens under the hood:\\n\\n![Rebuild](rebuild.excalidraw.svg)\\n\\nRebuilding kicks in when a partition has the following properties:\\n\\n1. **Outdated**: if a partitions does not have the latest partition version, it\\n   may not enjoy the latest features and optimizations. It makes it also faster\\n   to adopt VAST versions that include breaking changes in the storage layout.\\n   Therefore, VAST rebuilds outdated partitions to bring them into the most\\n   recent state.\\n\\n2. **Undersized**: numerous small partitions can cause fragmentation in the\\n   catalog, causing higher memory consumption, larger database footprint, and\\n   slower queries. Rebuilding merges undersized partitions, thereby\\n   defragmenting the system. This reduces the resource footprint and makes\\n   queries faster.\\n\\nTo enable automatic rebuilding, set the new `vast.automatic-rebuild` option.\\n\\n```yaml\\nvast:\\n  # Control automatic rebuilding of partitions in the background for\\n  # optimization purposes. The given number controls how many rebuilds to run\\n  # concurrently, and thus directly controls the performance vs. memory and CPU\\n  # usage trade-off. Set to 0 to disable. Defaults to 1.\\n  automatic-rebuild: 1\\n```\\n\\nNow that we have an LSM-style merge operation of partitions, we reduced\\nthe partition cutoff timeout to 5 minutes from 1 hour by default (controlled\\nthrough the option `vast.active-partition-timeout`). This reduces the risk of\\ndata loss in case of a crash. This comes in handy in particular for low-volume\\ndata sources that never exhaust their capacity.\\n\\n## Optional Partition Indexes\\n\\nHistorically, VAST evolved from a special-purpose bitmap indexing system into a\\ngeneral-purpose telemetry engine for security data. Today, VAST has a two-tiered\\nindexing architecture with sparse sketch structures at the top, followed by a\\nsecond layer of dense indexes. As of this release, it is possible to disable\\nthis second layer.\\n\\nThe space savings can be substantial based on the size of your index. For\\nexample, if the first layer of indexing always yields highly selective results,\\nthen it the dense indexes do not provide a lot of value. One scenario would be\\nretro-matching: if you only do IoC-style point queries, they will be most likely\\ncovered well by the sketches. If you do not have selective queries, the dense\\nindex is not helping much anyway, since you need access the base data anyway. A\\nreally good use case for the indexes when your have a scatterd data access\\npatterns, i.e., highly selective results *within* a partition, but a result that\\nspans many disparate partitions.\\n\\nIn a simplified model, VAST performs three steps when executing a query:\\n\\n1. Send the query to the catalog, which maintains VAST\'s partitions, and ask it\\n   for a list of candidate partitions. The catalog maintains the first tier of\\n   sparse indexes, currently one per partition.\\n\\n2. Send the query to all candidate partitions in parallel, each of which\\n   contains dense indexes for all fields in the partition\'s schema. The index\\n   lookup yields a set of candidate records IDs within the partition.\\n\\n3. Send the query to all candidate partition\'s stores, provided the index lookup\\n   yielded record IDs. Then evaluating the query against the candidate events\\n   and return the result.\\n\\nHere\'s how you can configure a partition index to be disabled:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n        # Don\'t create partition indexes the suricata.http.http.url field.\\n      - targets:\\n          - suricata.http.http.url\\n        partition-index: false\\n        # Don\'t create partition indexes for fields of type addr.\\n      - targets:\\n          - :ip\\n        partition-index: false\\n```\\n\\n## Improved Responsiveness Under High Load\\n\\nTwo small changes improve VAST\'s behavior under exceptionally high load.\\n\\nFirst, the new `vast.connection-timeout` option allows for modifying the default\\nclient-to-server connection timeout of 10 seconds. Previously, if a VAST server\\nwas too busy to respond to a new client within 10 seconds, the client simply\\nexited with an unintelligable `request_timeout` error message. Here\'s how you\\ncan set a custom timeout:\\n\\n```yaml\\nvast:\\n  # The timeout for connecting to a VAST server. Set to 0 seconds to wait\\n  # indefinitely.\\n  connection-timeout: 10s\\n```\\n\\nThe option is additionally available under the environment variable\\n`VAST_CONNECTION_TIMEOUT` and the `--connection-timeout` command-line option.\\n\\nSecond, we improved the operability of VAST servers under high load from\\nautomated low-priority queries. We noticed that when spawning thousands of\\nautomated retro-match queries that compaction would stall and make little\\nvisible progress, risking the disk running full or no longer being compliant\\nwith GDPR-related policies enforced by compaction.\\n\\nTo ensure that compaction\'s internal and regular user-issued queries work as\\nexpected even in this scenario, VAST now considers queries issued with\\n`--low-priority`, with even less priority compared to regular queries (down from\\n33.3% to 4%) and internal high-priority queries used for rebuilding and\\ncompaction (down from 12.5% to 1%)."},{"id":"/vast-v2.2","metadata":{"permalink":"/releases/vast-v2.2","source":"@site/releases/vast-v2.2/index.md","title":"VAST v2.2","description":"Pipelines","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"summarize","permalink":"/releases/tags/summarize"},{"label":"pipelines","permalink":"/releases/tags/pipelines"}],"readingTime":2.135,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.2","description":"Pipelines","authors":"lava","date":"2022-08-05T00:00:00.000Z","tags":["release","summarize","pipelines"]},"prevItem":{"title":"VAST v2.3","permalink":"/releases/vast-v2.3"},"nextItem":{"title":"VAST v2.1","permalink":"/releases/vast-v2.1"}},"content":"We released [VAST v2.2][github-vast-release] \ud83d\ude4c! Transforms now have a new name:\\n[pipelines](/releases/vast-v2.2#transforms-are-now-pipelines). The [summarize\\noperator](/releases/vast-v2.2#summarization-improvements) also underwent a facelift,\\nmaking aggregation functions pluggable and allowing for assigning names to\\noutput fields.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.2.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Transforms are now Pipelines\\n\\nAfter carefully reconsidering our naming decisions related to query execution\\nand data transformation, we came up with a naming convention that does a better\\njob in capturing the underlying concepts.\\n\\nMost notably, we renamed *transforms* to *pipelines*. A transform *step* is now a\\npipeline *operator*. This nomenclature is much more familiar to users coming\\nfrom dataflow and collection-based query engines. The implementation underneath\\nhasn\'t changed. As in the [Volcano model][volcano], data still flows through\\noperators, each of which consumes input from upstream operators and produces\\noutput for downstream operators. What we term a pipeline is the sequence of such\\nchained operators.\\n\\n[volcano]: https://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf\\n\\nWhile pipelines are not yet available at the query layer, they soon will be.\\nUntil then, you can deploy pipelines at load-time to transform data in motion\\nor data at rest.\\n\\nFrom a user perspective, the configuration keys associated with transforms have\\nchanged. Here\'s the updated example from our previous [VAST v1.0 release\\nblog](/releases/vast-v1.0).\\n\\n```yaml\\nvast:\\n  # Specify and name our pipelines, each of which are a list of configured\\n  # pipeline operators. Pipeline operators are plugins, enabling users to \\n  # write complex transformations in native code using C++ and Apache Arrow.\\n  pipelines:\\n     # Prevent events with certain strings to be exported, e.g., \\n     # \\"tenzir\\" or \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each pipeline at server- or client-side, on\\n  # `import` or `export`, and restrict them to a list of event types.\\n  pipeline-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - pipeline: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Summarization Improvements\\n\\nIn line with the above nomenclature changes, we\'ve improved the behavior of the\\n`summarize` operator. It is now possible to specify an explicit\\nname for the output fields. This is helpful when the downstream processing needs\\na predictable schema. Previously, VAST took simply the name of the input field.\\nThe syntax was as follows:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    min:\\n      - ts # implied name for aggregate field\\n```\\n\\nWe now switched the syntax such that the new field name is at the beginning:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    ts_min: # explicit name for aggregate field\\n      min: ts\\n```\\n\\nIn SQL, this would be the `AS` token: `SELECT min(ts) AS min_ts`."},{"id":"/vast-v2.1","metadata":{"permalink":"/releases/vast-v2.1","source":"@site/releases/vast-v2.1/index.md","title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","date":"2022-07-07T00:00:00.000Z","formattedDate":"July 7, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"performance","permalink":"/releases/tags/performance"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","authors":"dominiklohmann","date":"2022-07-07T00:00:00.000Z","tags":["release","performance"]},"prevItem":{"title":"VAST v2.2","permalink":"/releases/vast-v2.2"},"nextItem":{"title":"VAST v2.0","permalink":"/releases/vast-v2.0"}},"content":"[VAST v2.1][github-vast-release] is out! This release comes with a particular\\nfocus on performance and reducing the size of VAST databases. It brings a new\\nutility for optimizing databases in production, allowing existing deployments to\\ntake full advantage of the improvements after upgrading.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## New Project Site\\n\\nVAST has new project site: [vast.io](https://vast.io). We ported all\\ndocumentation from `https://docs.tenzir.com`, added a lot of new content, and\\nrestructured the reading experience along the user journey.\\n\\nYou can find the Threat Bus documentation in Use VAST \u2192 Integrate \u2192 Threat\\nBus. Threat Bus is now officially in\\nmaintainance mode: we are only supporting existing features with bugfixes. That\\nsaid, Threat Bus will resurface in a new shape with its existing functionality\\nintegrated into VAST itself. Stay tuned.\\n\\n## Performance Improvements\\n\\nVAST now compresses data with [Zstd](http://www.zstd.net). The default\\nconfiguration achieves over 2x space savings. When transferring data between\\nclient and server processes, compression reduces the amount of transferred data\\nby up to 5x.\\n\\nAdditionally, VAST now compresses on-disk indexes with Zstd, resulting in a\\n50-80% size reduction depending on the type of indexes used.\\n\\nThis allowed us to increase the default partition size from 1,048,576 to\\n4,194,304 events[^1], and the default number of events in a single batch from 1,024\\nto 65,536, resulting in a massive performance increase at the cost of a ~20%\\nlarger memory footprint at peak loads. Use the option `vast.max-partition-size`\\nto tune this space-time tradeoff.\\n\\nTo benchmark this, we used [`speeve`][speeve] to generate 20 EVE JSON files\\ncontaining 8,388,608 events each[^2]. We spawned a VAST server process and ran\\n20 VAST client processes in parallel, with one process per file.\\n\\nWe observed a reduction of **up to 73%** of disk space utilization:\\n\\n![Database Size](storage-light.png#gh-light-mode-only)\\n![Database Size](storage-dark.png#gh-dark-mode-only)\\n\\nIn addition, we were able to scale the ingest rate by almost **6x** due to the\\nhigher batch size and the reduced memory usage per batch:\\n\\n![Ingest Rate](rate-light.png#gh-light-mode-only)\\n![Ingest Rate](rate-dark.png#gh-dark-mode-only)\\n\\nThe table below summaries the benchmarks:\\n\\n||VAST v2.0|VAST v2.1|Change|\\n|-:|:-|:-|:-|\\n|Ingest Duration|1,650 s|242 s|-85.3%|\\n|Ingest Rate|101,680 events/s|693,273 events/s|+581.8%|\\n|Index Size|14,791 MiB|5,721 MiB|-61.3%|\\n|Store Size|37,656 MiB|8,491 MiB|-77.5%|\\n|Database Size|52,446 MiB|14,212 MiB|-72.9%|\\n\\n:::note Compressed Filesystems\\nThe above benchmarks ran on filesystems without compression. We expect the gain\\nfrom compression to be smaller when using compressed filesystems like\\n[`btrfs`][btrfs].\\n:::\\n\\n[speeve]: https://github.com/satta/speeve\\n[btrfs]: https://btrfs.wiki.kernel.org/index.php/Main_Page\\n\\n[^1]: VAST v2.0 failed to write its partitions to disk with the defaults for\\n  v2.1 because the on-disk size exceeded the maximum possible size of a\\n  FlatBuffers table, which VAST internally uses to have an open standard for its\\n  persistent state.\\n[^2]: This resulted in 167,772,160 events, with a total of 200\'917\'930 unique\\n  values with a schema distribution of 80.74% `suricata.flow`, 7.85%\\n  `suricata.dns`, 5.35% `suricata.http`, 4.57% `suricata.fileinfo`, 1.04%\\n  `suricata.tls`, 0.41% `suricata.ftp`, and 0.04% `suricata.smtp`.\\n\\n## Rebuild VAST Databases\\n\\nThe new changes to VAST\'s internal data format only apply to newly ingested\\ndata. To retrofit changes, we introduce a new `rebuild` command with this\\nrelease. A rebuild effectively re-ingests events from existing partitions and\\natomically replaces them with partitions of the new format.\\n\\nThis makes it possible to upgrade persistent state to a newer version, or\\nrecreate persistent state after changing configuration parameters, e.g.,\\nswitching from the Feather to the Parquet store backend (that will land in\\nv2.2). Rebuilding partitions also recreates their sparse indexes that\\naccellerate query execution. The process takes place asynchronously in the\\nbackground.\\n\\nWe recommend running `vast rebuild` to upgrade your VAST v1.x partitions to VAST\\nv2.x partitions to take advantage of the new compression and an improved\\ninternal representation.\\n\\nThis is how you run it:\\n\\n```bash\\nvast rebuild [--all] [--undersized] [--parallel=<number>] [<expression>]\\n```\\n\\nA rebuild is not only useful when upgrading outdated partitions, but also when\\nchanging parameters of up-to-date partitions. Use the `--all` flag to extend a\\nrebuild operation to _all_ partitions. (Internally, VAST versions the partition\\nstate via FlatBuffers. An outdated partition is one whose version number is not\\nthe newest.)\\n\\nThe `--undersized` flag causes VAST to only rebuild partitions that are under\\nthe configured partition size limit `vast.max-partition-size`.\\n\\nThe `--parallel` options is a performance tuning knob. The parallelism level\\ncontrols how many sets of partitions to rebuild in parallel. This value defaults\\nto 1 to limit the CPU and memory requirements of the rebuilding process, which\\ngrow linearly with the selected parallelism level.\\n\\nAn optional expression allows for restricting the set of partitions to rebuild.\\nVAST performs a catalog lookup with the expression to identify the set of\\ncandidate partitions. This process may yield false positives, as with regular\\nqueries, which may cause unaffected partitions to undergo a rebuild. For\\nexample, to rebuild outdated partitions containing `suricata.flow` events\\nolder than 2 weeks, run the following command:\\n\\n```bash\\nvast rebuild \'#type == \\"suricata.flow\\" && #import_time < 2 weeks ago\'\\n```"},{"id":"/vast-v2.0","metadata":{"permalink":"/releases/vast-v2.0","source":"@site/releases/vast-v2.0/index.md","title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","date":"2022-05-16T00:00:00.000Z","formattedDate":"May 16, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"compaction","permalink":"/releases/tags/compaction"},{"label":"performance","permalink":"/releases/tags/performance"},{"label":"pcap","permalink":"/releases/tags/pcap"}],"readingTime":6.335,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","authors":"dominiklohmann","date":"2022-05-16T00:00:00.000Z","tags":["release","compaction","performance","pcap"]},"prevItem":{"title":"VAST v2.1","permalink":"/releases/vast-v2.1"},"nextItem":{"title":"VAST v1.1.2","permalink":"/releases/vast-v1.1.2"}},"content":"Dear community, we are excited to announce [VAST v2.0][github-vast-release],\\nbringing faster execution of bulk-submitted queries, improved tunability of\\nindex structures, and new configurability through environment variables.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.0.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Scheduling\\n\\nVAST is now more intelligent in how it schedules queries.\\n\\nWhen a query arrives at the VAST server, VAST first goes to the catalog which\\nreturns a set of on-disk candidate partitions that the query may be applicable\\nto. Previous versions of VAST simply iterated through the available queries as\\nthey came in, loading partition by partition to extract events. Due to memory\\nconstraints, VAST is only able to keep some partitions in memory, which causes\\nfrequent loading and unloading of the same partitions for queries that access\\nthe same data. Now, VAST loads partitions depending on how many queries they are\\nrelevant for and evaluates all ongoing queries for one partition at a time.\\n\\nAdditionally, VAST now partitions the data for each schema separately, moving\\naway from partitions that contain events of multiple schemas. This helps with\\ncommon access patterns and speeds up queries restricted to a single schema.\\n\\nThe numbers speak for themselves:\\n\\n![Benchmarks](scheduler-light.png#gh-light-mode-only)\\n![Benchmarks](scheduler-dark.png#gh-dark-mode-only)\\n\\n## Updates to Aging, Compaction, and the Disk Monitor\\n\\nVAST v1.0 deprecated the experimental aging feature. Given popular demand we\'ve\\ndecided to un-deprecate it and to actually implement it on top of the same\\nbuilding blocks the new compaction mechanism uses, which means that it is now\\nfully working and no longer considered experimental.\\n\\nThe compaction plugin is now able to apply general time-based compactions that\\nare not restricted to a specific set of types. This makes it possible for\\noperators to implement rules like \\"delete all data after 1 week\\", without having\\nto list all possible data types that may occur.\\n\\nSome smaller interface changes improve the observability of the compactor for\\noperators: The  `vast compaction status` command prints the current compaction\\nstatus, and the `vast compaction list` command now lists all configured\\ncompaction rules of the VAST node.\\n\\nAdditionally, we\'ve improved overall stability and fault tolerance improvements\\nsurrounding the disk monitor and compaction features.\\n\\n## Fine-tuned Catalog Configuration\\n\\n:::note Advanced Users\\nThis section is for advanced users only.\\n:::\\n\\nThe catalog manages partition metadata and is responsible for deciding whether a\\npartition qualifies for a certain query. It does so by maintaining sketch data\\nstructures (e.g., Bloom filters, summary statistics) for each partition.\\nSketches are highly space-efficient at the cost of being probabilistic and\\nyielding false positives.\\n\\nDue to this characteristic, sketches can grow sublinear: doubling the number of\\nevents in a sketch does not lead to a doubling of the memory requirement.\\nBecause the catalog must be traversed in full for a given query it needs to be\\nmaintained in active memory to provide high responsiveness.\\n\\nA false positive can have substantial impact on the query latency by\\nmaterializing irrelevant partitions, which involves unnecessary I/O. Based on\\nthe cost of I/O, this penalty may be substantial. Conversely, reducing the false\\npositive rate increases the memory consumption, leading to a higher resident set\\nsize and larger RAM requirements.\\n\\nYou can control this space-time trade-off in the configuration section\\n`vast.index` by specifying index rules. Each rule corresponds to one sketch and\\nconsists of the following components:\\n\\n`targets`: a list of extractors to describe the set of fields whose values to\\nadd to the sketch. `fp-rate`: an optional value to control the false-positive\\nrate of the sketch.\\n\\nVAST does not create field-level sketches unless a dedicated rule with a\\nmatching target configuration exists. Here\'s an example:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n      - targets:\\n          # field synopses: need to specify fully qualified field name\\n          - suricata.http.http.url\\n        fp-rate: 0.005\\n      - targets:\\n          - :ip\\n        fp-rate: 0.1\\n```\\n\\nThis configuration includes two rules (= two sketches), where the first rule\\nincludes a field extractor and the second a type extractor. The first rule\\napplies to a single field, `suricata.http.http.url`, and has a false-positive\\nrate of 0.5%. The second rule creates one sketch for all fields of type `addr`\\nthat has a false-positive rate of 10%.\\n\\n## Configuring VAST with Environment Variables\\n\\nVAST now offers an additional configuration path besides editing YAML\\nconfiguration files and providing command line arguments: *setting environment\\nvariables*. This enables a convenient configuration experience when using\\ncontainer runtimes, such as Docker, where the other two configuration paths have\\na mediocre UX at best:\\n\\nThe container entry point is limited to adding command line arguments, where not\\nall options may be set. For Docker Compose and Kubernetes, it is often not\\ntrivially possible to even add command line arguments.\\n\\nProviding a manual configuration file is a heavy-weight action, because it\\nrequires (1) generating a potentially templated configuration file, and (2)\\nmounting that file into a location where VAST would read it.\\n\\nAn environment variable has the form `KEY=VALUE`. VAST processes only\\nenvironment variables having the form `VAST_{KEY}=VALUE`. For example,\\n`VAST_ENDPOINT=1.2.3.4` translates to the command line option\\n`--endpoint=1.2.3.4` and YAML configuration `vast.endpoint: 1.2.3.4`.\\n\\nRegarding precedence, environment variables override configuration file\\nsettings, and command line arguments override environment variables. Please\\nconsult the documentation for a more detailed explanation of how to specify keys\\nand values.\\n\\n## VLAN Tag Extraction and Better Packet Decapsulation\\n\\nVAST now extracts [802.1Q VLAN tags](https://en.wikipedia.org/wiki/IEEE_802.1Q)\\nfrom packets, making it possible to filter packets based on VLAN ID. The packet\\nschema includes a new nested record `vlan` with two fields: `outer` and `inner`\\nto represent the respective VLAN ID. For example, you can generate PCAP traces\\nof packets based on VLAN IDs as follows:\\n\\n```bash\\nvast export pcap \'vlan.outer > 0 || vlan.inner in [1, 2, 3]\' | tcpdump -r - -nl\\n```\\n\\nVLAN tags occur in many variations, and VAST extracts them in case of\\nsingle-tagging and  [QinQ\\ndouble-tagging](https://en.wikipedia.org/wiki/IEEE_802.1ad). Consult the PCAP\\ndocumentation for details on this feature.\\n\\nInternally, the packet decapsulation logic has been rewritten to follow a\\nlayered approach: frames, packets, and segments are the building blocks. The\\nplan is to reuse this architecture when switching to kernel-bypass packet\\nacquisition using DPDK. If you would like to see more work on the front of\\nhigh-performance packet recording, please reach out.\\n\\n## Breaking Changes\\n\\nThe `--verbosity` command-line option is now called `--console-verbosity`. The\\nshorthand options `-v`, `-vv`, `-vvv`, `-q`, `-qq`, and  `-qqq`  are unchanged.\\nThis aligns the command-line option with the configuration option\\n`vast.console-verbosity`, and disambiguates from the `vast.file-verbosity`\\noption.\\n\\nThe _Meta Index_ is now called the _Catalog_. This affects multiple status and\\nmetrics keys. We plan to extend the functionality of the Catalog in a future\\nrelease, turning it into a more powerful first instance for lookups.\\n\\nTransform steps that add or modify columns now add or modify the columns\\nin-place rather than at the end, preserving the nesting structure of the\\noriginal data.\\n\\n## Changes for Developers\\n\\nThe `vast get` command no longer exists. The command allowed for retrieving\\nevents by their internal unique ID, which we are looking to remove entirely in\\nthe future.\\n\\nChanges to the internal data representation of VAST require all transform step\\nplugins to be updated. The output format of the vast export arrow command\\nchanged for the address, subnet, pattern, and enumeration types, which are now\\nmodeled as [Arrow Extension\\nTypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types). The\\nrecord type is no longer flattened. The mapping of VAST types to Apache Arrow\\ndata types  is now considered stable.\\n\\n## Smaller Things\\n\\n- VAST client commands now start much faster and use less memory.\\n- The `vast count --estimate \'<query>\'` feature no longer unnecessarily causes\\n  stores to load from disk, resulting in major speedups for larger databases and\\n  broad queries.\\n- The [tenzir/vast](https://github.com/tenzir/vast) repository now contains\\n  experimental Terraform scripts for deploying VAST to AWS Fargate and Lambda."},{"id":"/vast-v1.1.2","metadata":{"permalink":"/releases/vast-v1.1.2","source":"@site/releases/vast-v1.1.2/index.md","title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","date":"2022-03-29T00:00:00.000Z","formattedDate":"March 29, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"compaction","permalink":"/releases/tags/compaction"},{"label":"query","permalink":"/releases/tags/query"}],"readingTime":0.33,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","authors":"lava","date":"2022-03-29T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v2.0","permalink":"/releases/vast-v2.0"},"nextItem":{"title":"VAST v1.1.1","permalink":"/releases/vast-v1.1.1"}},"content":"Dear community, we are happy to announce the release of [VAST\\nv1.1.2](https://github.com/tenzir/vast/releases/tag/v1.1.2), the latest release\\non the VAST v1.1 series. This release contains a fix for a race condition that\\ncould lead to VAST eventually becoming unresponsive to queries in large\\ndeployments.\\n\\n\x3c!--truncate--\x3e\\n\\nFixed a race condition that would cause queries to become stuck when an exporter\\nwould time out during the meta index lookup.\\n[#2165](https://github.com/tenzir/vast/pull/2165)"},{"id":"/vast-v1.1.1","metadata":{"permalink":"/releases/vast-v1.1.1","source":"@site/releases/vast-v1.1.1/index.md","title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","date":"2022-03-25T00:00:00.000Z","formattedDate":"March 25, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"compaction","permalink":"/releases/tags/compaction"},{"label":"query","permalink":"/releases/tags/query"}],"readingTime":0.635,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-25T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.2","permalink":"/releases/vast-v1.1.2"},"nextItem":{"title":"VAST v1.1","permalink":"/releases/vast-v1.1"}},"content":"Dear community, we are excited to announce [VAST\\nv1.1.1][github-vast-release-new].\\n\\nThis release contains some important bug fixes on top of everything included in\\nthe [VAST v1.1][github-vast-release-old] release.\\n\\n[github-vast-release-new]: https://github.com/tenzir/vast/releases/tag/v1.1.1\\n[github-vast-release-old]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n- The disk monitor now correctly continues deleting until below the low water\\n  mark after a partition failed to delete.\\n- We fixed a rarely occurring race condition that caused query workers to become\\n  stuck after delivering all results until the corresponding client process\\n  terminated.\\n- Queries that timed out or were externally terminated while in the query\\n  backlog that had more unhandled candidate than taste partitions no longer\\n  permanently get stuck. This critical bug caused VAST to idle permanently on\\n  the export path once all workers were stuck.\\n\\nThanks to [@norg](https://github.com/norg) for reporting the issues."},{"id":"/vast-v1.1","metadata":{"permalink":"/releases/vast-v1.1","source":"@site/releases/vast-v1.1/index.md","title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"compaction","permalink":"/releases/tags/compaction"},{"label":"query","permalink":"/releases/tags/query"}],"readingTime":5.975,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-03T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.1","permalink":"/releases/vast-v1.1.1"},"nextItem":{"title":"VAST v1.0","permalink":"/releases/vast-v1.0"}},"content":"Dear community, we are excited to announce [VAST v1.1][github-vast-release],\\nwhich ships with exciting new features: *query language plugins* to exchange the\\nquery expression frontend, and *compaction* as a mechanism for expressing\\nfine-grained data retention policies and gradually aging out data instead of\\nsimply deleting it.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Language Plugins\\n\\nVAST features a new query language plugin type\\nthat makes it possible to exchange the querying frontend, that is, replace the\\nlanguage in which the user writes queries. This makes it easier to integrate\\nVAST into specific domains without compromising the policy-neutral system core.\\n\\nThe first instance of the query language plugin is the [`sigma`\\nplugin](https://github.com/tenzir/vast/tree/master/plugins/sigma), which make it\\npossible to pass Sigma rules as\\ninput instead of a standard VAST query expression. Prior to this plugin, VAST\\nattempted to parse a query as Sigma rule first, and if that failed, tried to\\nparse it as a VAST expression. The behavior changed in that VAST now always\\ntries to interpret user input as VAST expression, and if that fails, goes\\nthrough all other loaded query language plugins.\\n\\nMoving forward, we will make it easier for integrators to BYO query language and\\nleverage VAST as an execution engine. We have already\\n[experimented](https://github.com/tenzir/vast/pull/2075) with\\n[Substrait](https://substrait.io), a cross-language protobuf spec for query\\nplans. The vision is that users can easily connect *any* query language that\\ncompiles into Substrait, and VAST takes the query plan as binary substrait blob.\\nSubstrait is still a very young project, but if the Arrow integration starts to\\nmature, it has the potential to enable very powerful types of queries without\\nmuch heavy lifting on our end. We already use the Arrow Compute API to implement\\ngeneric grouping and aggregation during compaction, which allows us to avoid\\nhand-roll and optimize compute kernels for standard functions.\\n\\n## Compaction Plugin\\n\\nCompaction is a feature to perform fine-grained transformation of historical\\ndata to manage a fixed storage budget. This gives operators full control over\\nshrinking data gradually\u2014both from a temporal and spatial angle:\\n\\n**Spatial**: Traditionally, reaching a storage budget triggers deletion of the\\noldest (or least-recently-used) data. This is a binary decision to throw away a\\nsubset of events. It does not differentiate the utility of data within an event.\\nWhat if you could only throw away the irrelevant parts and keep the information\\nthat might still be useful for longitudinal investigations? What if you could\\naggregate multiple events into a single one that captures valuable information?\\nImagine, for example, halving the space utilization of events with network flow\\ninformation and keeping them 6 months longer; or imagine you could roll up a set\\nof flows into a traffic matrix that only captures who communicated with whom in\\na given timeframe.\\n\\nBy incrementally elevating data into more space-efficient representations,\\ncompaction gives you a much more powerful mechanism to achieve long retention\\nperiods while working with high-volume telemetry.\\n\\n**Temporal**: data residency regulations often come with compliance policies\\nwith maximum retention periods, e.g., data containing personal data. For\\nexample, a policy may dictate a maximum retention of 1 week for events\\ncontaining URIs and 3 months for events containing IP addresses related to\\nnetwork connections. However, these retention windows could be broadened when\\npseudonomyzing or anonymizing the relevant fields.\\n\\nCompaction has a policy-based approach to specify these temporal constraints in\\na clear, declarative fashion.\\n\\nCompaction supersedes both the disk monitor and aging, being able to cover the\\nentire functionality of their behaviors in a more configurable way. The disk\\nmonitor remains unchanged and the experimental aging feature is deprecated (see\\nbelow).\\n\\n## Updates to Transform Steps\\n\\n### Aggregate Step\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/releases/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Additionally, we renamed the `aggregate` operator to\\n`summarize`. Please keep this in mind when reading the example\\nbelow and consult the documentation for the up-to-date syntax.\\n:::\\n\\nThe new `aggregate` transform step plugin allows for reducing data with an\\naggregation operation over a group of columns.\\n\\nAggregation is a two-step process of first bucketing data in groups of values,\\nand then executing an aggregation function that computes a single value over the\\nbucket. The functionality is in line with what standard execution engines offer\\nvia \\"group-by\\" and \\"aggregate\\".\\n\\nBased on how the transformation is invoked in VAST, the boundary for determining\\nwhat goes into a grouping can be a table slice (e.g., during import/export) or\\nan entire partition (during compaction).\\n\\nHow this works is best shown on example data. Consider the following events\\nrepresenting flow data that contain a source IP address, a start and end\\ntimestamp, the number of bytes per flow, a boolean flag whether there is an\\nassociated alert, and a unique identifier.\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 87122, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": false, \\"unique_id\\": 1}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2022-02-22T10:36:43\\", \\"end\\": \\"2022-02-22T10:36:48\\", \\"alerted\\": false, \\"unique_id\\": 2}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 640, \\"start\\": \\"2022-02-22T10:36:46\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": true, \\"unique_id\\": 3}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 2162, \\"start\\": \\"2022-02-22T10:36:49\\", \\"end\\": \\"2022-02-22T10:36:51\\", \\"alerted\\": false, \\"unique_id\\": 4}\\n```\\n\\nWe can now configure a transformation that groups the events by their source IP\\naddress, takes the sum of the number of bytes, the minimum of the start\\ntimestamp, the maximum of the end timestamp, and the disjunction of the alerted\\nflag. Since the unique identifier cannot be aggregated in a meaningful manner,\\nit  is discarded.\\n\\n```yaml\\nvast:\\n  transforms:\\n    example-aggregation:\\n      - aggregate:\\n          group-by:\\n            - source_ip\\n          sum:\\n            - num_bytes\\n          min:\\n            - start\\n          max:\\n            - end\\n          any:\\n            - alerted\\n```\\n\\nAfter applying the transform, the resulting events will look like this:\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 89924, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-02T10:36:51\\", \\"alerted\\": true}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2020-11-06T10:36:43\\", \\"end\\": \\"2020-02-22T10:36:48\\", \\"alerted\\": false}\\n```\\n\\nUnlike the built-in transform steps, `aggregate` is a separate open-source\\nplugin that needs to be manually enabled in your `vast.yaml` configuration to be\\nusable:\\n\\n```yaml\\nvast:\\n  plugins:\\n    - aggregate\\n```\\n\\n### Rename Step\\n\\nThe new `rename` transform step is a built-in that allows for changing the name\\nof the schema of data. This is particularly useful when a transformation changes\\nthe shape of the data. E.g., an aggregated `suricata.flow` should likely be\\nrenamed because it is of a different layout.\\n\\nThis is how you configure the transform step:\\n\\n```yaml\\nrename:\\n  layout-names:\\n    - from: suricata.flow\\n      to: suricata.aggregated_flow\\n```\\n\\n### Project and Select Steps\\n\\nThe built-in `project` and `select` transform steps now drop table slices where\\nno columns and rows match the configuration respectively instead of leaving the\\ndata untouched.\\n\\n## Deprecations\\n\\nThe `msgpack` encoding no longer exists. As we integrate deeper with Apache\\nArrow, the `arrow` encoding is now the only option. Configuration options for\\n`msgpack` will be removed in an upcoming major release. On startup, VAST now\\nwarns if any of the deprecated options are in use.\\n\\nVAST\u2019s *aging* feature never made it out of the experimental stage: it only\\nerased data without updating the index correctly, leading to unnecessary lookups\\ndue to overly large candidate sets and miscounts in the statistics. Because\\ntime-based compaction is a superset of the aging functionality (that also\\nupdates the index correctly), we will remove aging in a future release. VAST now\\nwarns on startup if it\u2019s configured to run aging."},{"id":"/vast-v1.0","metadata":{"permalink":"/releases/vast-v1.0","source":"@site/releases/vast-v1.0/index.md","title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"release","permalink":"/releases/tags/release"},{"label":"transforms","permalink":"/releases/tags/transforms"},{"label":"query","permalink":"/releases/tags/query"}],"readingTime":3.175,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"VP Engineering","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","authors":"dominiklohmann","date":"2022-01-27T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","transforms","query"]},"prevItem":{"title":"VAST v1.1","permalink":"/releases/vast-v1.1"}},"content":"We are happy to announce [VAST v1.0][github-vast-release]!\\n\\nThis release brings a new approach to software versioning for Tenzir. We laid\\nout the semantics in detail in a new [VERSIONING][github-versioning-md]\\ndocument.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.0.0\\n[github-versioning-md]: https://github.com/tenzir/vast/blob/v1.0.0/VERSIONING.md\\n\\n\x3c!--truncate--\x3e\\n\\n## Query events based on their import time\\n\\nThe new `#import_time` extractor allows for exporting\\nevents based on the time they arrived at VAST. Most of the time, this timestamp\\nis not far away from the timestamp of when the event occurred, but in certain\\ncases the two may deviate substantially, e.g., when ingesting historical events\\nfrom several years ago.\\n\\nFor example, to export all Suricata alerts that arrived at VAST on New Years Eve\\nas JSON, run this command:\\n\\n```bash\\nvast export json \'#type == \\"suricata.alert\\" && #import_time >= 2021-12-31 && #import_time < 2022-01-01\'\\n```\\n\\nThis differs from the `:timestamp` type extractor that\\nqueries all events that contain a type `timestamp`, which is an alias for the\\n`time` type.  By convention, the `timestamp` type represents the event time\\nembedded in the data itself. However, the import time  is not part of the event\\ndata itself, but rather part of metadata of every batch of events that VAST\\ncreates.\\n\\n## Omit `null` fields in the JSON export\\n\\nVAST renders all fields defined in the schema when exporting events as JSON. A\\ncommon option for many tools that handle JSON is to skip rendering `null`\\nfields, and the new `--omit-nulls` option to the JSON export does exactly that.\\n\\nTo use it on a case-by-case basis, add this flag to any JSON export.\\n\\n```bash\\nvast export json --omit-nulls \'<query>\'\\n\\n# This also works when attaching to a matcher.\\nvast matcher attach json --omit-nulls <matcher>\\n```\\n\\nTo always enable it, add this to your `vast.yaml` configuration file:\\n\\n```yaml\\nvast:\\n  import:\\n    omit-nulls: true\\n```\\n\\n## Selection and Projection Transform Steps\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/releases/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Please keep this in mind when reading the example below and consult the\\ndocumentation for the up-to-date syntax.\\n:::\\n\\nReshaping data during import and export is a common use case that VAST now\\nsupports. The two new built-in transform steps allow for filtering columns and\\nrows. Filtering columns (*projection*) takes a list of column names as input,\\nand filtering rows (*selection*)  works with an arbitrary query expression.\\n\\nHere\u2019s a usage example that sanitizes data leaving VAST during a query. If any\\nstring field in an event contains the value `tenzir` or `secret-username`, VAST\\nwill not include the event in the result set. The example below applies this\\nsanitization only to the events  `suricata.dns` and `suricata.http`, as defined\\nin the section `transform-triggers`.\\n\\n```yaml\\nvast:\\n  # Specify and name our transforms, each of which are a list of configured\\n  # transform steps. Transform steps are plugins, enabling users to write more\\n  # complex transformations in native code using C++ and Apache Arrow.\\n  transforms:\\n     # Prevent events with certain strings to be exported, e.g., \\"tenzir\\" or\\n     # \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each transform at server- or client-side, on\\n  # import or export, and restrict them to a list of event types.\\n  transform-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - transform: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Threat Bus 2022.01.27\\n\\nThanks to a contribution from Sascha Steinbiss\\n([@satta](https://github.com/satta)), Threat Bus only reports failure when\\ntransforming a sighting context if the return code of the transforming program\\nindicates failure.\\n\\nA small peek behind the curtain: We\u2019re building the next generation of Threat\\nBus as part of VAST. We will continue to develop and maintain Threat Bus and its\\napps for the time being.\\n\\nThreat Bus 2022.01.27 is available [\ud83d\udc49\\nhere](https://github.com/tenzir/threatbus/releases/tag/2022.01.27)."}]}')}}]);