---
title: "Parquet & Feather: Security Telemetry Benchmark"
authors:
  - dispanser
  - mavam
date: 2022-10-15
image: image.jpg
tags: [benchmark, arrow, parquet, feather, quarto, r]
---

How does Apache [Parquet][parquet] compare to [Feather][feather] for storing
structured security data? In this blog post, we answer this question.

[parquet]: https://parquet.apache.org/
[feather]: https://arrow.apache.org/docs/python/feather.html

<!--truncate-->

:::info Parquet & Feather: 2/3
This is blog post is part of a 3-piece series on Parquet and Feather.

1. [Enabling Open Investigations][parquet-and-feather-1]
2. Security Telemetry Benchmark (this blog post)
3. TBD

[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/
:::

In the [previous blog][parquet-and-feather-1], we explained why Parquet and
Feather are great building blocks for modern investigations. In this blog, we take
a look at how they actually perform on the write path in two dimensions:

- **Size**: how much space does typical security telemetry occupy?
- **Speed**: how fast can we write out to a store?

Parquet and Feather have different goals. While Parquet is an on-disk format
that optimizes for size, Feather is just a thin layer around the native Arrow
in-memory representation. This puts them at different points in the spectrum of
throughput and latency.

To better understand this spectrum, we instrumented the
write path of VAST, which consists roughly of the following steps:

1. Parse the input
2. Convert it into Arrow record batches
3. Ship Arrow record batches to a VAST server
4. Write Arrow out into a Parquet or Feather store

Since steps (1â€“3) are the same for both stores, we ignore them in the following
analysis and solely zoom in on (4).

## Dataset

For our evaluation, we use a dataset that models a "normal day in a corporate
network" fused with data from for real-world attacks. While this approach might
not be ideal for detection engineering, it provides enough diversity to analyze
storage and processing behavior.

Specifically, we rely on a 3.77 GB PCAP trace of the [M57 case study][m57]. We
also injected real-world attacks from
[malware-traffic-analysis.net][malware-traffic-analysis] into the PCAP trace. To
make the timestamps look somewhat realistic, we shifted the timestamps of the
PCAPs to pretend that the corresponding activity happens on the same day. For
this we used  [`editcap`][editcap] and then merged the resulting PCAPs into one
big file using [`mergecap`][mergecap].

We then ran [Zeek](https://zeek.org) and [Suricata](https://suricata.io) over
the trace to produce structured logs. For full reproducibility, we host this
custom data set in a [Google Drive folder][m57-drive].

[m57]: https://www.sciencedirect.com/science/article/pii/S1742287612000370
[m57-drive]: https://drive.google.com/drive/folders/1mPJYVGKTk86P2JU3KD-WFz8tUkTLK095?usp=sharing
[malware-traffic-analysis]: https://www.malware-traffic-analysis.net/index.html
[editcap]: https://www.wireshark.org/docs/wsug_html_chunked/AppToolseditcap.html
[mergecap]: https://www.wireshark.org/docs/wsug_html_chunked/AppToolsmergecap.html

VAST can ingest PCAP, Zeek, and Suricata natively. All three data sources are
highly valuable for detection and investigation, which is why we use them in
this analysis. They represent a good mix of nested and structured data (Zeek &
Suricata) vs. simple-but-bulky data (PCAP). To give you a flavor, here's an
example Zeek log:

```
#separator \x09
#set_separator	,
#empty_field	(empty)
#unset_field	-
#path	http
#open	2022-04-20-09-56-45
#fields	ts	uid	id.orig_h	id.orig_p	id.resp_h	id.resp_p	trans_depth	method	host	uri	referrer	version	user_agent	origin	request_body_len	response_body_len	status_code	status_msg	info_code	info_msg	tags	username	password	proxied	orig_fuids	orig_filenames	orig_mime_types	resp_fuids	resp_filenames	resp_mime_types
#types	time	string	addr	port	addr	port	count	string	string	string	string	string	string	string	count	count	count	string	count	string	set[enum]	string	string	set[string]	vector[string]	vector[string]	vector[string]	vector[string]	vector[string]	vector[string]
1637155963.249475	CrkwBA3xeEV9dzj1n	128.14.134.170	57468	198.71.247.91	80	1	GET	198.71.247.91	/	-	1.1	Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36 	-	0	51	200	OK	-	-	(empty)	-	-	-	-	-	-	FhEFqzHx1hVpkhWci	-	text/html
1637157241.722674	Csf8Re1mi6gYI3JC6f	87.251.64.137	64078	198.71.247.91	80	1	-	-	-	-	1.1	-	-	0	18	400	Bad Request	-	-	(empty)	-	-	-	-	-	-	FpKcQG2BmJjEU9FXwh	-	text/html
1637157318.182504	C1q1Lz1gxAAyf4Wrzk	139.162.242.152	57268	198.71.247.91	80	1	GET	198.71.247.91	/	-	1.1	Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0	-	0	51	200	OK	-	-	(empty)	-	-	-	-	-	-	FyTOLL1rVGzjXoNAb	-	text/html
1637157331.507633	C9FzNf12ebDETzvDLk	172.70.135.112	37220	198.71.247.91	80	1	GET	lifeisnetwork.com	/	-	1.1	Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36	-	0	51	200	OK	-	-	(empty)	-	-	X-FORWARDED-FOR -> 137.135.117.126	-	-	-	Fnmp6k1xVFoqqIO5Ub	-	text/html
1637157331.750342	C9FzNf12ebDETzvDLk	172.70.135.112	37220	198.71.247.91	80	2	GET	lifeisnetwork.com	/	-	1.1	Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36	-	0	51	200	OK	-	-	(empty)	-	-	X-FORWARDED-FOR -> 137.135.117.126	-	-	-	F1uLr1giTpXx81dP4	-	text/html
1637157331.915255	C9FzNf12ebDETzvDLk	172.70.135.112	37220	198.71.247.91	80	3	GET	lifeisnetwork.com	/wp-includes/wlwmanifest.xml	-	1.1	Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36	-	0	279	404	Not Found	-	-	(empty)	-	-	X-FORWARDED-FOR -> 137.135.117.126	-	-	-	F9dg5w2y748yNX9ZCc	-	text/html
1637157331.987527	C9FzNf12ebDETzvDLk	172.70.135.112	37220	198.71.247.91	80	4	GET	lifeisnetwork.com	/xmlrpc.php?rsd	-	1.1	Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36	-	0	279	404	Not Found	-	-	(empty)	-	-	X-FORWARDED-FOR -> 137.135.117.126	-	-	-	FxzLxklm7xyuzTF8h	-	text/html
```

Here's a snippet of a Suricata log:

```json
{"timestamp":"2021-11-17T14:32:43.262184+0100","flow_id":1129058930499898,"pcap_cnt":7,"event_type":"http","src_ip":"128.14.134.170","src_port":57468,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":0,"community_id":"1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=","http":{"hostname":"198.71.247.91","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51}}
{"timestamp":"2021-11-17T14:32:43.237882+0100","flow_id":675134617085815,"event_type":"flow","src_ip":"54.176.143.72","dest_ip":"198.71.247.91","proto":"ICMP","icmp_type":8,"icmp_code":0,"response_icmp_type":0,"response_icmp_code":0,"flow":{"pkts_toserver":1,"pkts_toclient":1,"bytes_toserver":50,"bytes_toclient":50,"start":"2021-11-17T14:43:34.649079+0100","end":"2021-11-17T14:43:34.649210+0100","age":0,"state":"established","reason":"timeout","alerted":false},"community_id":"1:WHH+8OuOygRPi50vrH45p9WwgA4="}
{"timestamp":"2021-11-17T14:32:48.254950+0100","flow_id":1129058930499898,"pcap_cnt":10,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"128.14.134.170","dest_port":57468,"proto":"TCP","http":{"hostname":"198.71.247.91","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51},"app_proto":"http","fileinfo":{"filename":"/","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":51,"tx_id":0}}
{"timestamp":"2021-11-17T14:55:18.327585+0100","flow_id":652708491465446,"pcap_cnt":206,"event_type":"http","src_ip":"139.162.242.152","src_port":57268,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":0,"community_id":"1:gEyyy4v7MJSsjLvl+3D17G/rOIY=","http":{"hostname":"198.71.247.91","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51}}
{"timestamp":"2021-11-17T14:55:18.329669+0100","flow_id":652708491465446,"pcap_cnt":208,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"139.162.242.152","dest_port":57268,"proto":"TCP","http":{"hostname":"198.71.247.91","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51},"app_proto":"http","fileinfo":{"filename":"/","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":51,"tx_id":0}}
{"timestamp":"2021-11-17T14:55:31.569634+0100","flow_id":987097466129838,"pcap_cnt":224,"event_type":"http","src_ip":"172.70.135.112","src_port":37220,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":0,"community_id":"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=","http":{"hostname":"lifeisnetwork.com","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51}}
{"timestamp":"2021-11-17T14:55:31.750383+0100","flow_id":987097466129838,"pcap_cnt":226,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"172.70.135.112","dest_port":37220,"proto":"TCP","http":{"hostname":"lifeisnetwork.com","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51},"app_proto":"http","fileinfo":{"filename":"/","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":51,"tx_id":0}}
{"timestamp":"2021-11-17T14:55:31.812254+0100","flow_id":987097466129838,"pcap_cnt":228,"event_type":"http","src_ip":"172.70.135.112","src_port":37220,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":1,"community_id":"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=","http":{"hostname":"lifeisnetwork.com","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51}}
{"timestamp":"2021-11-17T14:55:31.915298+0100","flow_id":987097466129838,"pcap_cnt":230,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"172.70.135.112","dest_port":37220,"proto":"TCP","http":{"hostname":"lifeisnetwork.com","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51},"app_proto":"http","fileinfo":{"filename":"/","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":51,"tx_id":1}}
{"timestamp":"2021-11-17T14:55:31.977269+0100","flow_id":987097466129838,"pcap_cnt":232,"event_type":"http","src_ip":"172.70.135.112","src_port":37220,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":2,"community_id":"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=","http":{"hostname":"lifeisnetwork.com","url":"/wp-includes/wlwmanifest.xml","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":404,"length":279}}
{"timestamp":"2021-11-17T14:55:31.987556+0100","flow_id":987097466129838,"pcap_cnt":234,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"172.70.135.112","dest_port":37220,"proto":"TCP","http":{"hostname":"lifeisnetwork.com","url":"/wp-includes/wlwmanifest.xml","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":404,"length":279},"app_proto":"http","fileinfo":{"filename":"/wp-includes/wlwmanifest.xml","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":279,"tx_id":2}}
{"timestamp":"2021-11-17T14:55:32.049539+0100","flow_id":987097466129838,"pcap_cnt":236,"event_type":"http","src_ip":"172.70.135.112","src_port":37220,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":3,"community_id":"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=","http":{"hostname":"lifeisnetwork.com","url":"/xmlrpc.php?rsd","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":404,"length":279}}
{"timestamp":"2021-11-17T14:55:32.057985+0100","flow_id":987097466129838,"pcap_cnt":238,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"172.70.135.112","dest_port":37220,"proto":"TCP","http":{"hostname":"lifeisnetwork.com","url":"/xmlrpc.php?rsd","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":404,"length":279},"app_proto":"http","fileinfo":{"filename":"/xmlrpc.php","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":279,"tx_id":3}}
{"timestamp":"2021-11-17T14:55:32.119589+0100","flow_id":987097466129838,"pcap_cnt":239,"event_type":"http","src_ip":"172.70.135.112","src_port":37220,"dest_ip":"198.71.247.91","dest_port":80,"proto":"TCP","tx_id":4,"community_id":"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=","http":{"hostname":"lifeisnetwork.com","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51}}
{"timestamp":"2021-11-17T14:55:32.127935+0100","flow_id":987097466129838,"pcap_cnt":241,"event_type":"fileinfo","src_ip":"198.71.247.91","src_port":80,"dest_ip":"172.70.135.112","dest_port":37220,"proto":"TCP","http":{"hostname":"lifeisnetwork.com","url":"/","http_user_agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36","xff":"137.135.117.126","http_content_type":"text/html","http_method":"GET","protocol":"HTTP/1.1","status":200,"length":51},"app_proto":"http","fileinfo":{"filename":"/","sid":[],"gaps":false,"state":"CLOSED","stored":false,"size":51,"tx_id":4}}
```

Note that Zeek's tab-separated value (TSV) format is already a structured table,
whereas Suricata data needs to be demultiplexed first through the `event_type`
field.

The PCAP packet type is currently hard-coded in VAST's PCAP plugin:

```cpp
type make_packet_type() {
  const auto port_type = type{"port", count_type{}};
  const auto timestamp_type = type{"timestamp", time_type{}};
  return type{
    "pcap.packet",
    record_type{
      {"time", timestamp_type},
      {"src", address_type{}},
      {"dst", address_type{}},
      {"sport", port_type},
      {"dport", port_type},
      {"vlan",
       record_type{
         {"outer", count_type{}},
         {"inner", count_type{}},
       }},
      {"community_id", type{string_type{}, {{"index", "hash"}}}},
      {"payload", type{string_type{}, {{"skip"}}}},
    },
  };
}
```

Now that we've looked at the structure of the dataset, let's take a look at our
measurement methodology.

### Measurement

Our objective is understanding the storage and runtime characteristics of
Parquet and Feather on the provided input data. To this end, we instrumented
VAST to produce us with a measurement trace file that we then analyze with R for
gaining insights. The [corresponding patch][patch] is not meant for further
production, so we kept it separate. But we did find an opportunity to improve
VAST and [made the Zstd compression level configurable][pr-2623]. Our [benchmark
sript](benchmark.fish) is available for full reproducibility.

[pr-2623]: https://github.com/tenzir/vast/pull/2623
[patch]: feather-parquet-zstd-experiments.diff

Our instrumentation produced a [CSV file](data.csv) with the following features:

- **Store**: the type of store plugin used in the measurement, i.e., `parquet`
  or `feather`.
- **Construction time**: the time it takes to convert Arrow record batches into
  Parquet or Feather. We fenced the corresponding code blocks and computed the
  difference in nanoseconds.
- **Input size**: the number of bytes that the to-be-converted record batches
  consume.
- **Output size**: the number of bytes that the store file takes up.
- **Number of events**: the total number of events in all input record batches
- **Number of record batches**
- **Schema**: the name of the schema; there exists one store file per schema
- **Zstd compression level**: the applied Zstd compression level

Every row corresponds to a single store file where we varied some of these
parameters. We used [hyperfine](https://github.com/sharkdp/hyperfine) as
benchmark driver tool, configured with 8 runs. Let's take a look at the data.

```{r}
#| code-fold: true
#| label: prepare-data
library(dplyr)
library(ggplot2)
library(lubridate)
library(scales)
library(stringr)
library(tidyr)

# For faceting, to show clearer boundaries.
theme_bw_trans <- function(...) {
  theme_bw(...) +
  theme(panel.background = element_rect(fill = "transparent"),
        plot.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent"),
        legend.background = element_rect(fill = "transparent"))
}

theme_set(theme_minimal())

data <- read.csv("data.csv") |>
  rename(store = store_type) |>
  mutate(duration = dnanoseconds(duration))

original <- read.csv("sizes.csv") |>
  mutate(store = "original", store_class = "original") |>
  select(store, store_class, schema, bytes)

# Global view on number of events per schema.
schemas <- data |>
  # Pick one element from the run matrix.
  filter(store == "feather" & zstd.level == 1) |>
  group_by(schema) |>
  summarize(n = sum(num_events),
            bytes_memory = sum(bytes_memory))

# Normalize store sizes by number of events/store.
normalized <- data |>
  mutate(duration_normalized = duration / num_events,
         bytes_memory_normalized = bytes_memory / num_events,
         bytes_storage_normalized = bytes_in_storage / num_events,
         bytes_ratio = bytes_in_storage / bytes_memory)

# Compute average over measurements.
aggregated <- normalized |>
  group_by(store, schema, zstd.level) |>
  summarize(duration = mean(duration_normalized),
            memory = mean(bytes_memory_normalized),
            storage = mean(bytes_storage_normalized))

# Treat in-memory measurements as just another storage type.
memory <- aggregated |>
  filter(store == "feather" & zstd.level == 1) |>
  mutate(store = "memory", store_class = "memory") |>
  select(store, store_class, schema, bytes = memory)

# Unite with rest of data.
unified <- aggregated |>
  select(-memory) |>
  rename(bytes = storage, store_class = store) |>
  unite("store", store_class, zstd.level, sep = "+", remove = FALSE)

schemas_gt10k <- schemas |> filter(n > 10e3) |> pull(schema)
schemas_gt100k <- schemas |> filter(n > 100e3) |> pull(schema)

# Only schemas with > 100k events.
cleaned <- unified |>
  filter(schema %in% schemas_gt100k) |>
  mutate(zstd.level = factor(str_replace_na(zstd.level),
                             levels = c("NA", "-5", "1", "9", "19")))

# Helper function to format numbers with SI unit suffixes.
fmt_si <- function(x) scales::label_number_si(accuracy = 0.1)(x)
```

### Schemas

We have a total of `r length(unique(data$schema))` unique schemas:

```{r}
#| label: schemas
#| echo: false
unique(data$schema)
```

The schemas belong to three data *modules*: Zeek, Suricata, and PCAP. A module
is the prefix of a concrete type, e.g., for the schema `zeek.conn` the module is
`zeek` and the type is `conn`. This is only a distinction in terminology,
internally VAST stores the full-qualified type as schema name.

How many events do we have per schema?

```{r}
#| code-fold: true
#| label: number-of-events-by-schema
schemas <- normalized |>
  # Pick one element from the run matrix.
  filter(store == "feather" & zstd.level == 1) |>
  group_by(schema) |>
  summarize(n = sum(num_events),
            bytes_memory = sum(bytes_memory))

schemas |>
  separate(schema, c("module", "type"), remove = FALSE) |>
  ggplot(aes(x = reorder(schema, -n), y = n, fill = module)) +
    geom_bar(stat = "identity") +
    scale_y_log10(labels = scales::label_comma()) +
    labs(x = "Schema", y = "Number of Events", fill = "Module") +
    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))
```

The above plot (log-scaled y-axis) shows how many events we have per type.
Between 1 and 100M events, we almost see everything.

What's the typical event size?

```{r}
#| code-fold: true
#| label: event-size-by-schema
schemas |>
  separate(schema, c("module", "type"), remove = FALSE) |>
  ggplot(aes(x = reorder(schema, -n), y = bytes_memory / n, fill = module)) +
    geom_bar(stat = "identity") +
    guides(fill = "none") +
    scale_y_continuous(labels = scales::label_bytes(units = "auto_si")) +
    labs(x = "Schema", y = "Bytes (in-memory)", color = "Module") +
    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))
```

The above plot keeps the x-axis from the previous plot, but exchanges the y-axis
to show normalized event size, in memory after parsing. Most events
take up a few 100 bytes, with packet data consuming a bit more, and one 5x
outlier: `suricata.ftp`.

Such distributions are normal, even with these outliers. Some telemetry events
simply have more string data that's a function of user input. For `suricata.ftp`
specifically, it can grow linearly with the data transmitted. Here's a stripped
down example of an event that is greater than 5 kB in its raw JSON:

```json
{
  "timestamp": "2021-11-19T05:08:50.885981+0100",
  "flow_id": 1339403323589433,
  "pcap_cnt": 5428538,
  "event_type": "ftp",
  "src_ip": "10.5.5.101",
  "src_port": 50479,
  "dest_ip": "62.24.128.228",
  "dest_port": 110,
  "proto": "TCP",
  "tx_id": 12,
  "community_id": "1:kUFeGEpYT1JO1VCwF8wZWUWn0J0=",
  "ftp": {
    "completion_code": [
      "155",
      ...
      <stripped 330 lines>
      ...
      "188",
      "188",
      "188"
    ],
    "reply": [
      " 41609",
      ...
      <stripped 330 lines>
      ...
      " 125448",
      " 126158",
      " 29639"
    ],
    "reply_received": "yes"
  }
}
```

This matches our mental model. A few hundred bytes per event with some outliers.

### Batching

On the inside, a store is just a concatenation of Arrow record batches. In
Feather they are literally concatenated sequentially, whereas Parquet
concatenates them column-wise. Most stores only consist of a few record
batches. PCAP is the only difference.

Small stores are suboptimal because the catalog keeps in-memory state that is
a linear function of the number of stores. (We are aware of this concern and are
exploring improvements, but this topic is out of scope for this post.) The
issue here is catalog fragmentation.

As of [v2.3](/blog/vast-v2.3), VAST has automatic rebuilding in place, which
merges underfull partitions to reduce pressure on the catalog. This doesn't fix
the problem of linear state, but gives us much sufficient reach for real-world
deployments.

## Size

To better understand the difference between Parquet and Feather, we now take a
look at them right next to each other. In addition to Feather and Parquet, we
use two other types of "stores" for the analysis to facilitate comparison:

1. **Original**: the size of the input prior it entered VAST, e.g., the raw JSON or
   a PCAP file.

3. **Memory**: the size of the data in memory, measured as the sum of Arrow
   buffers that make up the table slice.

Let's kick of the analysis by getting a better understanding at the size
distribution.

```{r}
#| code-fold: true
#| label: plot-schema-distribution-boxplot
unified |>
  bind_rows(original, memory) |>
  ggplot(aes(x = reorder(store, -bytes, FUN = "median"),
             y = bytes, color = store_class)) +
  geom_boxplot() +
  scale_y_log10(labels = scales::label_bytes(units = "auto_si")) +
  labs(x = "Store", y = "Bytes/Event", color = "Store") +
  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))
```

Every boxplot corresponds to one store, with `original` and `memory` being also
treated like stores. The suffix `-Z` indicates Zstd compression level `Z`, with
`NA` meaning "compression turned off" entirely.  Parquet stores on the right (in
purple) have the smallest size, followed by Feather (red), and then their
corresponding in-memory (green) and original (turquoise) representation. The
negative Zstd compression level -5 makes Parquet actually worse than Feather.

:::tip Analysis
What stands out is that disabling compression for Feather inflates the data
larger than the original. This is not the case for Parquet. Why? Because Parquet
has an orthogonal layer of compression using dictionaries. This absorbs
inefficiencies in heavy-tailed distributions, which are pretty standard in
machine-generated data.
:::

The y-axis of above plot is log-scaled, which makes it hard for relative
comparison. Let's focus on the medians (the bars in the box) only and bring the
y-axis to a linear scale:

```{r}
#| code-fold: true
#| label: plot-schema-distribution-medians
medians <- unified |>
  bind_rows(original, memory) |>
  group_by(store, store_class) |>
  summarize(bytes = median(bytes))

medians |>
  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::label_bytes(units = "auto_si")) +
  labs(x = "Store", y = "Bytes/Event", fill = "Store") +
  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))
```

To better understand the compression in numbers, we'll anchor the original size
at 100% and now show the *relative* gains of Parquet and Feather:

```{r}
#| echo: false
library(knitr)

median_original <- medians |> filter(store == "original") |> pull(bytes)
medians |>
  filter(store_class %in% c("feather", "parquet")) |>
  mutate(size = bytes / median_original * 100) |>
  mutate(compression = median_original / bytes) |>
  arrange(size) |>
  kable(digits = 1, col.names = c("Store", "Class", "Bytes/Event", "Size (%)",
                                  "Compression Ratio"))
```

:::tip Analysis
Parquet dominates Feather with respect to space savings, but not by much for
high Zstd compression levels. Zstd compression levels > 1 do not provide
substantial space savings on averge, where observe a compression ratio of
**~4x** over the base data. Parquet still provides a **2.6** compression ratio
in the absence of compression because it applies dictionary encoding.

Feather offers competite compression with **~3x** ratio for equal Zstd
compression levels. However, without compression Feather expands beyond the
original dataset size at a compression ratio of **~0.9**.
:::

The above analysis covered averages across schemas. If we juxtapose Parquet and
Feather per schema, we see the difference between the two formats more clearly:

```{r}
#| code-fold: true
#| label: plot-parquet-vs-feather
library(ggrepel)

parquet_vs_feather_size <- unified |>
  select(-store, -duration) |>
  pivot_wider(names_from = store_class,
              values_from = bytes,
              id_cols = c(schema, zstd.level))

plot_parquet_vs_feather <- function(data) {
  data |>
    mutate(zstd.level = str_replace_na(zstd.level)) |>
    separate(schema, c("module", "type"), remove = FALSE) |>
    ggplot(aes(x = parquet, y = feather,
               shape = zstd.level, color = zstd.level)) +
      geom_abline(intercept = 0, slope = 1, color = "grey") +
      geom_point(alpha = 0.6, size = 3) +
      geom_text_repel(aes(label = schema),
                color = "grey",
                size = 1, # font size
                box.padding = 0.2,
                min.segment.length = 0, # draw all line segments
                max.overlaps = Inf,
                segment.size = 0.2,
                segment.color = "grey",
                segment.alpha = 0.3) +
      scale_size(range = c(0, 10)) +
      labs(x = "Bytes (Parquet)", y = "Bytes (Feather)",
           shape = "Zstd Level", color = "Zstd Level")
}

parquet_vs_feather_size |>
  filter(schema %in% schemas_gt100k) |>
  plot_parquet_vs_feather() +
    scale_x_log10(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_log10(labels = scales::label_bytes(units = "auto_si"))
```

In the above log-log scatterplot, the straight line is the identity function.
Each point represents the median store size for a given schema. If a point is on
the line, it means there is no difference between Feather and Parquet. We only
look at schemas with more than 100k events to ensure that the constant factor
does not perturb the analysis. (Otherwise we end up with points *below* the
identity line, which are completely dwarfed by the bulk in practice.) The color
and shape shows the different Zstd compression levels, with `NA` meaning no
compression. Points clouds closer to the origin mean that the corresponding
store class takes up less space.

:::tip Analysis
We observe that **disabling compression hits Feather the hardest**.
Unexpectedly, a negative Zstd level of -5 does not compress well. The remaining
Zstd levels are difficult to take apart visually, but it appears that the point
clouds form a parallel line, indicating stable compression gains. Notably,
**compressing PCAP packets is nearly identical with Feather and Parquet**,
presumably because of the low entropy and packet meta data where general-purpose
compressors like Zstd shine.
:::

Zooming in to the bottom left area with average event size of less than 100B,
and removing the log scaling, we see the following:

```{r}
#| code-fold: true
#| label: plot-parquet-vs-feather-100

parquet_vs_feather_size |>
  filter(feather <= 100 & schema %in% schemas_gt100k) |>
  plot_parquet_vs_feather() +
    scale_x_continuous(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_continuous(labels = scales::label_bytes(units = "auto_si")) +
    coord_fixed()
```

The respective point clouds form a parallel to the identity function, i.e., the
compression ratio in this region pretty constant across schemas. There's also no
noticeable difference between Zstd level 1, 9, and 19.

If we take pick a single point, e.g., `zeek.conn` with
`r schemas |> filter(schema == "zeek.conn") |> pull(n) |> fmt_si()` events, we
can confirm that the relative performance matches the results of our analysis
above:

```{r}
#| code-fold: true
#| label: plot-zeek-suricata
unified |>
  filter(schema == "zeek.conn") |>
  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +
    geom_bar(stat = "identity") +
    guides(fill = "none") +
    labs(x = "Store", y = "Bytes/Event") +
    scale_y_continuous(labels = scales::label_bytes(units = "auto_si")) +
    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +
    facet_wrap(~ schema, scales = "free")
```

Finally, we look at the fraction of space Parquet takes compared to Feather on a
per schema basis, restricted to schemas with more than 10k events:

```{r}
#| code-fold: true
#| label: plot-parquet-divided-by-feather
library(tibble)

parquet_vs_feather_size |>
  filter(feather <= 100 & schema %in% schemas_gt10k) |>
  mutate(zstd.level = str_replace_na(zstd.level)) |>
  ggplot(aes(x = reorder(schema, -parquet / feather),
             y = parquet / feather,
             fill = zstd.level)) +
    geom_hline(yintercept = 1) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(x = "Schema", y = "Parquet / Feather (%)", fill = "Zstd Level") +
    scale_y_continuous(breaks = 6:1 * 20 / 100, labels = scales::label_percent()) +
    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))
```

The horizontal line is similar to the identiy line in the scatterplot,
indicating that Feather and Parquet compress equally well. The bars represent
that ratio of Parquet divided by Feather. The shorter the bars, the smaller the
size, so the higher the gain over Feather.

:::tip Analysis
We see that Zstd level 19 brings Parquet and Feather close together. Even at
Zstd level 1, the median ratio of Parquet stores is **78%**, and the 3rd
quartile **82%**. This shows that **Feather is remarkably competitive for typical
security analytics workloads**.
:::

## Speed

Now that we have looked at the spatial properties of Parquet and Feather, we
take a look at the runtime. Our mental model is that Feather outperforms
Parquet. But is that the case?

With *speed*, we mean the time it takes to transform Arrow Record Batches into
Parquet and Feather format. To avoid distortotion of small events, we also
restrict the analysis to schemas with more than 100k events.

```{r}
#| code-fold: true
#| label: duration-distribution
unified |>
  filter(schema %in% schemas_gt100k) |>
  ggplot(aes(x = reorder(store, -duration, FUN = "median"),
             y = duration, color = store_class)) +
  geom_boxplot() +
  scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +
  labs(x = "Store", y = "Speed (us)", color = "Store")
```

The above boxplots have a log-scaled y-axis and are sorted by their median,
similar to the size discussion above.

:::tip Analysis
Unexpectdly, we observe roughly an ordering according to Zstd level: more
copmression means a shorter runtime.

For the same Zstd level, **Parquet store creation was always faster**. Our
unconfirmed hunch is that Feather compression operates on more and smaller
column buffers, whereas Parquet compression only runs over the concatenated
Arrow buffers, yielding bigger strides.
:::

We don't have an explanation for why disabling compression for Parquet is
*slower* compared Zstd levels -5 and 1. In theory, strictly less cycles are
spent by disabling the code path. Perhaps compression results in different
memory layout that is more cache-efficient. Unfortunately, we did not have the
time to dig deeper into the analysis to figure out why disabling Parquet
compression is slower. Please don't hesitate to reach out, e.g., via our
[Community Slack](http://slack.tenzir.com).

Let's look at the runtime data per schema:

```{r}
#| code-fold: true
#| label: pairwise-runtime-comparion
parquet_vs_feather_duration <- unified |>
  filter(schema %in% schemas_gt100k) |>
  select(-store, -bytes) |>
  pivot_wider(names_from = store_class,
              values_from = duration,
              id_cols = c(schema, zstd.level))

parquet_vs_feather_duration |>
  mutate(zstd.level = str_replace_na(zstd.level)) |>
  separate(schema, c("module", "type"), remove = FALSE) |>
  ggplot(aes(x = parquet, y = feather,
             shape = zstd.level, color = zstd.level)) +
    geom_abline(intercept = 0, slope = 1, color = "grey") +
    geom_point(alpha = 0.7, size = 3) +
    geom_text_repel(aes(label = schema),
              color = "grey",
              size = 1, # font size
              box.padding = 0.2,
              min.segment.length = 0, # draw all line segments
              max.overlaps = Inf,
              segment.size = 0.2,
              segment.color = "grey",
              segment.alpha = 0.3) +
    scale_size(range = c(0, 10)) +
    scale_x_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
    labs(x = "Speed (Parquet)", y = "Speed (Feather)",
         shape = "Zstd Level", color = "Zstd Level")
```


## Space-Time Tradeoff

```{r}
#| code-fold: true
#| label: points-by-level
cleaned <- unified |>
  filter(schema %in% schemas_gt100k) |>
  mutate(zstd.level = factor(str_replace_na(zstd.level),
                             levels = c("NA", "-5", "1", "9", "19"))) |>
  group_by(schema, store_class, zstd.level) |>
  summarize(bytes = median(bytes), duration = median(duration))

cleaned |>
  ggplot(aes(x = bytes, y = duration,
             shape = store_class, color = zstd.level)) +
    geom_point(size = 3, alpha = 0.7) +
    scale_x_log10(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
    labs(x = "Size", y = "Speed", shape = "Store", color = "Zstd Level")
```

```{r}
#| code-fold: true
#| label: convex-hull
# Native convex hull does the job, no need to leverage ggforce.
convex_hull <- cleaned |>
  group_by(store_class, zstd.level) |>
  slice(chull(x = bytes, y = duration))

convex_hull |>
  ggplot(aes(x = bytes, y = duration,
             shape = store_class, color = zstd.level)) +
    geom_point(size = 3) +
    geom_polygon(aes(fill = zstd.level, color = zstd.level),
                 alpha = 0.1,
                 show.legend = FALSE) +
    scale_x_log10(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
    labs(x = "Size", y = "Speed", shape = "Store", color = "Zstd Level")
```


```{r}
#| code-fold: true
#| label: convex-hull-facet-by-level
cleaned |>
  group_by(store_class, zstd.level) |>
  # Native convex hull does the job, no need to leverage ggforce.
  slice(chull(x = bytes, y = duration)) |>
  ggplot(aes(x = bytes, y = duration,
             shape = store_class, color = store_class)) +
    geom_point(size = 3) +
    geom_polygon(aes(color = store_class, fill = store_class),
                 alpha = 0.3,
                 show.legend = FALSE) +
    scale_x_log10(n.breaks = 4, labels = scales::label_bytes(units = "auto_si")) +
    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
    labs(x = "Size", y = "Speed", shape = "Store", color = "Store") +
    facet_grid(cols = vars(zstd.level)) +
    theme_bw_trans()
```

```{r}
#| code-fold: true
#| label: convex-hull-facet-by-store
cleaned |>
  group_by(store_class, zstd.level) |>
  slice(chull(x = bytes, y = duration)) |>
  ggplot(aes(x = bytes, y = duration,
             shape = zstd.level, color = zstd.level)) +
    geom_point(size = 3) +
    geom_polygon(aes(color = zstd.level, fill = zstd.level),
                 alpha = 0.3,
                 show.legend = FALSE) +
    scale_x_log10(labels = scales::label_bytes(units = "auto_si")) +
    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = "us")) +
    labs(x = "Size", y = "Speed", shape = "Zstd Level", color = "Zstd Level") +
    facet_grid(rows = vars(store_class)) +
    theme_bw_trans()
```
